{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XnwhMizS8QV"
   },
   "source": [
    "#Pricing options and computing implied volatilities using neural networks\n",
    "Redwan Mekrami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw1fGnfYE_oI"
   },
   "source": [
    "# Pricing vanilla call options and computing implied volatilities using neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML4SlghDmi42"
   },
   "source": [
    "**EXECUTER CETTE CELLULE PUIS REDEMARRER L'ENVIRNEMENT ET REEXECUTER** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vrrvyBwafo8",
    "outputId": "c27addc2-9dc4-4b05-f49c-75e535014882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.7 in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.7) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "# Updating scipy to use module QMC for Latin hypercube sampling\n",
    "!pip install scipy==1.7\n",
    "from scipy.stats import qmc\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR, LinearLR, OneCycleLR, LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ll-jOXXHagp0"
   },
   "source": [
    "# 1) Black Scholes pricing network: BS-ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZJv3pMOHV-Q"
   },
   "source": [
    "### Closed-form formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hP3TMaNajHA"
   },
   "outputs": [],
   "source": [
    "def bs_price_call(S, K, T, t, r, sigma):\n",
    "  \"\"\"\n",
    "  Black-Sholes call option value price related to PDE (2). Returns option's price.\n",
    "  S: spot price\n",
    "  K: strike price\n",
    "  T: time to maturity\n",
    "  t: spot time\n",
    "  r: interest rate\n",
    "  sigma: volatility of underlying asset\n",
    "  \"\"\"\n",
    "\n",
    "  # Boundary condition of Black-Sholes PDE (2)\n",
    "  if T==t:\n",
    "    return np.maximum(0,S-K)  \n",
    "  # Solution of the PDE (2) on the open interval [t, T)\n",
    "  else : \n",
    "    d1 = ( np.log(S / K) + (r + 0.5 * sigma ** 2) * (T-t) ) / (sigma * np.sqrt(T-t))\n",
    "    d2 = d1 - sigma*np.sqrt(T-t)\n",
    "    value = S * norm.cdf(d1) - K * np.exp(-r * (T-t)) * norm.cdf(d2) \n",
    "  \n",
    "  return value\n",
    "\n",
    "def bs_moyeness_call(moyeness, tau, r, sigma):\n",
    "  \"\"\"\n",
    "  Same function as bs_price_call but in terms of moyeness. Returns V/K and \n",
    "  has S/K instead of two seperate paremeters S and K.\n",
    "  moyeness: S/K\n",
    "  Tau: time to maturity minus spot time\n",
    "  r: interest rate\n",
    "  sigma: volatility of underlying asset\n",
    "  \"\"\"\n",
    "\n",
    "  d1 = ( np.log(moyeness) + (r + 0.5 * sigma ** 2) * (tau) ) / (sigma * np.sqrt(tau))\n",
    "  d2 = d1 - sigma*np.sqrt(tau)\n",
    "  value_moyeness = moyeness * norm.cdf(d1) - np.exp(-r * (tau)) * norm.cdf(d2) \n",
    "  \n",
    "  return value_moyeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9NH1ca93i2T",
    "outputId": "5a5ca2a3-feb7-4a5b-9184-853d147a0fa9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the call prices with respect to strikes\n",
    "r, T, t, sigma, K, S = .02, 1, 1, .2, 100, np.arange(200)\n",
    "plt.plot(S, bs_price_call(S, K, T, t, r, sigma))\n",
    "plt.xlabel(\"Strike\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Call option price\" )\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdhtgoThB8B3"
   },
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DbQqHO6oRST",
    "outputId": "27c7d041-749d-4a25-c5b0-8e8b2e14e880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ INPUT ------\n",
      "S/K.......... [0.40 1.60]\n",
      "Tau.......... [0.20 1.10]\n",
      "r............ [0.02 0.10]\n",
      "Sigma........ [0.01 1.00]\n",
      "------ OUTPUT ------\n",
      "V/K.......... [0.00 0.90]\n"
     ]
    }
   ],
   "source": [
    "def bs_LHS_data_generator(n = 10**6, l_bounds = [.4, .2, .02, .01], u_bounds = [1.6, 1.1, .1, 1]):\n",
    "  \"\"\"\n",
    "  Generates samples of call prices using Latin hypercube sampling.\n",
    "  Returns a torch float tensor of dimension (n,5) containig the inputs samples and the value of the call.\n",
    "  The inputs are: Moyeness (S/K), time to maturity (tau), Risk free rate (r), Volatility (sigma).\n",
    "  n: number of samples.\n",
    "  l_bounds: lower bound for the inputs.  \n",
    "  u_bounds: upper bound for the inputs.\n",
    "\n",
    "  \"\"\"\n",
    "  sampler = qmc.LatinHypercube(d=4)\n",
    "  sample = sampler.random(n)\n",
    "  sample = qmc.scale(sample, l_bounds, u_bounds)\n",
    "  bs_prices = bs_moyeness_call(sample[:,0], sample[:,1], sample[:,2], sample[:,3]).reshape((-1,1))\n",
    "  bs_dataset = np.concatenate((sample, bs_prices),axis=1)\n",
    "\n",
    "  print(\"------ INPUT ------\")\n",
    "  print(\"S/K.......... [{:.2f} {:.2f}]\".format(min(bs_dataset[:,0]),max(bs_dataset[:,0])))\n",
    "  print(\"Tau.......... [{:.2f} {:.2f}]\".format(min(bs_dataset[:,1]),max(bs_dataset[:,1])))\n",
    "  print(\"r............ [{:.2f} {:.2f}]\".format(min(bs_dataset[:,2]),max(bs_dataset[:,2])))\n",
    "  print(\"Sigma........ [{:.2f} {:.2f}]\".format(min(bs_dataset[:,3]),max(bs_dataset[:,3])))\n",
    "  print(\"------ OUTPUT ------\")\n",
    "  print(\"V/K.......... [{:.2f} {:.2f}]\".format(min(bs_dataset[:,4]),max(bs_dataset[:,4])))\n",
    "\n",
    "\n",
    "  return torch.FloatTensor(bs_dataset)\n",
    "\n",
    "# Testing the function\n",
    "bs_dataset = bs_LHS_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSC_BRFkRpqs",
    "outputId": "c87f1fdf-a213-491b-b756-11845e4831af"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the LHS distribution of the input\n",
    "plt.figure(figsize=(10,7))\n",
    "h = plt.hist(bs_dataset[:,0], bins= 10)\n",
    "plt.xlabel(\"values\")\n",
    "plt.ylabel(\"sample\")\n",
    "plt.title(\"Stock price histogram (S/K)\" )\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uM-KZboXVeZp",
    "outputId": "f6717fe6-4ab6-43c1-98f7-e6a8c3620f7a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAG5CAYAAADswBI7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5hlVX3n//cndEBUBBStKKBNtM2ESzJiB/CXTKaVRBrJ2MxvkIAo4KCMEU0mwSjG5EcGZUYTGQwESVAI4BABiZE2YggPUmNuIAiRBpTY4SLdoig3aQlg6/f3x1ltThfVzenLqepa/X49z3nq7O9ee6+1axXN59mXc1JVSJIkqQ8/NtsDkCRJ0uZjuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFO0oxKUkle0t6fn+T9m3Hfn0tyzObaX9vneseYZFWSn9ycfW6JkuyZ5IYkGdP+v5hkr3HsW9raGO4kbbAkr2//o1+V5N4Wqn5htsdVVQdX1QUz3Oczq+qO9bVJsijJipka05i8D/hQVVWSv05yytQGSZYk+WaSeUO125O8dGpITrJX+9t5Zyt9CHjSPiVtOMOdpA2S5LeADwP/E5gAXgh8BFgyi2NKkq3237Mk24x5/88HXgl8upUuAN4wzVm8NwIXVdXqtt2LgW2q6p+n7O9lwDXA+6vqQ628FHhlkp8Y02FIW42t9h9DSRsuyY4Mzq6cUFWfqqrvVdX3q+ozVfXbrc1+Sf4xyUPtzMwfJ9l2I/o6Nsnft+0fTvLVJAcOrZ9McmqSvwceBX6y1d481OYtSb6S5JEktyXZt9VfkOQvknw7yZ1Jfv0phrNzks+2/VzXQsuaPoYvM7+m9fNIkpVJ3pnkGcDngBe0M52rWv/bJflwkm+014eTbDe033e13983krx5msvZZye5Isn3GISiQ5LclOS7Se5J8vtD+5rftn9TW/dgkrcm+bkkN7e5+uP1HP8vAzdW1WNt+dPAc4D/MNTHzsCvABcObXcIcMXwjpLsB1wF/E5VnbWm3vb9JeCgp5gLSU/BcCdpQ7wCeBrwl+tp8wPgN4FdWvsDgbdtZH/7A//S9nUy8Kkkzx5a/0bgeGAH4O7hDZO8Dvh94GjgWcBrgfvbGb7PAF8Gdm3j++9J1hcqjgD+B7AzsBw4dR3tzgX+W1XtAOwNfL6qvgccDHyjXcJ9ZlV9A3gvcADw74GfBfYDfreNfTHwW8AvAS8BFk3T1+vbOHYA/g74XjvWnRiEql9LcuiUbfYHFgC/yuDs63tbH3sBhyf5j+s4rn2A29csVNW/Ape2/tY4HPhqVX15qPYa4LNDy/sBfw38ZlV9bJp+vsLgdyFpExjuJG2I5wDfWXPZbTpV9aWquraqVlfVXcCfAusKDU/lPuDD7ezgJQwCxiFD68+vqltbX9+fsu2bgT+oqutrYHlV3Q38HPDcqjqlqp5o98t9lEGAW5e/rKovtuO+iEEgm873gT2TPKuqHqyqG9ezz6OAU6rqvqr6NoPw+Ma27nDgz9qxPcogpE51eVX9fVX9sKoeq6rJqlrWlm8GPsGTf+/va23/hkEY/ETrfyXwt8DL1jHWnYBHptQuAA5L8rS2fHSrAZDk6Qx+15ND2xwAPMzgTOZ0Hml9SdoEhjtJG+J+YJfhG+anajfP/1W7sf67DO7N22Uj+1tZVTW0fDfwgqHle9az7e4MzvpN9SIGl0gfWvMCfofB/YPr8s2h948Cz1xHu//C4GzV3Un+b5JXrGefL2Dts43Dx/YC1j626Y5zrVqS/ZNc0y41Pwy8lSf/3r819P5fp1le13E9yOAM4Y9U1d8B3wEObZep9wP+fKjJgcA/VNXjQ7WzgBuAq9pl3Kl2AB5axxgkjchwJ2lD/CPwODD1ct+ws4GvAguq6lkMgtPGfnzGrlNu2n8h8I2h5WLd7gFevI76nVW109Brh6p6zUaO8d8GMzhLuAR4HoP70i5dzzi/wSBorjF8bPcCuw2t23267qYs/zmDhxJ2r6odgT9h43/vU90MvHSa+oUMzti9AbiyqobD4muYcr8dg0v2rwe+DlyZ5FlT1v80g8vlkjaB4U7SyKrqYeD/A85KcmiSpyf58SQHJ/mD1mwH4LvAqiT/Dvi1TejyecCvtz5ex+B//lMDw7p8DHhnkpdn4CVJXgR8EXgkybuTbJ9kmyR7J/m5TRgnSbZNclSSHdsl4u8CP2yrvwU8J4MHUtb4BPC7SZ6bZBcGv9f/09ZdCrwpyU+3y5u/N8IQdgAeqKrH2kMLr9+U45niKmDfoUuwa1zI4J69tzB0SbY5mLXvtwOg/W5ex+Cs3xXtgRPavl/e+pK0CQx3kjZIVZ3G4Gb/3wW+zeBM2Nv5t4/JeCeDYPEIg3vZLtmE7q5j8ADAdxg8PHBYVd0/4jg/2bb58zaWTwPPrqofMHiq898Dd7Z9fwzYcR272hBvBO5ql6PfyuC+OqrqqwzC3B3tUvALgPczuER5M7AMuLHVqKrPAWcw+LiQ5cC1bf/DlzinehtwSpJHGATFS9fTdoO0M3KfZ8rH3bR7Kv8BeAaDs4YAJNkbWFVVX1/H/p4A/l/gMeAzSbYH/hMw2R42kbQJsvbtLJK0ZUhyLPDmqpr1D0eebUl+GrgF2G59D7OMeQx7Mjg7t189xf84krwL2KWq3rUB+78OOK6qbtm0kUpa503RkqTZk+Q/M7gE/XTgg8BnZivYAVTVbQyefh3FXQw+bmZD9r//ho5J0vS8LCtJW6b/xuCjYP6FwYMIm3Lv4oyqqkur6iuzPQ5pa+VlWUmSpI545k6SJKkj3nPX7LLLLjV//vyx9vG9732PZzzjGWPtQ+PnPM59zuHc5xzOfc7hpvnSl770nap67nTrDHfN/PnzueGGG8bax+TkJIsWLRprHxo/53Hucw7nPudw7nMON02Su9e1zsuykiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkfmzfYAtibLVj7MsSd9dq3aXR84ZJZGI0mSeuSZO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSerI2MJdkvOS3Jfklin1dyT5apJbk/zBUP09SZYnuT3JQUP1xa22PMlJQ/U9klzX6pck2bbVt2vLy9v6+eM6RkmSpC3NOM/cnQ8sHi4keSWwBPjZqtoL+FCr7wkcAezVtvlIkm2SbAOcBRwM7Akc2doCfBA4vapeAjwIHNfqxwEPtvrprZ0kSdJWYWzhrqq+ADwwpfxrwAeq6vHW5r5WXwJcXFWPV9WdwHJgv/ZaXlV3VNUTwMXAkiQBXgVc1ra/ADh0aF8XtPeXAQe29pIkSd2bN8P9vRT4D0lOBR4D3llV1wO7AtcOtVvRagD3TKnvDzwHeKiqVk/Tftc121TV6iQPt/bfmTqYJMcDxwNMTEwwOTm5qce3XhPbw4n7rF6rNu4+tfmtWrXKeZvjnMO5zzmc+5zD8ZnpcDcPeDZwAPBzwKVJfnKGx/AjVXUOcA7AwoULa9GiRWPt78yLLue0ZWv/yu86arx9avObnJxk3H8rGi/ncO5zDuc+53B8Zvpp2RXAp2rgi8APgV2AlcDuQ+12a7V11e8Hdkoyb0qd4W3a+h1be0mSpO7NdLj7NPBKgCQvBbZlcLl0KXBEe9J1D2AB8EXgemBBezJ2WwYPXSytqgKuAQ5r+z0GuLy9X9qWaes/39pLkiR1b2yXZZN8AlgE7JJkBXAycB5wXvt4lCeAY1rwujXJpcBtwGrghKr6QdvP24ErgW2A86rq1tbFu4GLk7wfuAk4t9XPBT6eZDmDBzqOGNcxSpIkbWnGFu6q6sh1rHrDOtqfCpw6Tf0K4Ipp6ncweJp2av0x4HUbNFhJkqRO+A0VkiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1ZGzhLsl5Se5Lcss0605MUkl2actJckaS5UluTrLvUNtjknytvY4Zqr88ybK2zRlJ0urPTnJVa39Vkp3HdYySJElbmnGeuTsfWDy1mGR34NXA14fKBwML2ut44OzW9tnAycD+wH7AyUNh7WzgLUPbrenrJODqqloAXN2WJUmStgpjC3dV9QXggWlWnQ68C6ih2hLgwhq4FtgpyfOBg4CrquqBqnoQuApY3NY9q6quraoCLgQOHdrXBe39BUN1SZKk7s2byc6SLAFWVtWX21XUNXYF7hlaXtFq66uvmKYOMFFV97b33wQm1jOe4xmcKWRiYoLJyckNPKINM7E9nLjP6rVq4+5Tm9+qVauctznOOZz7nMO5zzkcnxkLd0meDvwOg0uyM6KqKkmtZ/05wDkACxcurEWLFo11PGdedDmnLVv7V37XUePtU5vf5OQk4/5b0Xg5h3Ofczj3OYfjM5NPy74Y2AP4cpK7gN2AG5P8BLAS2H2o7W6ttr76btPUAb7VLtvSft632Y9EkiRpCzVj4a6qllXV86pqflXNZ3Apdd+q+iawFDi6PTV7APBwu7R6JfDqJDu3ByleDVzZ1n03yQHtKdmjgctbV0uBNU/VHjNUlyRJ6t44PwrlE8A/Aj+VZEWS49bT/ArgDmA58FHgbQBV9QDwPuD69jql1WhtPta2+Rfgc63+AeCXk3wN+KW2LEmStFUY2z13VXXkU6yfP/S+gBPW0e484Lxp6jcAe09Tvx84cAOHK0mS1AW/oUKSJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI6MLdwlOS/JfUluGar9YZKvJrk5yV8m2Wlo3XuSLE9ye5KDhuqLW215kpOG6nskua7VL0mybatv15aXt/Xzx3WMkiRJW5pxnrk7H1g8pXYVsHdV/Qzwz8B7AJLsCRwB7NW2+UiSbZJsA5wFHAzsCRzZ2gJ8EDi9ql4CPAgc1+rHAQ+2+umtnSRJ0lZhbOGuqr4APDCl9jdVtbotXgvs1t4vAS6uqser6k5gObBfey2vqjuq6gngYmBJkgCvAi5r218AHDq0rwva+8uAA1t7SZKk7s2bxb7/K3BJe78rg7C3xopWA7hnSn1/4DnAQ0NBcbj9rmu2qarVSR5u7b8zdQBJjgeOB5iYmGBycnLTjugpTGwPJ+6zeq3auPvU5rdq1SrnbY5zDuc+53Ducw7HZ1bCXZL3AquBi2aj/zWq6hzgHICFCxfWokWLxtrfmRddzmnL1v6V33XUePvU5jc5Ocm4/1Y0Xs7h3Occzn3O4fjMeLhLcizwK8CBVVWtvBLYfajZbq3GOur3AzslmdfO3g23X7OvFUnmATu29pIkSd2b0Y9CSbIYeBfw2qp6dGjVUuCI9qTrHsAC4IvA9cCC9mTstgweuljaQuE1wGFt+2OAy4f2dUx7fxjw+aEQKUmS1LWxnblL8glgEbBLkhXAyQyejt0OuKo943BtVb21qm5NcilwG4PLtSdU1Q/aft4OXAlsA5xXVbe2Lt4NXJzk/cBNwLmtfi7w8STLGTzQccS4jlGSJGlLM7ZwV1VHTlM+d5ramvanAqdOU78CuGKa+h0MnqadWn8MeN0GDVaSJKkTfkOFJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdGVu4S3JekvuS3DJUe3aSq5J8rf3cudWT5Iwky5PcnGTfoW2Oae2/luSYofrLkyxr25yRJOvrQ5IkaWswzjN35wOLp9ROAq6uqgXA1W0Z4GBgQXsdD5wNg6AGnAzsD+wHnDwU1s4G3jK03eKn6EOSJKl7Ywt3VfUF4IEp5SXABe39BcChQ/ULa+BaYKckzwcOAq6qqgeq6kHgKmBxW/esqrq2qgq4cMq+putDkiSpe/NmuL+Jqrq3vf8mMNHe7wrcM9RuRautr75imvr6+niSJMczOFPIxMQEk5OTG3g4G2Ziezhxn9Vr1cbdpza/VatWOW9znHM49zmHc59zOD4zHe5+pKoqSc1mH1V1DnAOwMKFC2vRokXjHA5nXnQ5py1b+1d+11Hj7VOb3+TkJOP+W9F4OYdzn3M49zmH4zPTT8t+q11Spf28r9VXArsPtdut1dZX322a+vr6kCRJ6t5Mh7ulwJonXo8BLh+qH92emj0AeLhdWr0SeHWSnduDFK8GrmzrvpvkgPaU7NFT9jVdH5IkSd0b22XZJJ8AFgG7JFnB4KnXDwCXJjkOuBs4vDW/AngNsBx4FHgTQFU9kOR9wPWt3SlVteYhjbcxeCJ3e+Bz7cV6+pAkSere2MJdVR25jlUHTtO2gBPWsZ/zgPOmqd8A7D1N/f7p+pAkSdoa+A0VkiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkdGCndJnp7k95J8tC0vSPIr4x2aJEmSNtSoZ+7+DHgceEVbXgm8fywjkiRJ0kYbNdy9uKr+APg+QFU9CmRso5IkSdJGGTXcPZFke6AAkryYwZk8SZIkbUFG/RDjk4G/BnZPchHw88Cx4xqUJEmSNs5I4a6qrkpyI3AAg8uxv1FV3xnryCRJkrTB1hvukuw7pXRv+/nCJC+sqhvHMyxJkiRtjKc6c3faetYV8KrNOBZJkiRtovWGu6p65UwNRJIkSZtupHvukjwNeBvwCwzO2P0t8CdV9dgYxyZJkqQNNOrTshcCjwBntuXXAx8HXjeOQUmSJGnjjBru9q6qPYeWr0ly2zgGJEmSpI036ocY35jkgDULSfYHbhjPkCRJkrSxRj1z93LgH5J8vS2/ELg9yTKgqupnxjI6SZIkbZBRw93isY5CkiRJm8Wo31Bxd5Kdgd2Ht/FDjCVJkrYso34UyvsYfJfsvzD4KBTwQ4wlSZK2OKNelj0ceHFVPTHOwUiSJGnTjPq07C3ATuMciCRJkjbdqGfu/hdwU5JbgMfXFKvqtWMZlSRJkjbKqOHuAuCDwDLgh+MbjiRJkjbFqOHu0ao6Y6wjkSRJ0iYbNdz9bZL/BSxl7cuyfhSKJEnSFmTUcPey9vOAoZofhSJJkrSFGfVDjF857oFIkiRp04165o4khwB7AU9bU6uqU8YxKEmSJG2ckT7nLsmfAL8KvAMI8DrgRWMclyRJkjbCqB9i/P9U1dHAg1X1P4BXAC8d37AkSZK0MUYNd4+1n48meQGwGnj+eIYkSZKkjTXqPXefSbIT8IfAjQyelP3o2EYlSZKkjTLqmbuvAj+oqr8AzgKuBT69sZ0m+c0ktya5JcknkjwtyR5JrkuyPMklSbZtbbdry8vb+vlD+3lPq9+e5KCh+uJWW57kpI0dpyRJ0lwzarj7vap6JMkvMPhsu48BZ29Mh0l2BX4dWFhVewPbAEcw+Hqz06vqJcCDwHFtk+MY3Ov3EuD01o4ke7bt9gIWAx9Jsk2SbRgE0IOBPYEjW1tJkqTujRruftB+HgJ8tKo+C2y7Cf3OA7ZPMg94OnAvg9B4WVt/AXBoe7+kLdPWH5gkrX5xVT1eVXcCy4H92mt5Vd1RVU8AF7e2kiRJ3Rv1nruVSf4U+GXgg0m2Y/RguJaqWpnkQ8DXgX8F/gb4EvBQVa1uzVYAu7b3uwL3tG1XJ3kYeE6rXzu06+Ft7plS33+6sSQ5HjgeYGJigsnJyY05pJFNbA8n7rN6rdq4+9Tmt2rVKudtjnMO5z7ncO5zDsdn1HB3OINLnx+qqoeSPB/47Y3pMMnODM6k7QE8BHyy7XvGVdU5wDkACxcurEWLFo21vzMvupzTlq39K7/rqPH2qc1vcnKScf+taLycw7nPOZz7nMPxGfXrxx4FPjW0fC+DS6kb45eAO6vq2wBJPgX8PLBTknnt7N1uwMrWfiWwO7CiXcbdEbh/qL7G8DbrqkuSJHVtoy6tbqKvAwckeXq7d+5A4DbgGuCw1uYY4PL2fmlbpq3/fFVVqx/RnqbdA1gAfBG4HljQnr7dlsFDF0tn4LgkSZJm3cjfLbu5VNV1SS5j8Hl5q4GbGFwa/SxwcZL3t9q5bZNzgY8nWQ48wCCsUVW3JrmUQTBcDZxQVT8ASPJ24EoGT+KeV1W3ztTxSZIkzaYZD3cAVXUycPKU8h0MnnSd2vYxBt9lO91+TgVOnaZ+BXDFpo9UkiRpbpmNy7KSJEkaE8OdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHZiXcJdkpyWVJvprkK0lekeTZSa5K8rX2c+fWNknOSLI8yc1J9h3azzGt/deSHDNUf3mSZW2bM5JkNo5TkiRpps3Wmbs/Av66qv4d8LPAV4CTgKuragFwdVsGOBhY0F7HA2cDJHk2cDKwP7AfcPKaQNjavGVou8UzcEySJEmzbsbDXZIdgV8EzgWoqieq6iFgCXBBa3YBcGh7vwS4sAauBXZK8nzgIOCqqnqgqh4ErgIWt3XPqqprq6qAC4f2JUmS1LV5s9DnHsC3gT9L8rPAl4DfACaq6t7W5pvARHu/K3DP0PYrWm199RXT1J8kyfEMzgYyMTHB5OTkRh/UKCa2hxP3Wb1Wbdx9avNbtWqV8zbHOYdzn3M49zmH4zMb4W4esC/wjqq6Lskf8W+XYAGoqkpS4x5IVZ0DnAOwcOHCWrRo0Vj7O/Oiyzlt2dq/8ruOGm+f2vwmJycZ99+Kxss5nPucw7nPORyf2bjnbgWwoqqua8uXMQh732qXVGk/72vrVwK7D22/W6utr77bNHVJkqTuzXi4q6pvAvck+alWOhC4DVgKrHni9Rjg8vZ+KXB0e2r2AODhdvn2SuDVSXZuD1K8GriyrftukgPaU7JHD+1LkiSpa7NxWRbgHcBFSbYF7gDexCBoXprkOOBu4PDW9grgNcBy4NHWlqp6IMn7gOtbu1Oq6oH2/m3A+cD2wOfaS5IkqXuzEu6q6p+AhdOsOnCatgWcsI79nAecN039BmDvTRymJEnSnOM3VEiSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1JFZC3dJtklyU5K/ast7JLkuyfIklyTZttW3a8vL2/r5Q/t4T6vfnuSgofriVlue5KSZPjZJkqTZMptn7n4D+MrQ8geB06vqJcCDwHGtfhzwYKuf3tqRZE/gCGAvYDHwkRYYtwHOAg4G9gSObG0lSZK6NyvhLsluwCHAx9pygFcBl7UmFwCHtvdL2jJt/YGt/RLg4qp6vKruBJYD+7XX8qq6o6qeAC5ubSVJkro3b5b6/TDwLmCHtvwc4KGqWt2WVwC7tve7AvcAVNXqJA+39rsC1w7tc3ibe6bU959uEEmOB44HmJiYYHJycuOPaAQT28OJ+6xeqzbuPrX5rVq1ynmb45zDuc85nPucw/GZ8XCX5FeA+6rqS0kWzXT/w6rqHOAcgIULF9aiReMdzpkXXc5py9b+ld911Hj71OY3OTnJuP9WNF7O4dznHM59zuH4zMaZu58HXpvkNcDTgGcBfwTslGReO3u3G7CytV8J7A6sSDIP2BG4f6i+xvA266pLkiR1bcbvuauq91TVblU1n8EDEZ+vqqOAa4DDWrNjgMvb+6Vtmbb+81VVrX5Ee5p2D2AB8EXgemBBe/p229bH0hk4NEmSpFk3W/fcTefdwMVJ3g/cBJzb6ucCH0+yHHiAQVijqm5NcilwG7AaOKGqfgCQ5O3AlcA2wHlVdeuMHokkSdIsmdVwV1WTwGR7fweDJ12ntnkMeN06tj8VOHWa+hXAFZtxqJIkSXOC31AhSZLUEcOdJElSRwx3kiRJHdmSHqjYKs0/6bNPqt31gUNmYSSSJKkHnrmTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqVqwDO8AAAqaSURBVCOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjsyb7QHoyeaf9Nlp63d94JAZHokkSZprPHMnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEDzGeQ6b7cGM/2FiSJA3zzJ0kSVJHDHeSJEkdMdxJkiR1xHAnSZLUER+omON8yEKSJA2b8TN3SXZPck2S25LcmuQ3Wv3ZSa5K8rX2c+dWT5IzkixPcnOSfYf2dUxr/7UkxwzVX55kWdvmjCSZ6eOUJEmaDbNxWXY1cGJV7QkcAJyQZE/gJODqqloAXN2WAQ4GFrTX8cDZMAiDwMnA/sB+wMlrAmFr85ah7RbPwHFJkiTNuhkPd1V1b1Xd2N4/AnwF2BVYAlzQml0AHNreLwEurIFrgZ2SPB84CLiqqh6oqgeBq4DFbd2zquraqirgwqF9SZIkdW1W77lLMh94GXAdMFFV97ZV3wQm2vtdgXuGNlvRauurr5imPl3/xzM4G8jExASTk5MbfSyjmNgeTtxn9Vj7ADjzosufVNtn1x3H3u/WYtWqVWP/W9F4OYdzn3M49zmH4zNr4S7JM4G/AP57VX13+La4qqokNe4xVNU5wDkACxcurEWLFo21vzMvupzTls3Or/yuoxbNSr89mpycZNx/Kxov53Ducw7nPudwfGblo1CS/DiDYHdRVX2qlb/VLqnSft7X6iuB3Yc2363V1lffbZq6JElS92b8NFJ7cvVc4CtV9b+HVi0FjgE+0H5ePlR/e5KLGTw88XBV3ZvkSuB/Dj1E8WrgPVX1QJLvJjmAweXeo4Ezx35gWzg/MkWSpK3DbFwj/HngjcCyJP/Uar/DINRdmuQ44G7g8LbuCuA1wHLgUeBNAC3EvQ+4vrU7paoeaO/fBpwPbA98rr0kSZK6N+Phrqr+DljX584dOE37Ak5Yx77OA86bpn4DsPcmDHOr4Nk8SZL649ePSZIkdcRwJ0mS1BG/W1Zr8VKtJElzm2fuJEmSOuKZOz0lz+ZJkjR3GO60UQx8kiRtmbwsK0mS1BHP3Gmz8WyeJEmzz3CnsZou8E3HEChJ0ubhZVlJkqSOeOZOWwTP8EmStHkY7jSnrCsEGvokSRow3KkLo575m47BUJLUE8Odtno+5StJ6onhTprG+s4EnrjPao7dwDOFhkVJ0kwx3EkzYFMuG4/KAClJAsOd1I1NDZDThUMvWUvS3GO4kwSMHg4398MrBkhJ2rwMd5JmzEwEyFFszH2TG2vUQDsTDM3S1sFwJ0ljNFtBbjpb0lg21fmLnzHbQ5C2WIY7SdKcs2zlwyOdffVspbZGfresJElSRzxzJ0nqlg/saGtkuJMkbVUMfOqd4U6StNVb18Mmhj7NRd5zJ0mS1BHP3EmStA5ewtVcZLiTJGkDGPi0pTPcSZK0iQx82pIY7iRJGgMDn2aL4U6SpBli4NNMMNxJkjSLRv3OX0OgRmW4kyRpDvCsn0ZluJMkaY7yrJ+m0224S7IY+CNgG+BjVfWBWR6SJEmzYtQQuCEMjFuuLsNdkm2As4BfBlYA1ydZWlW3ze7IJEnqw6YERoPheHUZ7oD9gOVVdQdAkouBJYDhTpKkWTb/pM9y4j6rOXYMZxTXZ2sJlamq2R7DZpfkMGBxVb25Lb8R2L+q3j6l3fHA8W3xp4Dbxzy0XYDvjLkPjZ/zOPc5h3Ofczj3OYeb5kVV9dzpVvR65m4kVXUOcM5M9ZfkhqpaOFP9aTycx7nPOZz7nMO5zzkcnx+b7QGMyUpg96Hl3VpNkiSpa72Gu+uBBUn2SLItcASwdJbHJEmSNHZdXpatqtVJ3g5cyeCjUM6rqltneVgwg5eANVbO49znHM59zuHc5xyOSZcPVEiSJG2ter0sK0mStFUy3EmSJHXEcDcGSRYnuT3J8iQnTbN+uySXtPXXJZk/86PU+owwh7+V5LYkNye5OsmLZmOcWrenmsOhdv8lSSXxIxm2QKPMY5LD23+Ptyb585keo9ZvhH9PX5jkmiQ3tX9TXzMb4+yJ99xtZu2rz/6Zoa8+A44c/uqzJG8Dfqaq3prkCOA/V9WvzsqA9SQjzuErgeuq6tEkvwYscg63HKPMYWu3A/BZYFvg7VV1w0yPVes24n+LC4BLgVdV1YNJnldV983KgPUkI87hOcBNVXV2kj2BK6pq/myMtxeeudv8fvTVZ1X1BLDmq8+GLQEuaO8vAw5Mkhkco9bvKeewqq6pqkfb4rUMPktRW45R/jsEeB/wQeCxmRycRjbKPL4FOKuqHgQw2G1xRpnDAp7V3u8IfGMGx9clw93mtytwz9Dyilabtk1VrQYeBp4zI6PTKEaZw2HHAZ8b64i0oZ5yDpPsC+xeVTP75ZbaEKP8t/hS4KVJ/j7JtUkWz9joNIpR5vD3gTckWQFcAbxjZobWry4/506aKUneACwE/uNsj0WjS/JjwP8Gjp3loWjTzQMWAIsYnEH/QpJ9quqhWR2VNsSRwPlVdVqSVwAfT7J3Vf1wtgc2V3nmbvMb5avPftQmyTwGp6Hvn5HRaRQjfX1dkl8C3gu8tqoen6GxaTRPNYc7AHsDk0nuAg4AlvpQxRZnlP8WVwBLq+r7VXUng/u7FszQ+PTURpnD4xjcN0lV/SPwNGCXGRldpwx3m98oX322FDimvT8M+Hz5ZMuW5CnnMMnLgD9lEOy8x2fLs945rKqHq2qXqprfbty+lsFc+kDFlmWUf08/zeCsHUl2YXCZ9o6ZHKTWa5Q5/DpwIECSn2YQ7r49o6PsjOFuM2v30K356rOvAJdW1a1JTkny2tbsXOA5SZYDvwWs82MaNPNGnMM/BJ4JfDLJPyXxu4u3ICPOobZwI87jlcD9SW4DrgF+u6q8ErKFGHEOTwTekuTLwCeAYz3hsWn8KBRJkqSOeOZOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0najJKsmu0xSNq6Ge4kSZI6YriTpPVI8oEkJwwt/36S301ydZIbkyxLsmSa7RYl+auh5T9Ocmx7//Ik/zfJl5JcmeT5rf7rSW5LcnOSi2fg8CR1aN5sD0CStnCXAB8GzmrLhwMHAWdU1XfbV15dm2TpKJ+qn+THgTOBJVX17SS/CpwK/FcG31azR1U9nmSncRyMpP4Z7iRpParqpiTPS/IC4LnAg8A3gdOT/CLwQ2BXYKLVn8pPAXsDVyUB2Aa4t627GbgoyacZfGeqJG0ww50kPbVPAocBP8HgTN5RDILey6vq+0nuYvBl58NWs/atL2vWB7i1ql4xTT+HAL8I/CfgvUn2ad/NKUkj8547SXpqlwBHMAh4nwR2BO5rwe6VwIum2eZuYM8k27VLrAe2+u3Ac5O8AgaXaZPsleTHgN2r6hrg3a2PZ471qCR1yTN3kvQUqurWJDsAK6vq3iQXAZ9Jsgy4AfjqNNvck+RS4BbgTuCmVn8iyWHAGUl2ZPDv8IeBfwb+T6uFwT19D83E8UnqS0a4/1eSJElzhJdlJUmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkj/z86eRmvxc7cJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the output distribution (V/K).\n",
    "plt.figure(figsize=(10,7))\n",
    "h = plt.hist(bs_dataset[:,-1],bins=100)\n",
    "plt.xlabel(\"values\")\n",
    "plt.ylabel(\"sample\")\n",
    "plt.title(\"Call price histogram (V/K)\" )\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuOxrTEIZFMY"
   },
   "source": [
    "**Remark:** The majority of option prices are below 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNTO4M51R3cd"
   },
   "source": [
    "### Defining class BS-ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc26TY2fYdoa"
   },
   "outputs": [],
   "source": [
    "class BS_ANN(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Initilize BS-ANN netwwork with four hidden layers. \n",
    "    Same architecture as the one used in table 2.\n",
    "    Weight are initialized using Glorot_uniform also known as Xavier_uniform.\n",
    "    \"\"\"\n",
    "    super(BS_ANN, self).__init__()\n",
    "    # 4 layers\n",
    "    self.fc1 = nn.Linear(4, 400)\n",
    "    self.fc2 = nn.Linear(400, 400)\n",
    "    self.fc3 = nn.Linear(400, 400)\n",
    "    self.fc4 = nn.Linear(400, 400)\n",
    "    self.fc5 = nn.Linear(400, 1)\n",
    "    # initilizing the layer's weights\n",
    "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Foward pass of the network. \n",
    "    Relu activation are used and a linear activation is used in the final layer.\n",
    "    x: input torch tensor (CUDA or CPU)\n",
    "    \"\"\"\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = F.relu(self.fc4(x))\n",
    "    x = self.fc5(x)\n",
    "    return x.flatten()\n",
    "\n",
    "  def train_model(self, args, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training over train_loader batches for one epoch.\n",
    "    Returns average training loss.\n",
    "    args: training parameters.\n",
    "    device: CUDA or CPU.\n",
    "    train_loader: train data loader.\n",
    "    optimizer: gradient descent optmizer.\n",
    "    epoch: current epoch.\n",
    "    \"\"\"\n",
    "    # train mode\n",
    "    self.train()\n",
    "    train_loss = 0\n",
    "    # loop over data loader\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "      # extracting the inputs and target\n",
    "      data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "      # zeroing the gradients\n",
    "      optimizer.zero_grad()\n",
    "      # foward pass\n",
    "      output = self(data)\n",
    "      # computing the loss\n",
    "      loss = F.mse_loss(output, target)\n",
    "      train_loss += loss.item() *data.shape[0]\n",
    "      # back propagation\n",
    "      loss.backward()\n",
    "      # updating the optimizer step\n",
    "      optimizer.step()\n",
    "      # printing batch information\n",
    "      if batch_idx % args[\"log_interval\"] == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlog Loss: {:.6f}'.format(\n",
    "            epoch, int(batch_idx / len(train_loader)*len(train_loader.dataset)), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.log(loss.item())))\n",
    "        if args[\"dry_run\"]:\n",
    "          break\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average log loss: {:.4f}\\n'.format(np.log(train_loss))) \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "  def test_model(self, device, test_loader):\n",
    "    \"\"\"\n",
    "    Testing over test_loader batches for one epoch. \n",
    "    Returns average test loss.\n",
    "    device: CUDA or CPU.\n",
    "    test_loader: test data loader.\n",
    "    \"\"\"\n",
    "    # evaluation mode\n",
    "    self.eval()\n",
    "    test_loss = 0\n",
    "    # freezing the gradients\n",
    "    with torch.no_grad():\n",
    "      # looping over the test laoder\n",
    "      for data in test_loader:\n",
    "        # extracting the inputs and target\n",
    "        data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "        # foward pass\n",
    "        output = self(data)\n",
    "        #computing the loss\n",
    "        test_loss += F.mse_loss(output, target, reduction='sum').item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(np.log(test_loss))) \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_OVhr6nR-Iz"
   },
   "source": [
    "### Search of optimal Learning rate interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSYLHZd9k0cO",
    "outputId": "cce1c0ff-4a34-4244-f1a9-3f12923cdb50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ INPUT ------\n",
      "S/K.......... [0.40 1.60]\n",
      "Tau.......... [0.20 1.10]\n",
      "r............ [0.02 0.10]\n",
      "Sigma........ [0.01 1.00]\n",
      "------ OUTPUT ------\n",
      "V/K.......... [0.00 0.88]\n",
      "Current Learning rate 1e-09\n",
      "Train Epoch: 1 [0/9000 (0%)]\tlog Loss: -1.269020\n",
      "\n",
      "Train set: Average log loss: -1.2404\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2383\n",
      "\n",
      "Current Learning rate 1.122667773510816e-09\n",
      "Train Epoch: 2 [0/9000 (0%)]\tlog Loss: -1.269033\n",
      "\n",
      "Train set: Average log loss: -1.2404\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2383\n",
      "\n",
      "Current Learning rate 1.2603829296797274e-09\n",
      "Train Epoch: 3 [0/9000 (0%)]\tlog Loss: -1.269047\n",
      "\n",
      "Train set: Average log loss: -1.2404\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2383\n",
      "\n",
      "Current Learning rate 1.4149912974345788e-09\n",
      "Train Epoch: 4 [0/9000 (0%)]\tlog Loss: -1.269062\n",
      "\n",
      "Train set: Average log loss: -1.2405\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2384\n",
      "\n",
      "Current Learning rate 1.5885651294280526e-09\n",
      "Train Epoch: 5 [0/9000 (0%)]\tlog Loss: -1.269079\n",
      "\n",
      "Train set: Average log loss: -1.2405\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2384\n",
      "\n",
      "Current Learning rate 1.7834308769319057e-09\n",
      "Train Epoch: 6 [0/9000 (0%)]\tlog Loss: -1.269096\n",
      "\n",
      "Train set: Average log loss: -1.2405\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2384\n",
      "\n",
      "Current Learning rate 2.0022003718155843e-09\n",
      "Train Epoch: 7 [0/9000 (0%)]\tlog Loss: -1.269114\n",
      "\n",
      "Train set: Average log loss: -1.2405\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2384\n",
      "\n",
      "Current Learning rate 2.24780583354873e-09\n",
      "Train Epoch: 8 [0/9000 (0%)]\tlog Loss: -1.269168\n",
      "\n",
      "Train set: Average log loss: -1.2406\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2385\n",
      "\n",
      "Current Learning rate 2.5235391704347657e-09\n",
      "Train Epoch: 9 [0/9000 (0%)]\tlog Loss: -1.269224\n",
      "\n",
      "Train set: Average log loss: -1.2406\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2386\n",
      "\n",
      "Current Learning rate 2.83309610183933e-09\n",
      "Train Epoch: 10 [0/9000 (0%)]\tlog Loss: -1.269282\n",
      "\n",
      "Train set: Average log loss: -1.2407\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2386\n",
      "\n",
      "Current Learning rate 3.180625692794119e-09\n",
      "Train Epoch: 11 [0/9000 (0%)]\tlog Loss: -1.269349\n",
      "\n",
      "Train set: Average log loss: -1.2408\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2387\n",
      "\n",
      "Current Learning rate 3.57078596490047e-09\n",
      "Train Epoch: 12 [0/9000 (0%)]\tlog Loss: -1.269418\n",
      "\n",
      "Train set: Average log loss: -1.2408\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2388\n",
      "\n",
      "Current Learning rate 4.008806328898465e-09\n",
      "Train Epoch: 13 [0/9000 (0%)]\tlog Loss: -1.269490\n",
      "\n",
      "Train set: Average log loss: -1.2409\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2389\n",
      "\n",
      "Current Learning rate 4.500557675700507e-09\n",
      "Train Epoch: 14 [0/9000 (0%)]\tlog Loss: -1.269619\n",
      "\n",
      "Train set: Average log loss: -1.2411\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2390\n",
      "\n",
      "Current Learning rate 5.05263106533568e-09\n",
      "Train Epoch: 15 [0/9000 (0%)]\tlog Loss: -1.269751\n",
      "\n",
      "Train set: Average log loss: -1.2412\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2392\n",
      "\n",
      "Current Learning rate 5.6724260684919895e-09\n",
      "Train Epoch: 16 [0/9000 (0%)]\tlog Loss: -1.269894\n",
      "\n",
      "Train set: Average log loss: -1.2414\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2393\n",
      "\n",
      "Current Learning rate 6.368249944718586e-09\n",
      "Train Epoch: 17 [0/9000 (0%)]\tlog Loss: -1.270076\n",
      "\n",
      "Train set: Average log loss: -1.2415\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2395\n",
      "\n",
      "Current Learning rate 7.1494289865975915e-09\n",
      "Train Epoch: 18 [0/9000 (0%)]\tlog Loss: -1.270263\n",
      "\n",
      "Train set: Average log loss: -1.2417\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2397\n",
      "\n",
      "Current Learning rate 8.026433522257173e-09\n",
      "Train Epoch: 19 [0/9000 (0%)]\tlog Loss: -1.270462\n",
      "\n",
      "Train set: Average log loss: -1.2419\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2399\n",
      "\n",
      "Current Learning rate 9.011018251665037e-09\n",
      "Train Epoch: 20 [0/9000 (0%)]\tlog Loss: -1.270666\n",
      "\n",
      "Train set: Average log loss: -1.2421\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2401\n",
      "\n",
      "Current Learning rate 1.011637979766207e-08\n",
      "Train Epoch: 21 [0/9000 (0%)]\tlog Loss: -1.270884\n",
      "\n",
      "Train set: Average log loss: -1.2424\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2404\n",
      "\n",
      "Current Learning rate 1.1357333583431052e-08\n",
      "Train Epoch: 22 [0/9000 (0%)]\tlog Loss: -1.271144\n",
      "\n",
      "Train set: Average log loss: -1.2427\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2407\n",
      "\n",
      "Current Learning rate 1.2750512407130128e-08\n",
      "Train Epoch: 23 [0/9000 (0%)]\tlog Loss: -1.271473\n",
      "\n",
      "Train set: Average log loss: -1.2430\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2411\n",
      "\n",
      "Current Learning rate 1.4314589375234786e-08\n",
      "Train Epoch: 24 [0/9000 (0%)]\tlog Loss: -1.271819\n",
      "\n",
      "Train set: Average log loss: -1.2434\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2415\n",
      "\n",
      "Current Learning rate 1.6070528182616383e-08\n",
      "Train Epoch: 25 [0/9000 (0%)]\tlog Loss: -1.272216\n",
      "\n",
      "Train set: Average log loss: -1.2438\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2419\n",
      "\n",
      "Current Learning rate 1.8041864093920717e-08\n",
      "Train Epoch: 26 [0/9000 (0%)]\tlog Loss: -1.272634\n",
      "\n",
      "Train set: Average log loss: -1.2442\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2423\n",
      "\n",
      "Current Learning rate 2.0255019392306666e-08\n",
      "Train Epoch: 27 [0/9000 (0%)]\tlog Loss: -1.273106\n",
      "\n",
      "Train set: Average log loss: -1.2447\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2429\n",
      "\n",
      "Current Learning rate 2.2739657523579275e-08\n",
      "Train Epoch: 28 [0/9000 (0%)]\tlog Loss: -1.273655\n",
      "\n",
      "Train set: Average log loss: -1.2453\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2435\n",
      "\n",
      "Current Learning rate 2.5529080682395165e-08\n",
      "Train Epoch: 29 [0/9000 (0%)]\tlog Loss: -1.274261\n",
      "\n",
      "Train set: Average log loss: -1.2459\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2441\n",
      "\n",
      "Current Learning rate 2.8660676169482502e-08\n",
      "Train Epoch: 30 [0/9000 (0%)]\tlog Loss: -1.274934\n",
      "\n",
      "Train set: Average log loss: -1.2466\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2449\n",
      "\n",
      "Current Learning rate 3.217641750250735e-08\n",
      "Train Epoch: 31 [0/9000 (0%)]\tlog Loss: -1.275725\n",
      "\n",
      "Train set: Average log loss: -1.2474\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2458\n",
      "\n",
      "Current Learning rate 3.6123426997094305e-08\n",
      "Train Epoch: 32 [0/9000 (0%)]\tlog Loss: -1.276587\n",
      "\n",
      "Train set: Average log loss: -1.2483\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2467\n",
      "\n",
      "Current Learning rate 4.055460735840828e-08\n",
      "Train Epoch: 33 [0/9000 (0%)]\tlog Loss: -1.277579\n",
      "\n",
      "Train set: Average log loss: -1.2493\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2478\n",
      "\n",
      "Current Learning rate 4.5529350748669475e-08\n",
      "Train Epoch: 34 [0/9000 (0%)]\tlog Loss: -1.278655\n",
      "\n",
      "Train set: Average log loss: -1.2505\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2490\n",
      "\n",
      "Current Learning rate 5.111433483440165e-08\n",
      "Train Epoch: 35 [0/9000 (0%)]\tlog Loss: -1.279870\n",
      "\n",
      "Train set: Average log loss: -1.2517\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2504\n",
      "\n",
      "Current Learning rate 5.7384416483023926e-08\n",
      "Train Epoch: 36 [0/9000 (0%)]\tlog Loss: -1.281266\n",
      "\n",
      "Train set: Average log loss: -1.2532\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2519\n",
      "\n",
      "Current Learning rate 6.44236350872137e-08\n",
      "Train Epoch: 37 [0/9000 (0%)]\tlog Loss: -1.282822\n",
      "\n",
      "Train set: Average log loss: -1.2548\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2536\n",
      "\n",
      "Current Learning rate 7.232633896483534e-08\n",
      "Train Epoch: 38 [0/9000 (0%)]\tlog Loss: -1.284568\n",
      "\n",
      "Train set: Average log loss: -1.2566\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2556\n",
      "\n",
      "Current Learning rate 8.11984499318401e-08\n",
      "Train Epoch: 39 [0/9000 (0%)]\tlog Loss: -1.286524\n",
      "\n",
      "Train set: Average log loss: -1.2587\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2577\n",
      "\n",
      "Current Learning rate 9.115888299750819e-08\n",
      "Train Epoch: 40 [0/9000 (0%)]\tlog Loss: -1.288727\n",
      "\n",
      "Train set: Average log loss: -1.2609\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2601\n",
      "\n",
      "Current Learning rate 1.0234114021054527e-07\n",
      "Train Epoch: 41 [0/9000 (0%)]\tlog Loss: -1.291167\n",
      "\n",
      "Train set: Average log loss: -1.2635\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2629\n",
      "\n",
      "Current Learning rate 1.1489510001873085e-07\n",
      "Train Epoch: 42 [0/9000 (0%)]\tlog Loss: -1.293927\n",
      "\n",
      "Train set: Average log loss: -1.2664\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2659\n",
      "\n",
      "Current Learning rate 1.289890261253308e-07\n",
      "Train Epoch: 43 [0/9000 (0%)]\tlog Loss: -1.297005\n",
      "\n",
      "Train set: Average log loss: -1.2696\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2693\n",
      "\n",
      "Current Learning rate 1.448118227674533e-07\n",
      "Train Epoch: 44 [0/9000 (0%)]\tlog Loss: -1.300463\n",
      "\n",
      "Train set: Average log loss: -1.2732\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2731\n",
      "\n",
      "Current Learning rate 1.6257556664437934e-07\n",
      "Train Epoch: 45 [0/9000 (0%)]\tlog Loss: -1.304343\n",
      "\n",
      "Train set: Average log loss: -1.2772\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2774\n",
      "\n",
      "Current Learning rate 1.8251834943190423e-07\n",
      "Train Epoch: 46 [0/9000 (0%)]\tlog Loss: -1.308704\n",
      "\n",
      "Train set: Average log loss: -1.2817\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2823\n",
      "\n",
      "Current Learning rate 2.049074689815846e-07\n",
      "Train Epoch: 47 [0/9000 (0%)]\tlog Loss: -1.313599\n",
      "\n",
      "Train set: Average log loss: -1.2868\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2877\n",
      "\n",
      "Current Learning rate 2.300430119772917e-07\n",
      "Train Epoch: 48 [0/9000 (0%)]\tlog Loss: -1.319096\n",
      "\n",
      "Train set: Average log loss: -1.2926\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.2937\n",
      "\n",
      "Current Learning rate 2.582618760682675e-07\n",
      "Train Epoch: 49 [0/9000 (0%)]\tlog Loss: -1.325258\n",
      "\n",
      "Train set: Average log loss: -1.2990\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3006\n",
      "\n",
      "Current Learning rate 2.899422853882875e-07\n",
      "Train Epoch: 50 [0/9000 (0%)]\tlog Loss: -1.332180\n",
      "\n",
      "Train set: Average log loss: -1.3062\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3082\n",
      "\n",
      "Current Learning rate 3.255088599835056e-07\n",
      "Train Epoch: 51 [0/9000 (0%)]\tlog Loss: -1.339946\n",
      "\n",
      "Train set: Average log loss: -1.3143\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3168\n",
      "\n",
      "Current Learning rate 3.654383070957262e-07\n",
      "Train Epoch: 52 [0/9000 (0%)]\tlog Loss: -1.348657\n",
      "\n",
      "Train set: Average log loss: -1.3233\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3264\n",
      "\n",
      "Current Learning rate 4.1026581058271904e-07\n",
      "Train Epoch: 53 [0/9000 (0%)]\tlog Loss: -1.358433\n",
      "\n",
      "Train set: Average log loss: -1.3335\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3373\n",
      "\n",
      "Current Learning rate 4.605922041145113e-07\n",
      "Train Epoch: 54 [0/9000 (0%)]\tlog Loss: -1.369404\n",
      "\n",
      "Train set: Average log loss: -1.3449\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3494\n",
      "\n",
      "Current Learning rate 5.170920242896755e-07\n",
      "Train Epoch: 55 [0/9000 (0%)]\tlog Loss: -1.381718\n",
      "\n",
      "Train set: Average log loss: -1.3577\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3630\n",
      "\n",
      "Current Learning rate 5.805225516094908e-07\n",
      "Train Epoch: 56 [0/9000 (0%)]\tlog Loss: -1.395537\n",
      "\n",
      "Train set: Average log loss: -1.3721\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3783\n",
      "\n",
      "Current Learning rate 6.517339604882421e-07\n",
      "Train Epoch: 57 [0/9000 (0%)]\tlog Loss: -1.411057\n",
      "\n",
      "Train set: Average log loss: -1.3882\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.3955\n",
      "\n",
      "Current Learning rate 7.316807143427207e-07\n",
      "Train Epoch: 58 [0/9000 (0%)]\tlog Loss: -1.428494\n",
      "\n",
      "Train set: Average log loss: -1.4064\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.4148\n",
      "\n",
      "Current Learning rate 8.214343584919422e-07\n",
      "Train Epoch: 59 [0/9000 (0%)]\tlog Loss: -1.448112\n",
      "\n",
      "Train set: Average log loss: -1.4268\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.4366\n",
      "\n",
      "Current Learning rate 9.221978823334341e-07\n",
      "Train Epoch: 60 [0/9000 (0%)]\tlog Loss: -1.470198\n",
      "\n",
      "Train set: Average log loss: -1.4498\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.4611\n",
      "\n",
      "Current Learning rate 1.0353218432956615e-06\n",
      "Train Epoch: 61 [0/9000 (0%)]\tlog Loss: -1.495098\n",
      "\n",
      "Train set: Average log loss: -1.4757\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.4887\n",
      "\n",
      "Current Learning rate 1.1623224686798541e-06\n",
      "Train Epoch: 62 [0/9000 (0%)]\tlog Loss: -1.523187\n",
      "\n",
      "Train set: Average log loss: -1.5050\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.5199\n",
      "\n",
      "Current Learning rate 1.3049019780144016e-06\n",
      "Train Epoch: 63 [0/9000 (0%)]\tlog Loss: -1.554880\n",
      "\n",
      "Train set: Average log loss: -1.5379\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.5551\n",
      "\n",
      "Current Learning rate 1.4649713983072878e-06\n",
      "Train Epoch: 64 [0/9000 (0%)]\tlog Loss: -1.590608\n",
      "\n",
      "Train set: Average log loss: -1.5751\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.5947\n",
      "\n",
      "Current Learning rate 1.6446761779946627e-06\n",
      "Train Epoch: 65 [0/9000 (0%)]\tlog Loss: -1.630900\n",
      "\n",
      "Train set: Average log loss: -1.6170\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.6394\n",
      "\n",
      "Current Learning rate 1.8464249428955464e-06\n",
      "Train Epoch: 66 [0/9000 (0%)]\tlog Loss: -1.676330\n",
      "\n",
      "Train set: Average log loss: -1.6642\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.6898\n",
      "\n",
      "Current Learning rate 2.0729217795953697e-06\n",
      "Train Epoch: 67 [0/9000 (0%)]\tlog Loss: -1.727514\n",
      "\n",
      "Train set: Average log loss: -1.7175\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.7465\n",
      "\n",
      "Current Learning rate 2.327202478960412e-06\n",
      "Train Epoch: 68 [0/9000 (0%)]\tlog Loss: -1.785246\n",
      "\n",
      "Train set: Average log loss: -1.7776\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.8107\n",
      "\n",
      "Current Learning rate 2.6126752255633263e-06\n",
      "Train Epoch: 69 [0/9000 (0%)]\tlog Loss: -1.850502\n",
      "\n",
      "Train set: Average log loss: -1.8457\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.8836\n",
      "\n",
      "Current Learning rate 2.9331662783900485e-06\n",
      "Train Epoch: 70 [0/9000 (0%)]\tlog Loss: -1.924652\n",
      "\n",
      "Train set: Average log loss: -1.9233\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.9671\n",
      "\n",
      "Current Learning rate 3.292971255097148e-06\n",
      "Train Epoch: 71 [0/9000 (0%)]\tlog Loss: -2.009288\n",
      "\n",
      "Train set: Average log loss: -2.0120\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.0625\n",
      "\n",
      "Current Learning rate 3.696912707195032e-06\n",
      "Train Epoch: 72 [0/9000 (0%)]\tlog Loss: -2.105913\n",
      "\n",
      "Train set: Average log loss: -2.1130\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.1707\n",
      "\n",
      "Current Learning rate 4.150404757850472e-06\n",
      "Train Epoch: 73 [0/9000 (0%)]\tlog Loss: -2.215457\n",
      "\n",
      "Train set: Average log loss: -2.2268\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.2913\n",
      "\n",
      "Current Learning rate 4.659525668664687e-06\n",
      "Train Epoch: 74 [0/9000 (0%)]\tlog Loss: -2.337574\n",
      "\n",
      "Train set: Average log loss: -2.3525\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.4228\n",
      "\n",
      "Current Learning rate 5.2310993080562585e-06\n",
      "Train Epoch: 75 [0/9000 (0%)]\tlog Loss: -2.470298\n",
      "\n",
      "Train set: Average log loss: -2.4884\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.5639\n",
      "\n",
      "Current Learning rate 5.872786613189489e-06\n",
      "Train Epoch: 76 [0/9000 (0%)]\tlog Loss: -2.611865\n",
      "\n",
      "Train set: Average log loss: -2.6332\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.7124\n",
      "\n",
      "Current Learning rate 6.593188271333541e-06\n",
      "Train Epoch: 77 [0/9000 (0%)]\tlog Loss: -2.760559\n",
      "\n",
      "Train set: Average log loss: -2.7833\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.8620\n",
      "\n",
      "Current Learning rate 7.4019599969156525e-06\n",
      "Train Epoch: 78 [0/9000 (0%)]\tlog Loss: -2.909497\n",
      "\n",
      "Train set: Average log loss: -2.9306\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0029\n",
      "\n",
      "Current Learning rate 8.309941949353388e-06\n",
      "Train Epoch: 79 [0/9000 (0%)]\tlog Loss: -3.048487\n",
      "\n",
      "Train set: Average log loss: -3.0660\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.1277\n",
      "\n",
      "Current Learning rate 9.329304026284696e-06\n",
      "Train Epoch: 80 [0/9000 (0%)]\tlog Loss: -3.170104\n",
      "\n",
      "Train set: Average log loss: -3.1828\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2330\n",
      "\n",
      "Current Learning rate 1.0473708979594487e-05\n",
      "Train Epoch: 81 [0/9000 (0%)]\tlog Loss: -3.272113\n",
      "\n",
      "Train set: Average log loss: -3.2828\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.3247\n",
      "\n",
      "Current Learning rate 1.1758495540521557e-05\n",
      "Train Epoch: 82 [0/9000 (0%)]\tlog Loss: -3.361360\n",
      "\n",
      "Train set: Average log loss: -3.3732\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.4126\n",
      "\n",
      "Current Learning rate 1.3200884008314168e-05\n",
      "Train Epoch: 83 [0/9000 (0%)]\tlog Loss: -3.448238\n",
      "\n",
      "Train set: Average log loss: -3.4633\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.5051\n",
      "\n",
      "Current Learning rate 1.482020705798857e-05\n",
      "Train Epoch: 84 [0/9000 (0%)]\tlog Loss: -3.542081\n",
      "\n",
      "Train set: Average log loss: -3.5609\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.6076\n",
      "\n",
      "Current Learning rate 1.6638168860761273e-05\n",
      "Train Epoch: 85 [0/9000 (0%)]\tlog Loss: -3.646675\n",
      "\n",
      "Train set: Average log loss: -3.6696\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.7213\n",
      "\n",
      "Current Learning rate 1.867913599020781e-05\n",
      "Train Epoch: 86 [0/9000 (0%)]\tlog Loss: -3.761182\n",
      "\n",
      "Train set: Average log loss: -3.7924\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.8562\n",
      "\n",
      "Current Learning rate 2.0970464013232308e-05\n",
      "Train Epoch: 87 [0/9000 (0%)]\tlog Loss: -3.896781\n",
      "\n",
      "Train set: Average log loss: -3.9411\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.0200\n",
      "\n",
      "Current Learning rate 2.3542864143224153e-05\n",
      "Train Epoch: 88 [0/9000 (0%)]\tlog Loss: -4.061013\n",
      "\n",
      "Train set: Average log loss: -4.1190\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.2130\n",
      "\n",
      "Current Learning rate 2.643081486974103e-05\n",
      "Train Epoch: 89 [0/9000 (0%)]\tlog Loss: -4.256263\n",
      "\n",
      "Train set: Average log loss: -4.3307\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.4440\n",
      "\n",
      "Current Learning rate 2.9673024081888666e-05\n",
      "Train Epoch: 90 [0/9000 (0%)]\tlog Loss: -4.489168\n",
      "\n",
      "Train set: Average log loss: -4.5850\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.7183\n",
      "\n",
      "Current Learning rate 3.33129478793467e-05\n",
      "Train Epoch: 91 [0/9000 (0%)]\tlog Loss: -4.762916\n",
      "\n",
      "Train set: Average log loss: -4.8799\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.0286\n",
      "\n",
      "Current Learning rate 3.739937302478794e-05\n",
      "Train Epoch: 92 [0/9000 (0%)]\tlog Loss: -5.068805\n",
      "\n",
      "Train set: Average log loss: -5.1992\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.3380\n",
      "\n",
      "Current Learning rate 4.1987070844439056e-05\n",
      "Train Epoch: 93 [0/9000 (0%)]\tlog Loss: -5.369775\n",
      "\n",
      "Train set: Average log loss: -5.4971\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.6060\n",
      "\n",
      "Current Learning rate 4.713753134116719e-05\n",
      "Train Epoch: 94 [0/9000 (0%)]\tlog Loss: -5.624887\n",
      "\n",
      "Train set: Average log loss: -5.7407\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.8259\n",
      "\n",
      "Current Learning rate 5.2919787359584364e-05\n",
      "Train Epoch: 95 [0/9000 (0%)]\tlog Loss: -5.833019\n",
      "\n",
      "Train set: Average log loss: -5.9557\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.0428\n",
      "\n",
      "Current Learning rate 5.941133984965028e-05\n",
      "Train Epoch: 96 [0/9000 (0%)]\tlog Loss: -6.047750\n",
      "\n",
      "Train set: Average log loss: -6.1868\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.3079\n",
      "\n",
      "Current Learning rate 6.669919663030115e-05\n",
      "Train Epoch: 97 [0/9000 (0%)]\tlog Loss: -6.314693\n",
      "\n",
      "Train set: Average log loss: -6.4723\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.6128\n",
      "\n",
      "Current Learning rate 7.488103857590015e-05\n",
      "Train Epoch: 98 [0/9000 (0%)]\tlog Loss: -6.622962\n",
      "\n",
      "Train set: Average log loss: -6.7832\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.9573\n",
      "\n",
      "Current Learning rate 8.406652885618316e-05\n",
      "Train Epoch: 99 [0/9000 (0%)]\tlog Loss: -6.964139\n",
      "\n",
      "Train set: Average log loss: -7.1239\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3192\n",
      "\n",
      "Current Learning rate 9.437878277775372e-05\n",
      "Train Epoch: 100 [0/9000 (0%)]\tlog Loss: -7.335798\n",
      "\n",
      "Train set: Average log loss: -7.4834\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.6393\n",
      "\n",
      "Current Learning rate 0.0001059560179277617\n",
      "Train Epoch: 101 [0/9000 (0%)]\tlog Loss: -7.676066\n",
      "\n",
      "Train set: Average log loss: -7.8138\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.9783\n",
      "\n",
      "Current Learning rate 0.00011895340673703207\n",
      "Train Epoch: 102 [0/9000 (0%)]\tlog Loss: -8.034537\n",
      "\n",
      "Train set: Average log loss: -8.1004\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.2169\n",
      "\n",
      "Current Learning rate 0.00013354515629299002\n",
      "Train Epoch: 103 [0/9000 (0%)]\tlog Loss: -8.300510\n",
      "\n",
      "Train set: Average log loss: -8.3359\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.4301\n",
      "\n",
      "Current Learning rate 0.0001499268432786047\n",
      "Train Epoch: 104 [0/9000 (0%)]\tlog Loss: -8.533133\n",
      "\n",
      "Train set: Average log loss: -8.5615\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.5606\n",
      "\n",
      "Current Learning rate 0.00016831803533309583\n",
      "Train Epoch: 105 [0/9000 (0%)]\tlog Loss: -8.691328\n",
      "\n",
      "Train set: Average log loss: -8.8028\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.8804\n",
      "\n",
      "Current Learning rate 0.00018896523396912115\n",
      "Train Epoch: 106 [0/9000 (0%)]\tlog Loss: -8.978643\n",
      "\n",
      "Train set: Average log loss: -9.0411\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.0347\n",
      "\n",
      "Current Learning rate 0.0002121451784910632\n",
      "Train Epoch: 107 [0/9000 (0%)]\tlog Loss: -9.180743\n",
      "\n",
      "Train set: Average log loss: -8.7267\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.4656\n",
      "\n",
      "Current Learning rate 0.00023816855519761607\n",
      "Train Epoch: 108 [0/9000 (0%)]\tlog Loss: -8.582715\n",
      "\n",
      "Train set: Average log loss: -8.9278\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.0898\n",
      "\n",
      "Current Learning rate 0.00026738416158399495\n",
      "Train Epoch: 109 [0/9000 (0%)]\tlog Loss: -9.124603\n",
      "\n",
      "Train set: Average log loss: -8.8230\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.8418\n",
      "\n",
      "Current Learning rate 0.0003001835813575592\n",
      "Train Epoch: 110 [0/9000 (0%)]\tlog Loss: -8.855528\n",
      "\n",
      "Train set: Average log loss: -9.3187\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.5258\n",
      "\n",
      "Current Learning rate 0.00033700643292719316\n",
      "Train Epoch: 111 [0/9000 (0%)]\tlog Loss: -9.618487\n",
      "\n",
      "Train set: Average log loss: -7.1617\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.5907\n",
      "\n",
      "Current Learning rate 0.00037834626171319327\n",
      "Train Epoch: 112 [0/9000 (0%)]\tlog Loss: -6.566531\n",
      "\n",
      "Train set: Average log loss: -7.4291\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.7031\n",
      "\n",
      "Current Learning rate 0.0004247571552536903\n",
      "Train Epoch: 113 [0/9000 (0%)]\tlog Loss: -8.805918\n",
      "\n",
      "Train set: Average log loss: -7.9823\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.1943\n",
      "\n",
      "Current Learning rate 0.0004768611697714474\n",
      "Train Epoch: 114 [0/9000 (0%)]\tlog Loss: -8.289777\n",
      "\n",
      "Train set: Average log loss: -8.3667\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.3913\n",
      "\n",
      "Current Learning rate 0.000535356667741073\n",
      "Train Epoch: 115 [0/9000 (0%)]\tlog Loss: -9.584552\n",
      "\n",
      "Train set: Average log loss: -8.8125\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.6645\n",
      "\n",
      "Current Learning rate 0.0006010276782070388\n",
      "Train Epoch: 116 [0/9000 (0%)]\tlog Loss: -8.679564\n",
      "\n",
      "Train set: Average log loss: -9.0903\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.0852\n",
      "\n",
      "Current Learning rate 0.0006747544053110699\n",
      "Train Epoch: 117 [0/9000 (0%)]\tlog Loss: -9.143536\n",
      "\n",
      "Train set: Average log loss: -6.6093\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.5466\n",
      "\n",
      "Current Learning rate 0.000757525025877192\n",
      "Train Epoch: 118 [0/9000 (0%)]\tlog Loss: -6.577237\n",
      "\n",
      "Train set: Average log loss: -6.6238\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.6428\n",
      "\n",
      "Current Learning rate 0.0008504489341802686\n",
      "Train Epoch: 119 [0/9000 (0%)]\tlog Loss: -6.678628\n",
      "\n",
      "Train set: Average log loss: -7.2767\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.6144\n",
      "\n",
      "Current Learning rate 0.0009547716114208065\n",
      "Train Epoch: 120 [0/9000 (0%)]\tlog Loss: -8.744773\n",
      "\n",
      "Train set: Average log loss: -8.0834\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.2012\n",
      "\n",
      "Current Learning rate 0.0010718913192051286\n",
      "Train Epoch: 121 [0/9000 (0%)]\tlog Loss: -8.229053\n",
      "\n",
      "Train set: Average log loss: -8.1995\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.6898\n",
      "\n",
      "Current Learning rate 0.0012033778407775906\n",
      "Train Epoch: 122 [0/9000 (0%)]\tlog Loss: -8.738243\n",
      "\n",
      "Train set: Average log loss: -7.0483\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.0756\n",
      "\n",
      "Current Learning rate 0.0013509935211980279\n",
      "Train Epoch: 123 [0/9000 (0%)]\tlog Loss: -5.070451\n",
      "\n",
      "Train set: Average log loss: -5.5876\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.6863\n",
      "\n",
      "Current Learning rate 0.001516716888470924\n",
      "Train Epoch: 124 [0/9000 (0%)]\tlog Loss: -7.764648\n",
      "\n",
      "Train set: Average log loss: -6.8009\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.9430\n",
      "\n",
      "Current Learning rate 0.0017027691722259013\n",
      "Train Epoch: 125 [0/9000 (0%)]\tlog Loss: -6.912010\n",
      "\n",
      "Train set: Average log loss: -4.1671\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.5913\n",
      "\n",
      "Current Learning rate 0.0019116440753857036\n",
      "Train Epoch: 126 [0/9000 (0%)]\tlog Loss: -4.640599\n",
      "\n",
      "Train set: Average log loss: -5.0869\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.8714\n",
      "\n",
      "Current Learning rate 0.0021461411978584057\n",
      "Train Epoch: 127 [0/9000 (0%)]\tlog Loss: -5.913326\n",
      "\n",
      "Train set: Average log loss: -6.6941\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.8970\n",
      "\n",
      "Current Learning rate 0.0024094035602395267\n",
      "Train Epoch: 128 [0/9000 (0%)]\tlog Loss: -7.906686\n",
      "\n",
      "Train set: Average log loss: -5.5724\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.5071\n",
      "\n",
      "Current Learning rate 0.002704959730463137\n",
      "Train Epoch: 129 [0/9000 (0%)]\tlog Loss: -5.537842\n",
      "\n",
      "Train set: Average log loss: -5.7394\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.8260\n",
      "\n",
      "Current Learning rate 0.0030367711180354605\n",
      "Train Epoch: 130 [0/9000 (0%)]\tlog Loss: -7.899775\n",
      "\n",
      "Train set: Average log loss: -6.8664\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.9393\n",
      "\n",
      "Current Learning rate 0.0034092850697468144\n",
      "Train Epoch: 131 [0/9000 (0%)]\tlog Loss: -6.988581\n",
      "\n",
      "Train set: Average log loss: -7.1922\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.9790\n",
      "\n",
      "Current Learning rate 0.003827494478516315\n",
      "Train Epoch: 132 [0/9000 (0%)]\tlog Loss: -8.020634\n",
      "\n",
      "Train set: Average log loss: -8.0616\n",
      "\n",
      "\n",
      "Test set: Average loss: -8.3972\n",
      "\n",
      "Current Learning rate 0.004297004704320844\n",
      "Train Epoch: 133 [0/9000 (0%)]\tlog Loss: -8.464312\n",
      "\n",
      "Train set: Average log loss: -8.7045\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.6438\n",
      "\n",
      "Current Learning rate 0.004824108704165373\n",
      "Train Epoch: 134 [0/9000 (0%)]\tlog Loss: -9.688171\n",
      "\n",
      "Train set: Average log loss: -9.3047\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.4780\n",
      "\n",
      "Current Learning rate 0.005415871378079476\n",
      "Train Epoch: 135 [0/9000 (0%)]\tlog Loss: -9.551801\n",
      "\n",
      "Train set: Average log loss: -9.6780\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.9900\n",
      "\n",
      "Current Learning rate 0.006080224261649427\n",
      "Train Epoch: 136 [0/9000 (0%)]\tlog Loss: -10.090771\n",
      "\n",
      "Train set: Average log loss: -9.9815\n",
      "\n",
      "\n",
      "Test set: Average loss: -10.0139\n",
      "\n",
      "Current Learning rate 0.006826071834272393\n",
      "Train Epoch: 137 [0/9000 (0%)]\tlog Loss: -10.091469\n",
      "\n",
      "Train set: Average log loss: -10.3179\n",
      "\n",
      "\n",
      "Test set: Average loss: -10.4300\n",
      "\n",
      "Current Learning rate 0.007663410868007463\n",
      "Train Epoch: 138 [0/9000 (0%)]\tlog Loss: -10.516816\n",
      "\n",
      "Train set: Average log loss: -10.5155\n",
      "\n",
      "\n",
      "Test set: Average loss: -10.7951\n",
      "\n",
      "Current Learning rate 0.00860346441668451\n",
      "Train Epoch: 139 [0/9000 (0%)]\tlog Loss: -10.892964\n",
      "\n",
      "Train set: Average log loss: -10.6034\n",
      "\n",
      "\n",
      "Test set: Average loss: -9.5134\n",
      "\n",
      "Current Learning rate 0.009658832241158708\n",
      "Train Epoch: 140 [0/9000 (0%)]\tlog Loss: -9.554710\n",
      "\n",
      "Train set: Average log loss: -4.8084\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.1372\n",
      "\n",
      "Current Learning rate 0.010843659686896108\n",
      "Train Epoch: 141 [0/9000 (0%)]\tlog Loss: -2.158504\n",
      "\n",
      "Train set: Average log loss: 0.2576\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6871\n",
      "\n",
      "Current Learning rate 0.012173827277396621\n",
      "Train Epoch: 142 [0/9000 (0%)]\tlog Loss: 0.702018\n",
      "\n",
      "Train set: Average log loss: -1.0546\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.8380\n",
      "\n",
      "Current Learning rate 0.013667163564620072\n",
      "Train Epoch: 143 [0/9000 (0%)]\tlog Loss: -2.852286\n",
      "\n",
      "Train set: Average log loss: -2.9324\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.3536\n",
      "\n",
      "Current Learning rate 0.015343684089300131\n",
      "Train Epoch: 144 [0/9000 (0%)]\tlog Loss: -3.426065\n",
      "\n",
      "Train set: Average log loss: -4.2479\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.0381\n",
      "\n",
      "Current Learning rate 0.017225859653987874\n",
      "Train Epoch: 145 [0/9000 (0%)]\tlog Loss: -5.071673\n",
      "\n",
      "Train set: Average log loss: -5.6164\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.8086\n",
      "\n",
      "Current Learning rate 0.01933891750455232\n",
      "Train Epoch: 146 [0/9000 (0%)]\tlog Loss: -5.809664\n",
      "\n",
      "Train set: Average log loss: -6.2911\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.7869\n",
      "\n",
      "Current Learning rate 0.021711179456945052\n",
      "Train Epoch: 147 [0/9000 (0%)]\tlog Loss: -6.849208\n",
      "\n",
      "Train set: Average log loss: -6.9439\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.0959\n",
      "\n",
      "Current Learning rate 0.024374441501222217\n",
      "Train Epoch: 148 [0/9000 (0%)]\tlog Loss: -6.978543\n",
      "\n",
      "Train set: Average log loss: -7.2432\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3517\n",
      "\n",
      "Current Learning rate 0.02736439997074672\n",
      "Train Epoch: 149 [0/9000 (0%)]\tlog Loss: -7.257045\n",
      "\n",
      "Train set: Average log loss: -7.4227\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.6093\n",
      "\n",
      "Current Learning rate 0.03072112998861759\n",
      "Train Epoch: 150 [0/9000 (0%)]\tlog Loss: -7.532503\n",
      "\n",
      "Train set: Average log loss: -6.5603\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.8955\n",
      "\n",
      "Current Learning rate 0.0344896226040576\n",
      "Train Epoch: 151 [0/9000 (0%)]\tlog Loss: -6.831194\n",
      "\n",
      "Train set: Average log loss: -6.1704\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.0847\n",
      "\n",
      "Current Learning rate 0.03872038781812557\n",
      "Train Epoch: 152 [0/9000 (0%)]\tlog Loss: -7.114041\n",
      "\n",
      "Train set: Average log loss: -6.6153\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.0946\n",
      "\n",
      "Current Learning rate 0.043470131581250265\n",
      "Train Epoch: 153 [0/9000 (0%)]\tlog Loss: -7.051174\n",
      "\n",
      "Train set: Average log loss: -5.7148\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.3168\n",
      "\n",
      "Current Learning rate 0.04880251583654434\n",
      "Train Epoch: 154 [0/9000 (0%)]\tlog Loss: -6.367578\n",
      "\n",
      "Train set: Average log loss: -5.0271\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.7302\n",
      "\n",
      "Current Learning rate 0.05478901179593945\n",
      "Train Epoch: 155 [0/9000 (0%)]\tlog Loss: -4.770408\n",
      "\n",
      "Train set: Average log loss: -4.3495\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.2319\n",
      "\n",
      "Current Learning rate 0.06150985788580504\n",
      "Train Epoch: 156 [0/9000 (0%)]\tlog Loss: -4.285326\n",
      "\n",
      "Train set: Average log loss: -4.7818\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.8889\n",
      "\n",
      "Current Learning rate 0.0690551352016233\n",
      "Train Epoch: 157 [0/9000 (0%)]\tlog Loss: -4.914816\n",
      "\n",
      "Train set: Average log loss: -4.1261\n",
      "\n",
      "\n",
      "Test set: Average loss: -5.4872\n",
      "\n",
      "Current Learning rate 0.07752597488629465\n",
      "Train Epoch: 158 [0/9000 (0%)]\tlog Loss: -5.523631\n",
      "\n",
      "Train set: Average log loss: -4.9155\n",
      "\n",
      "\n",
      "Test set: Average loss: -4.1109\n",
      "\n",
      "Current Learning rate 0.08703591361485166\n",
      "Train Epoch: 159 [0/9000 (0%)]\tlog Loss: -4.125709\n",
      "\n",
      "Train set: Average log loss: -5.0337\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.0033\n",
      "\n",
      "Current Learning rate 0.09771241535346502\n",
      "Train Epoch: 160 [0/9000 (0%)]\tlog Loss: -6.169976\n",
      "\n",
      "Train set: Average log loss: -5.5903\n",
      "\n",
      "\n",
      "Test set: Average loss: -6.0230\n",
      "\n",
      "Current Learning rate 0.10969857978923818\n",
      "Train Epoch: 161 [0/9000 (0%)]\tlog Loss: -6.149983\n",
      "\n",
      "Train set: Average log loss: -3.0531\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.1878\n",
      "\n",
      "Current Learning rate 0.12315506032928261\n",
      "Train Epoch: 162 [0/9000 (0%)]\tlog Loss: -2.165088\n",
      "\n",
      "Train set: Average log loss: -1.8232\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.6398\n",
      "\n",
      "Current Learning rate 0.13826221737646535\n",
      "Train Epoch: 163 [0/9000 (0%)]\tlog Loss: -2.694727\n",
      "\n",
      "Train set: Average log loss: -2.8659\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.8899\n",
      "\n",
      "Current Learning rate 0.1552225357427048\n",
      "Train Epoch: 164 [0/9000 (0%)]\tlog Loss: -2.890587\n",
      "\n",
      "Train set: Average log loss: -2.9532\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0222\n",
      "\n",
      "Current Learning rate 0.1742633386009647\n",
      "Train Epoch: 165 [0/9000 (0%)]\tlog Loss: -3.042732\n",
      "\n",
      "Train set: Average log loss: -2.4146\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0196\n",
      "\n",
      "Current Learning rate 0.19563983435170648\n",
      "Train Epoch: 166 [0/9000 (0%)]\tlog Loss: -3.058479\n",
      "\n",
      "Train set: Average log loss: -2.8212\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.9660\n",
      "\n",
      "Current Learning rate 0.21963853724165425\n",
      "Train Epoch: 167 [0/9000 (0%)]\tlog Loss: -2.975249\n",
      "\n",
      "Train set: Average log loss: -2.9334\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0212\n",
      "\n",
      "Current Learning rate 0.2465811075822604\n",
      "Train Epoch: 168 [0/9000 (0%)]\tlog Loss: -3.041479\n",
      "\n",
      "Train set: Average log loss: -3.0195\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0283\n",
      "\n",
      "Current Learning rate 0.2768286630392061\n",
      "Train Epoch: 169 [0/9000 (0%)]\tlog Loss: -3.064615\n",
      "\n",
      "Train set: Average log loss: -3.0218\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0356\n",
      "\n",
      "Current Learning rate 0.3107866187782014\n",
      "Train Epoch: 170 [0/9000 (0%)]\tlog Loss: -3.062805\n",
      "\n",
      "Train set: Average log loss: -3.0275\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0179\n",
      "\n",
      "Current Learning rate 0.34891012134067667\n",
      "Train Epoch: 171 [0/9000 (0%)]\tlog Loss: -3.037170\n",
      "\n",
      "Train set: Average log loss: -3.0324\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0366\n",
      "\n",
      "Current Learning rate 0.39171014908092605\n",
      "Train Epoch: 172 [0/9000 (0%)]\tlog Loss: -3.067252\n",
      "\n",
      "Train set: Average log loss: -3.0417\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0367\n",
      "\n",
      "Current Learning rate 0.4397603609302712\n",
      "Train Epoch: 173 [0/9000 (0%)]\tlog Loss: -3.066543\n",
      "\n",
      "Train set: Average log loss: -3.0403\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0353\n",
      "\n",
      "Current Learning rate 0.4937047852839004\n",
      "Train Epoch: 174 [0/9000 (0%)]\tlog Loss: -3.062155\n",
      "\n",
      "Train set: Average log loss: -3.0407\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0367\n",
      "\n",
      "Current Learning rate 0.5542664520663095\n",
      "Train Epoch: 175 [0/9000 (0%)]\tlog Loss: -3.066368\n",
      "\n",
      "Train set: Average log loss: -3.0415\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0362\n",
      "\n",
      "Current Learning rate 0.6222570836730231\n",
      "Train Epoch: 176 [0/9000 (0%)]\tlog Loss: -3.064177\n",
      "\n",
      "Train set: Average log loss: -3.0404\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0353\n",
      "\n",
      "Current Learning rate 0.6985879746785235\n",
      "Train Epoch: 177 [0/9000 (0%)]\tlog Loss: -3.062283\n",
      "\n",
      "Train set: Average log loss: -3.0399\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0352\n",
      "\n",
      "Current Learning rate 0.7842822061337682\n",
      "Train Epoch: 178 [0/9000 (0%)]\tlog Loss: -3.062009\n",
      "\n",
      "Train set: Average log loss: -3.0395\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0345\n",
      "\n",
      "Current Learning rate 0.8804883581643447\n",
      "Train Epoch: 179 [0/9000 (0%)]\tlog Loss: -3.060660\n",
      "\n",
      "Train set: Average log loss: -3.0383\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0331\n",
      "\n",
      "Current Learning rate 0.9884959046625587\n",
      "Train Epoch: 180 [0/9000 (0%)]\tlog Loss: -3.058344\n",
      "\n",
      "Train set: Average log loss: -3.0362\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0321\n",
      "\n",
      "Current Learning rate 1.10975249641207\n",
      "Train Epoch: 181 [0/9000 (0%)]\tlog Loss: -3.056770\n",
      "\n",
      "Train set: Average log loss: -3.0329\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0338\n",
      "\n",
      "Current Learning rate 1.2458833642950082\n",
      "Train Epoch: 182 [0/9000 (0%)]\tlog Loss: -3.059454\n",
      "\n",
      "Train set: Average log loss: -3.0291\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0367\n",
      "\n",
      "Current Learning rate 1.3987131026472357\n",
      "Train Epoch: 183 [0/9000 (0%)]\tlog Loss: -3.066916\n",
      "\n",
      "Train set: Average log loss: -3.0270\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0243\n",
      "\n",
      "Current Learning rate 1.5702901247293775\n",
      "Train Epoch: 184 [0/9000 (0%)]\tlog Loss: -3.061926\n",
      "\n",
      "Train set: Average log loss: -3.0281\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0099\n",
      "\n",
      "Current Learning rate 1.7629141180959444\n",
      "Train Epoch: 185 [0/9000 (0%)]\tlog Loss: -3.050806\n",
      "\n",
      "Train set: Average log loss: -3.0345\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0325\n",
      "\n",
      "Current Learning rate 1.9791668678535574\n",
      "Train Epoch: 186 [0/9000 (0%)]\tlog Loss: -3.066976\n",
      "\n",
      "Train set: Average log loss: -3.0409\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0352\n",
      "\n",
      "Current Learning rate 2.221946860939519\n",
      "Train Epoch: 187 [0/9000 (0%)]\tlog Loss: -3.067872\n",
      "\n",
      "Train set: Average log loss: -3.0402\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0314\n",
      "\n",
      "Current Learning rate 2.4945081352303164\n",
      "Train Epoch: 188 [0/9000 (0%)]\tlog Loss: -3.066443\n",
      "\n",
      "Train set: Average log loss: -3.0401\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0340\n",
      "\n",
      "Current Learning rate 2.8005038941836253\n",
      "Train Epoch: 189 [0/9000 (0%)]\tlog Loss: -3.067613\n",
      "\n",
      "Train set: Average log loss: -3.0395\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0340\n",
      "\n",
      "Current Learning rate 3.1440354715915\n",
      "Train Epoch: 190 [0/9000 (0%)]\tlog Loss: -3.067591\n",
      "\n",
      "Train set: Average log loss: -3.0390\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0303\n",
      "\n",
      "Current Learning rate 3.529707302730643\n",
      "Train Epoch: 191 [0/9000 (0%)]\tlog Loss: -3.065835\n",
      "\n",
      "Train set: Average log loss: -3.0359\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0221\n",
      "\n",
      "Current Learning rate 3.9626886387014784\n",
      "Train Epoch: 192 [0/9000 (0%)]\tlog Loss: -3.060384\n",
      "\n",
      "Train set: Average log loss: -3.0296\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0157\n",
      "\n",
      "Current Learning rate 4.448782831127576\n",
      "Train Epoch: 193 [0/9000 (0%)]\tlog Loss: -3.055453\n",
      "\n",
      "Train set: Average log loss: -3.0216\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0367\n",
      "\n",
      "Current Learning rate 4.99450511585514\n",
      "Train Epoch: 194 [0/9000 (0%)]\tlog Loss: -3.066114\n",
      "\n",
      "Train set: Average log loss: -3.0301\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0329\n",
      "\n",
      "Current Learning rate 5.6071699382054465\n",
      "Train Epoch: 195 [0/9000 (0%)]\tlog Loss: -3.058092\n",
      "\n",
      "Train set: Average log loss: -3.0104\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0243\n",
      "\n",
      "Current Learning rate 6.294988990221888\n",
      "Train Epoch: 196 [0/9000 (0%)]\tlog Loss: -3.045595\n",
      "\n",
      "Train set: Average log loss: -2.9836\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0333\n",
      "\n",
      "Current Learning rate 7.067181273927477\n",
      "Train Epoch: 197 [0/9000 (0%)]\tlog Loss: -3.058659\n",
      "\n",
      "Train set: Average log loss: -2.8909\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.6305\n",
      "\n",
      "Current Learning rate 7.934096665797492\n",
      "Train Epoch: 198 [0/9000 (0%)]\tlog Loss: -2.685377\n",
      "\n",
      "Train set: Average log loss: -2.7941\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.8849\n",
      "\n",
      "Current Learning rate 8.907354638610421\n",
      "Train Epoch: 199 [0/9000 (0%)]\tlog Loss: -2.936380\n",
      "\n",
      "Train set: Average log loss: -2.5210\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.9361\n",
      "\n",
      "Current Learning rate 10.0\n",
      "Train Epoch: 200 [0/9000 (0%)]\tlog Loss: -1.912333\n",
      "\n",
      "Train set: Average log loss: 2.2121\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.2403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting the Training parameters\n",
    "args =  {\"batch_size\": 1024,\n",
    "         \"test_batch_size\": 1024,\n",
    "         \"epochs\" : 200,\n",
    "         \"lr\": 1,\n",
    "         \"gamma\": 1,\n",
    "         \"no_cuda\" : False,\n",
    "         \"run_dry\": False,\n",
    "         \"seed\": 0,\n",
    "         \"log_interval\" : 10**6,\n",
    "         \"dry_run\" : False,\n",
    "         \"save_model\": False}\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "# loading Train / Test Data\n",
    "bs_dataset = bs_LHS_data_generator(n = 10**4)\n",
    "train_size, test_size = int(bs_dataset.shape[0]*0.9), int(bs_dataset.shape[0]*0.1 )\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(bs_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])\n",
    "\n",
    "\n",
    "# Model training with Linear increase of the learning rate\n",
    "model = BS_ANN().to(device)\n",
    "# Adam optmizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "# Linear increse of LR\n",
    "lambda1 = lambda epoch: 10**(-9+epoch*10/(args[\"epochs\"]-1) )\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "lrs = []\n",
    "# Training loop over epochs\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "  print(\"Current Learning rate {}\".format(scheduler.get_last_lr()[0]))\n",
    "  lrs.append(scheduler.get_last_lr())\n",
    "  train_loss = model.train_model(args, device, train_loader, optimizer, epoch)\n",
    "  test_loss = model.test_model(device, test_loader)\n",
    "  scheduler.step()\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "  if args[\"save_model\"] & (epoch % 10**3) ==0:\n",
    "    torch.save(model.state_dict(), \"BS_ANN\"+str(epoch)+\".pt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "gb-Qb0ipYuu4",
    "outputId": "14686d79-11bf-43d8-df6e-4c80d2a8e962"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f668d8a5750>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the log MSE as a funtion of the learning rate\n",
    "plt.plot(np.log10(lrs), np.log(train_losses), label = \"train\")\n",
    "plt.xlabel(\"Learning rate (log scale)\")\n",
    "plt.ylabel(\"log(MSE)\")\n",
    "plt.title(\"Average training loss over learning rate\" )\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2OSmViKYxuM"
   },
   "source": [
    "**Remark:** it seems like the best interval is from $10^{-4}$ to $10^{-6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWu772JpMRaZ"
   },
   "source": [
    "### Training BS-ANN with DecayLR from $10^{-4}$ to $10^{-6}$ Wide dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4lds0Lmm7jf",
    "outputId": "1aa40059-3175-4e42-d701-5c18ab5bb785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mLe flux de sortie a t tronqu et ne contient que les 5000dernires lignes.\u001b[0m\n",
      "Train Epoch: 2376 [0/90000 (0%)]\tlog Loss: -17.512297\n",
      "\n",
      "Train set: Average log loss: -17.5304\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2377 [0/90000 (0%)]\tlog Loss: -17.512380\n",
      "\n",
      "Train set: Average log loss: -17.5305\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2378 [0/90000 (0%)]\tlog Loss: -17.512437\n",
      "\n",
      "Train set: Average log loss: -17.5305\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2379 [0/90000 (0%)]\tlog Loss: -17.512526\n",
      "\n",
      "Train set: Average log loss: -17.5307\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2380 [0/90000 (0%)]\tlog Loss: -17.512731\n",
      "\n",
      "Train set: Average log loss: -17.5308\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2381 [0/90000 (0%)]\tlog Loss: -17.512665\n",
      "\n",
      "Train set: Average log loss: -17.5308\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2382 [0/90000 (0%)]\tlog Loss: -17.512781\n",
      "\n",
      "Train set: Average log loss: -17.5309\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2383 [0/90000 (0%)]\tlog Loss: -17.513013\n",
      "\n",
      "Train set: Average log loss: -17.5310\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2384 [0/90000 (0%)]\tlog Loss: -17.512967\n",
      "\n",
      "Train set: Average log loss: -17.5311\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3940\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2385 [0/90000 (0%)]\tlog Loss: -17.513297\n",
      "\n",
      "Train set: Average log loss: -17.5311\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2386 [0/90000 (0%)]\tlog Loss: -17.513291\n",
      "\n",
      "Train set: Average log loss: -17.5313\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2387 [0/90000 (0%)]\tlog Loss: -17.513446\n",
      "\n",
      "Train set: Average log loss: -17.5313\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2388 [0/90000 (0%)]\tlog Loss: -17.513551\n",
      "\n",
      "Train set: Average log loss: -17.5314\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2389 [0/90000 (0%)]\tlog Loss: -17.513652\n",
      "\n",
      "Train set: Average log loss: -17.5315\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2390 [0/90000 (0%)]\tlog Loss: -17.513735\n",
      "\n",
      "Train set: Average log loss: -17.5315\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2391 [0/90000 (0%)]\tlog Loss: -17.513798\n",
      "\n",
      "Train set: Average log loss: -17.5316\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2392 [0/90000 (0%)]\tlog Loss: -17.513860\n",
      "\n",
      "Train set: Average log loss: -17.5317\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2393 [0/90000 (0%)]\tlog Loss: -17.513948\n",
      "\n",
      "Train set: Average log loss: -17.5318\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2394 [0/90000 (0%)]\tlog Loss: -17.514107\n",
      "\n",
      "Train set: Average log loss: -17.5319\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2395 [0/90000 (0%)]\tlog Loss: -17.514177\n",
      "\n",
      "Train set: Average log loss: -17.5320\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2396 [0/90000 (0%)]\tlog Loss: -17.514241\n",
      "\n",
      "Train set: Average log loss: -17.5321\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2397 [0/90000 (0%)]\tlog Loss: -17.514457\n",
      "\n",
      "Train set: Average log loss: -17.5321\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3950\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2398 [0/90000 (0%)]\tlog Loss: -17.514666\n",
      "\n",
      "Train set: Average log loss: -17.5322\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2399 [0/90000 (0%)]\tlog Loss: -17.514692\n",
      "\n",
      "Train set: Average log loss: -17.5323\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2400 [0/90000 (0%)]\tlog Loss: -17.514897\n",
      "\n",
      "Train set: Average log loss: -17.5324\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2401 [0/90000 (0%)]\tlog Loss: -17.514973\n",
      "\n",
      "Train set: Average log loss: -17.5325\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2402 [0/90000 (0%)]\tlog Loss: -17.515080\n",
      "\n",
      "Train set: Average log loss: -17.5326\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3954\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2403 [0/90000 (0%)]\tlog Loss: -17.515183\n",
      "\n",
      "Train set: Average log loss: -17.5326\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2404 [0/90000 (0%)]\tlog Loss: -17.515340\n",
      "\n",
      "Train set: Average log loss: -17.5327\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2405 [0/90000 (0%)]\tlog Loss: -17.515576\n",
      "\n",
      "Train set: Average log loss: -17.5328\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3958\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2406 [0/90000 (0%)]\tlog Loss: -17.515638\n",
      "\n",
      "Train set: Average log loss: -17.5329\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2407 [0/90000 (0%)]\tlog Loss: -17.515795\n",
      "\n",
      "Train set: Average log loss: -17.5330\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2408 [0/90000 (0%)]\tlog Loss: -17.515914\n",
      "\n",
      "Train set: Average log loss: -17.5331\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2409 [0/90000 (0%)]\tlog Loss: -17.516171\n",
      "\n",
      "Train set: Average log loss: -17.5331\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2410 [0/90000 (0%)]\tlog Loss: -17.516136\n",
      "\n",
      "Train set: Average log loss: -17.5332\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2411 [0/90000 (0%)]\tlog Loss: -17.516233\n",
      "\n",
      "Train set: Average log loss: -17.5333\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2412 [0/90000 (0%)]\tlog Loss: -17.516328\n",
      "\n",
      "Train set: Average log loss: -17.5334\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2413 [0/90000 (0%)]\tlog Loss: -17.516282\n",
      "\n",
      "Train set: Average log loss: -17.5335\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2414 [0/90000 (0%)]\tlog Loss: -17.516562\n",
      "\n",
      "Train set: Average log loss: -17.5336\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2415 [0/90000 (0%)]\tlog Loss: -17.516724\n",
      "\n",
      "Train set: Average log loss: -17.5336\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2416 [0/90000 (0%)]\tlog Loss: -17.516840\n",
      "\n",
      "Train set: Average log loss: -17.5337\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2417 [0/90000 (0%)]\tlog Loss: -17.516949\n",
      "\n",
      "Train set: Average log loss: -17.5338\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2418 [0/90000 (0%)]\tlog Loss: -17.517150\n",
      "\n",
      "Train set: Average log loss: -17.5339\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2419 [0/90000 (0%)]\tlog Loss: -17.517376\n",
      "\n",
      "Train set: Average log loss: -17.5340\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2420 [0/90000 (0%)]\tlog Loss: -17.517407\n",
      "\n",
      "Train set: Average log loss: -17.5341\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2421 [0/90000 (0%)]\tlog Loss: -17.517576\n",
      "\n",
      "Train set: Average log loss: -17.5341\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2422 [0/90000 (0%)]\tlog Loss: -17.517475\n",
      "\n",
      "Train set: Average log loss: -17.5342\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2423 [0/90000 (0%)]\tlog Loss: -17.517631\n",
      "\n",
      "Train set: Average log loss: -17.5343\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2424 [0/90000 (0%)]\tlog Loss: -17.517809\n",
      "\n",
      "Train set: Average log loss: -17.5344\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2425 [0/90000 (0%)]\tlog Loss: -17.517769\n",
      "\n",
      "Train set: Average log loss: -17.5344\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2426 [0/90000 (0%)]\tlog Loss: -17.518067\n",
      "\n",
      "Train set: Average log loss: -17.5345\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2427 [0/90000 (0%)]\tlog Loss: -17.518050\n",
      "\n",
      "Train set: Average log loss: -17.5346\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2428 [0/90000 (0%)]\tlog Loss: -17.518203\n",
      "\n",
      "Train set: Average log loss: -17.5347\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2429 [0/90000 (0%)]\tlog Loss: -17.518357\n",
      "\n",
      "Train set: Average log loss: -17.5348\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2430 [0/90000 (0%)]\tlog Loss: -17.518424\n",
      "\n",
      "Train set: Average log loss: -17.5348\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2431 [0/90000 (0%)]\tlog Loss: -17.518587\n",
      "\n",
      "Train set: Average log loss: -17.5349\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2432 [0/90000 (0%)]\tlog Loss: -17.518611\n",
      "\n",
      "Train set: Average log loss: -17.5350\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2433 [0/90000 (0%)]\tlog Loss: -17.518715\n",
      "\n",
      "Train set: Average log loss: -17.5351\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2434 [0/90000 (0%)]\tlog Loss: -17.518695\n",
      "\n",
      "Train set: Average log loss: -17.5351\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2435 [0/90000 (0%)]\tlog Loss: -17.518790\n",
      "\n",
      "Train set: Average log loss: -17.5352\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2436 [0/90000 (0%)]\tlog Loss: -17.518894\n",
      "\n",
      "Train set: Average log loss: -17.5353\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3983\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2437 [0/90000 (0%)]\tlog Loss: -17.519051\n",
      "\n",
      "Train set: Average log loss: -17.5354\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2438 [0/90000 (0%)]\tlog Loss: -17.519087\n",
      "\n",
      "Train set: Average log loss: -17.5355\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2439 [0/90000 (0%)]\tlog Loss: -17.519128\n",
      "\n",
      "Train set: Average log loss: -17.5356\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2440 [0/90000 (0%)]\tlog Loss: -17.519302\n",
      "\n",
      "Train set: Average log loss: -17.5356\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2441 [0/90000 (0%)]\tlog Loss: -17.519288\n",
      "\n",
      "Train set: Average log loss: -17.5357\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2442 [0/90000 (0%)]\tlog Loss: -17.519474\n",
      "\n",
      "Train set: Average log loss: -17.5358\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2443 [0/90000 (0%)]\tlog Loss: -17.519622\n",
      "\n",
      "Train set: Average log loss: -17.5359\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2444 [0/90000 (0%)]\tlog Loss: -17.519784\n",
      "\n",
      "Train set: Average log loss: -17.5360\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2445 [0/90000 (0%)]\tlog Loss: -17.519781\n",
      "\n",
      "Train set: Average log loss: -17.5361\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2446 [0/90000 (0%)]\tlog Loss: -17.519903\n",
      "\n",
      "Train set: Average log loss: -17.5362\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2447 [0/90000 (0%)]\tlog Loss: -17.520079\n",
      "\n",
      "Train set: Average log loss: -17.5362\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2448 [0/90000 (0%)]\tlog Loss: -17.520162\n",
      "\n",
      "Train set: Average log loss: -17.5363\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2449 [0/90000 (0%)]\tlog Loss: -17.520157\n",
      "\n",
      "Train set: Average log loss: -17.5364\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3993\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2450 [0/90000 (0%)]\tlog Loss: -17.520261\n",
      "\n",
      "Train set: Average log loss: -17.5365\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2451 [0/90000 (0%)]\tlog Loss: -17.520287\n",
      "\n",
      "Train set: Average log loss: -17.5365\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2452 [0/90000 (0%)]\tlog Loss: -17.520497\n",
      "\n",
      "Train set: Average log loss: -17.5367\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2453 [0/90000 (0%)]\tlog Loss: -17.520469\n",
      "\n",
      "Train set: Average log loss: -17.5367\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2454 [0/90000 (0%)]\tlog Loss: -17.520714\n",
      "\n",
      "Train set: Average log loss: -17.5368\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2455 [0/90000 (0%)]\tlog Loss: -17.520825\n",
      "\n",
      "Train set: Average log loss: -17.5369\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.3999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2456 [0/90000 (0%)]\tlog Loss: -17.520934\n",
      "\n",
      "Train set: Average log loss: -17.5370\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2457 [0/90000 (0%)]\tlog Loss: -17.521126\n",
      "\n",
      "Train set: Average log loss: -17.5371\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2458 [0/90000 (0%)]\tlog Loss: -17.521360\n",
      "\n",
      "Train set: Average log loss: -17.5372\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2459 [0/90000 (0%)]\tlog Loss: -17.521494\n",
      "\n",
      "Train set: Average log loss: -17.5373\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2460 [0/90000 (0%)]\tlog Loss: -17.521636\n",
      "\n",
      "Train set: Average log loss: -17.5374\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2461 [0/90000 (0%)]\tlog Loss: -17.521701\n",
      "\n",
      "Train set: Average log loss: -17.5374\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2462 [0/90000 (0%)]\tlog Loss: -17.521850\n",
      "\n",
      "Train set: Average log loss: -17.5375\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2463 [0/90000 (0%)]\tlog Loss: -17.522010\n",
      "\n",
      "Train set: Average log loss: -17.5376\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4007\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2464 [0/90000 (0%)]\tlog Loss: -17.522049\n",
      "\n",
      "Train set: Average log loss: -17.5377\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4007\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2465 [0/90000 (0%)]\tlog Loss: -17.522209\n",
      "\n",
      "Train set: Average log loss: -17.5377\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2466 [0/90000 (0%)]\tlog Loss: -17.522473\n",
      "\n",
      "Train set: Average log loss: -17.5378\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2467 [0/90000 (0%)]\tlog Loss: -17.522459\n",
      "\n",
      "Train set: Average log loss: -17.5379\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2468 [0/90000 (0%)]\tlog Loss: -17.522708\n",
      "\n",
      "Train set: Average log loss: -17.5380\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2469 [0/90000 (0%)]\tlog Loss: -17.522713\n",
      "\n",
      "Train set: Average log loss: -17.5381\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4011\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2470 [0/90000 (0%)]\tlog Loss: -17.522801\n",
      "\n",
      "Train set: Average log loss: -17.5381\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2471 [0/90000 (0%)]\tlog Loss: -17.522944\n",
      "\n",
      "Train set: Average log loss: -17.5382\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4013\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2472 [0/90000 (0%)]\tlog Loss: -17.522963\n",
      "\n",
      "Train set: Average log loss: -17.5383\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2473 [0/90000 (0%)]\tlog Loss: -17.523244\n",
      "\n",
      "Train set: Average log loss: -17.5384\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2474 [0/90000 (0%)]\tlog Loss: -17.523311\n",
      "\n",
      "Train set: Average log loss: -17.5385\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2475 [0/90000 (0%)]\tlog Loss: -17.523290\n",
      "\n",
      "Train set: Average log loss: -17.5386\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4016\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2476 [0/90000 (0%)]\tlog Loss: -17.523276\n",
      "\n",
      "Train set: Average log loss: -17.5386\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2477 [0/90000 (0%)]\tlog Loss: -17.523536\n",
      "\n",
      "Train set: Average log loss: -17.5387\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2478 [0/90000 (0%)]\tlog Loss: -17.523578\n",
      "\n",
      "Train set: Average log loss: -17.5387\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2479 [0/90000 (0%)]\tlog Loss: -17.523523\n",
      "\n",
      "Train set: Average log loss: -17.5388\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2480 [0/90000 (0%)]\tlog Loss: -17.523488\n",
      "\n",
      "Train set: Average log loss: -17.5389\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2481 [0/90000 (0%)]\tlog Loss: -17.523604\n",
      "\n",
      "Train set: Average log loss: -17.5390\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2482 [0/90000 (0%)]\tlog Loss: -17.523634\n",
      "\n",
      "Train set: Average log loss: -17.5391\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2483 [0/90000 (0%)]\tlog Loss: -17.523744\n",
      "\n",
      "Train set: Average log loss: -17.5392\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2484 [0/90000 (0%)]\tlog Loss: -17.523774\n",
      "\n",
      "Train set: Average log loss: -17.5392\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4020\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2485 [0/90000 (0%)]\tlog Loss: -17.523823\n",
      "\n",
      "Train set: Average log loss: -17.5393\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4021\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2486 [0/90000 (0%)]\tlog Loss: -17.524010\n",
      "\n",
      "Train set: Average log loss: -17.5394\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2487 [0/90000 (0%)]\tlog Loss: -17.524178\n",
      "\n",
      "Train set: Average log loss: -17.5395\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4023\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2488 [0/90000 (0%)]\tlog Loss: -17.524144\n",
      "\n",
      "Train set: Average log loss: -17.5396\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2489 [0/90000 (0%)]\tlog Loss: -17.524407\n",
      "\n",
      "Train set: Average log loss: -17.5397\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2490 [0/90000 (0%)]\tlog Loss: -17.524466\n",
      "\n",
      "Train set: Average log loss: -17.5398\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2491 [0/90000 (0%)]\tlog Loss: -17.524635\n",
      "\n",
      "Train set: Average log loss: -17.5398\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2492 [0/90000 (0%)]\tlog Loss: -17.524624\n",
      "\n",
      "Train set: Average log loss: -17.5399\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2493 [0/90000 (0%)]\tlog Loss: -17.524687\n",
      "\n",
      "Train set: Average log loss: -17.5399\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2494 [0/90000 (0%)]\tlog Loss: -17.524684\n",
      "\n",
      "Train set: Average log loss: -17.5400\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2495 [0/90000 (0%)]\tlog Loss: -17.524830\n",
      "\n",
      "Train set: Average log loss: -17.5401\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2496 [0/90000 (0%)]\tlog Loss: -17.524857\n",
      "\n",
      "Train set: Average log loss: -17.5402\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2497 [0/90000 (0%)]\tlog Loss: -17.524919\n",
      "\n",
      "Train set: Average log loss: -17.5403\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2498 [0/90000 (0%)]\tlog Loss: -17.525097\n",
      "\n",
      "Train set: Average log loss: -17.5404\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2499 [0/90000 (0%)]\tlog Loss: -17.525298\n",
      "\n",
      "Train set: Average log loss: -17.5405\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2500 [0/90000 (0%)]\tlog Loss: -17.525384\n",
      "\n",
      "Train set: Average log loss: -17.5405\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2501 [0/90000 (0%)]\tlog Loss: -17.525472\n",
      "\n",
      "Train set: Average log loss: -17.5406\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2502 [0/90000 (0%)]\tlog Loss: -17.525564\n",
      "\n",
      "Train set: Average log loss: -17.5407\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2503 [0/90000 (0%)]\tlog Loss: -17.525652\n",
      "\n",
      "Train set: Average log loss: -17.5408\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2504 [0/90000 (0%)]\tlog Loss: -17.525818\n",
      "\n",
      "Train set: Average log loss: -17.5409\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2505 [0/90000 (0%)]\tlog Loss: -17.525824\n",
      "\n",
      "Train set: Average log loss: -17.5409\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4035\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2506 [0/90000 (0%)]\tlog Loss: -17.525950\n",
      "\n",
      "Train set: Average log loss: -17.5410\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2507 [0/90000 (0%)]\tlog Loss: -17.526108\n",
      "\n",
      "Train set: Average log loss: -17.5411\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2508 [0/90000 (0%)]\tlog Loss: -17.526396\n",
      "\n",
      "Train set: Average log loss: -17.5412\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2509 [0/90000 (0%)]\tlog Loss: -17.526322\n",
      "\n",
      "Train set: Average log loss: -17.5413\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2510 [0/90000 (0%)]\tlog Loss: -17.526538\n",
      "\n",
      "Train set: Average log loss: -17.5413\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2511 [0/90000 (0%)]\tlog Loss: -17.526530\n",
      "\n",
      "Train set: Average log loss: -17.5414\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2512 [0/90000 (0%)]\tlog Loss: -17.526589\n",
      "\n",
      "Train set: Average log loss: -17.5415\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2513 [0/90000 (0%)]\tlog Loss: -17.526710\n",
      "\n",
      "Train set: Average log loss: -17.5415\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2514 [0/90000 (0%)]\tlog Loss: -17.526699\n",
      "\n",
      "Train set: Average log loss: -17.5416\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2515 [0/90000 (0%)]\tlog Loss: -17.526861\n",
      "\n",
      "Train set: Average log loss: -17.5417\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2516 [0/90000 (0%)]\tlog Loss: -17.526901\n",
      "\n",
      "Train set: Average log loss: -17.5418\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2517 [0/90000 (0%)]\tlog Loss: -17.527059\n",
      "\n",
      "Train set: Average log loss: -17.5419\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2518 [0/90000 (0%)]\tlog Loss: -17.527010\n",
      "\n",
      "Train set: Average log loss: -17.5419\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2519 [0/90000 (0%)]\tlog Loss: -17.527065\n",
      "\n",
      "Train set: Average log loss: -17.5420\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2520 [0/90000 (0%)]\tlog Loss: -17.527009\n",
      "\n",
      "Train set: Average log loss: -17.5421\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2521 [0/90000 (0%)]\tlog Loss: -17.527026\n",
      "\n",
      "Train set: Average log loss: -17.5422\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2522 [0/90000 (0%)]\tlog Loss: -17.527324\n",
      "\n",
      "Train set: Average log loss: -17.5423\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2523 [0/90000 (0%)]\tlog Loss: -17.527378\n",
      "\n",
      "Train set: Average log loss: -17.5424\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2524 [0/90000 (0%)]\tlog Loss: -17.527541\n",
      "\n",
      "Train set: Average log loss: -17.5424\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2525 [0/90000 (0%)]\tlog Loss: -17.527480\n",
      "\n",
      "Train set: Average log loss: -17.5426\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2526 [0/90000 (0%)]\tlog Loss: -17.527605\n",
      "\n",
      "Train set: Average log loss: -17.5426\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2527 [0/90000 (0%)]\tlog Loss: -17.527534\n",
      "\n",
      "Train set: Average log loss: -17.5427\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2528 [0/90000 (0%)]\tlog Loss: -17.527707\n",
      "\n",
      "Train set: Average log loss: -17.5428\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4049\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2529 [0/90000 (0%)]\tlog Loss: -17.527765\n",
      "\n",
      "Train set: Average log loss: -17.5429\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4050\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2530 [0/90000 (0%)]\tlog Loss: -17.527955\n",
      "\n",
      "Train set: Average log loss: -17.5430\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4051\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2531 [0/90000 (0%)]\tlog Loss: -17.528015\n",
      "\n",
      "Train set: Average log loss: -17.5431\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2532 [0/90000 (0%)]\tlog Loss: -17.528274\n",
      "\n",
      "Train set: Average log loss: -17.5431\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4053\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2533 [0/90000 (0%)]\tlog Loss: -17.528370\n",
      "\n",
      "Train set: Average log loss: -17.5432\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4054\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2534 [0/90000 (0%)]\tlog Loss: -17.528299\n",
      "\n",
      "Train set: Average log loss: -17.5433\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2535 [0/90000 (0%)]\tlog Loss: -17.528496\n",
      "\n",
      "Train set: Average log loss: -17.5433\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2536 [0/90000 (0%)]\tlog Loss: -17.528564\n",
      "\n",
      "Train set: Average log loss: -17.5434\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4057\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2537 [0/90000 (0%)]\tlog Loss: -17.528674\n",
      "\n",
      "Train set: Average log loss: -17.5435\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4058\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2538 [0/90000 (0%)]\tlog Loss: -17.528737\n",
      "\n",
      "Train set: Average log loss: -17.5436\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4059\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2539 [0/90000 (0%)]\tlog Loss: -17.529042\n",
      "\n",
      "Train set: Average log loss: -17.5437\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4061\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2540 [0/90000 (0%)]\tlog Loss: -17.529014\n",
      "\n",
      "Train set: Average log loss: -17.5438\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4060\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2541 [0/90000 (0%)]\tlog Loss: -17.529059\n",
      "\n",
      "Train set: Average log loss: -17.5439\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4062\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2542 [0/90000 (0%)]\tlog Loss: -17.529109\n",
      "\n",
      "Train set: Average log loss: -17.5439\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4062\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2543 [0/90000 (0%)]\tlog Loss: -17.529203\n",
      "\n",
      "Train set: Average log loss: -17.5440\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4063\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2544 [0/90000 (0%)]\tlog Loss: -17.529356\n",
      "\n",
      "Train set: Average log loss: -17.5441\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4064\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2545 [0/90000 (0%)]\tlog Loss: -17.529390\n",
      "\n",
      "Train set: Average log loss: -17.5442\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4065\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2546 [0/90000 (0%)]\tlog Loss: -17.529562\n",
      "\n",
      "Train set: Average log loss: -17.5443\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4066\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2547 [0/90000 (0%)]\tlog Loss: -17.529671\n",
      "\n",
      "Train set: Average log loss: -17.5443\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4067\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2548 [0/90000 (0%)]\tlog Loss: -17.529751\n",
      "\n",
      "Train set: Average log loss: -17.5445\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4068\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2549 [0/90000 (0%)]\tlog Loss: -17.529892\n",
      "\n",
      "Train set: Average log loss: -17.5445\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4069\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2550 [0/90000 (0%)]\tlog Loss: -17.529925\n",
      "\n",
      "Train set: Average log loss: -17.5446\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4070\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2551 [0/90000 (0%)]\tlog Loss: -17.530000\n",
      "\n",
      "Train set: Average log loss: -17.5447\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4071\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2552 [0/90000 (0%)]\tlog Loss: -17.530262\n",
      "\n",
      "Train set: Average log loss: -17.5448\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4072\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2553 [0/90000 (0%)]\tlog Loss: -17.530208\n",
      "\n",
      "Train set: Average log loss: -17.5449\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4073\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2554 [0/90000 (0%)]\tlog Loss: -17.530280\n",
      "\n",
      "Train set: Average log loss: -17.5449\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4073\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2555 [0/90000 (0%)]\tlog Loss: -17.530593\n",
      "\n",
      "Train set: Average log loss: -17.5450\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4075\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2556 [0/90000 (0%)]\tlog Loss: -17.530425\n",
      "\n",
      "Train set: Average log loss: -17.5451\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4075\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2557 [0/90000 (0%)]\tlog Loss: -17.530585\n",
      "\n",
      "Train set: Average log loss: -17.5452\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4076\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2558 [0/90000 (0%)]\tlog Loss: -17.530749\n",
      "\n",
      "Train set: Average log loss: -17.5452\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4077\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2559 [0/90000 (0%)]\tlog Loss: -17.530858\n",
      "\n",
      "Train set: Average log loss: -17.5453\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4078\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2560 [0/90000 (0%)]\tlog Loss: -17.530909\n",
      "\n",
      "Train set: Average log loss: -17.5454\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2561 [0/90000 (0%)]\tlog Loss: -17.531040\n",
      "\n",
      "Train set: Average log loss: -17.5455\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4080\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2562 [0/90000 (0%)]\tlog Loss: -17.531121\n",
      "\n",
      "Train set: Average log loss: -17.5456\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4081\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2563 [0/90000 (0%)]\tlog Loss: -17.531225\n",
      "\n",
      "Train set: Average log loss: -17.5457\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4083\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2564 [0/90000 (0%)]\tlog Loss: -17.531551\n",
      "\n",
      "Train set: Average log loss: -17.5458\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4083\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2565 [0/90000 (0%)]\tlog Loss: -17.531458\n",
      "\n",
      "Train set: Average log loss: -17.5459\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4085\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2566 [0/90000 (0%)]\tlog Loss: -17.531903\n",
      "\n",
      "Train set: Average log loss: -17.5460\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4086\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2567 [0/90000 (0%)]\tlog Loss: -17.531954\n",
      "\n",
      "Train set: Average log loss: -17.5461\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4087\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2568 [0/90000 (0%)]\tlog Loss: -17.532029\n",
      "\n",
      "Train set: Average log loss: -17.5461\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4088\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2569 [0/90000 (0%)]\tlog Loss: -17.532173\n",
      "\n",
      "Train set: Average log loss: -17.5462\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4088\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2570 [0/90000 (0%)]\tlog Loss: -17.532126\n",
      "\n",
      "Train set: Average log loss: -17.5462\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2571 [0/90000 (0%)]\tlog Loss: -17.532295\n",
      "\n",
      "Train set: Average log loss: -17.5463\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4090\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2572 [0/90000 (0%)]\tlog Loss: -17.532417\n",
      "\n",
      "Train set: Average log loss: -17.5464\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4091\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2573 [0/90000 (0%)]\tlog Loss: -17.532575\n",
      "\n",
      "Train set: Average log loss: -17.5465\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4092\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2574 [0/90000 (0%)]\tlog Loss: -17.532765\n",
      "\n",
      "Train set: Average log loss: -17.5466\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4094\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2575 [0/90000 (0%)]\tlog Loss: -17.532786\n",
      "\n",
      "Train set: Average log loss: -17.5467\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4093\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2576 [0/90000 (0%)]\tlog Loss: -17.532970\n",
      "\n",
      "Train set: Average log loss: -17.5467\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4095\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2577 [0/90000 (0%)]\tlog Loss: -17.532979\n",
      "\n",
      "Train set: Average log loss: -17.5469\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4096\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2578 [0/90000 (0%)]\tlog Loss: -17.533032\n",
      "\n",
      "Train set: Average log loss: -17.5469\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4096\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2579 [0/90000 (0%)]\tlog Loss: -17.533238\n",
      "\n",
      "Train set: Average log loss: -17.5470\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4098\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2580 [0/90000 (0%)]\tlog Loss: -17.533455\n",
      "\n",
      "Train set: Average log loss: -17.5471\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4099\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2581 [0/90000 (0%)]\tlog Loss: -17.533497\n",
      "\n",
      "Train set: Average log loss: -17.5472\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4100\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2582 [0/90000 (0%)]\tlog Loss: -17.533614\n",
      "\n",
      "Train set: Average log loss: -17.5472\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4102\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2583 [0/90000 (0%)]\tlog Loss: -17.533810\n",
      "\n",
      "Train set: Average log loss: -17.5473\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4103\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2584 [0/90000 (0%)]\tlog Loss: -17.533987\n",
      "\n",
      "Train set: Average log loss: -17.5474\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4104\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2585 [0/90000 (0%)]\tlog Loss: -17.534110\n",
      "\n",
      "Train set: Average log loss: -17.5475\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4105\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2586 [0/90000 (0%)]\tlog Loss: -17.534138\n",
      "\n",
      "Train set: Average log loss: -17.5476\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4106\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2587 [0/90000 (0%)]\tlog Loss: -17.534354\n",
      "\n",
      "Train set: Average log loss: -17.5476\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4106\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2588 [0/90000 (0%)]\tlog Loss: -17.534347\n",
      "\n",
      "Train set: Average log loss: -17.5477\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4107\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2589 [0/90000 (0%)]\tlog Loss: -17.534484\n",
      "\n",
      "Train set: Average log loss: -17.5478\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4109\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2590 [0/90000 (0%)]\tlog Loss: -17.534565\n",
      "\n",
      "Train set: Average log loss: -17.5479\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4109\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2591 [0/90000 (0%)]\tlog Loss: -17.534606\n",
      "\n",
      "Train set: Average log loss: -17.5480\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4110\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2592 [0/90000 (0%)]\tlog Loss: -17.534727\n",
      "\n",
      "Train set: Average log loss: -17.5480\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4111\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2593 [0/90000 (0%)]\tlog Loss: -17.534923\n",
      "\n",
      "Train set: Average log loss: -17.5482\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4112\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2594 [0/90000 (0%)]\tlog Loss: -17.535082\n",
      "\n",
      "Train set: Average log loss: -17.5482\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4113\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2595 [0/90000 (0%)]\tlog Loss: -17.535108\n",
      "\n",
      "Train set: Average log loss: -17.5483\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4114\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2596 [0/90000 (0%)]\tlog Loss: -17.535163\n",
      "\n",
      "Train set: Average log loss: -17.5484\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4114\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2597 [0/90000 (0%)]\tlog Loss: -17.535186\n",
      "\n",
      "Train set: Average log loss: -17.5484\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4115\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2598 [0/90000 (0%)]\tlog Loss: -17.535250\n",
      "\n",
      "Train set: Average log loss: -17.5485\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4115\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2599 [0/90000 (0%)]\tlog Loss: -17.535262\n",
      "\n",
      "Train set: Average log loss: -17.5486\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4116\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2600 [0/90000 (0%)]\tlog Loss: -17.535403\n",
      "\n",
      "Train set: Average log loss: -17.5487\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4117\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2601 [0/90000 (0%)]\tlog Loss: -17.535355\n",
      "\n",
      "Train set: Average log loss: -17.5488\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4119\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2602 [0/90000 (0%)]\tlog Loss: -17.535626\n",
      "\n",
      "Train set: Average log loss: -17.5488\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4120\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2603 [0/90000 (0%)]\tlog Loss: -17.535675\n",
      "\n",
      "Train set: Average log loss: -17.5489\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4121\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2604 [0/90000 (0%)]\tlog Loss: -17.535826\n",
      "\n",
      "Train set: Average log loss: -17.5490\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4121\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2605 [0/90000 (0%)]\tlog Loss: -17.535807\n",
      "\n",
      "Train set: Average log loss: -17.5491\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4121\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2606 [0/90000 (0%)]\tlog Loss: -17.535848\n",
      "\n",
      "Train set: Average log loss: -17.5492\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4123\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2607 [0/90000 (0%)]\tlog Loss: -17.535929\n",
      "\n",
      "Train set: Average log loss: -17.5492\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4124\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2608 [0/90000 (0%)]\tlog Loss: -17.536170\n",
      "\n",
      "Train set: Average log loss: -17.5493\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4126\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2609 [0/90000 (0%)]\tlog Loss: -17.536183\n",
      "\n",
      "Train set: Average log loss: -17.5494\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4126\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2610 [0/90000 (0%)]\tlog Loss: -17.536399\n",
      "\n",
      "Train set: Average log loss: -17.5495\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4127\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2611 [0/90000 (0%)]\tlog Loss: -17.536468\n",
      "\n",
      "Train set: Average log loss: -17.5495\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4128\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2612 [0/90000 (0%)]\tlog Loss: -17.536438\n",
      "\n",
      "Train set: Average log loss: -17.5496\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4129\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2613 [0/90000 (0%)]\tlog Loss: -17.536647\n",
      "\n",
      "Train set: Average log loss: -17.5497\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4130\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2614 [0/90000 (0%)]\tlog Loss: -17.536592\n",
      "\n",
      "Train set: Average log loss: -17.5498\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4130\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2615 [0/90000 (0%)]\tlog Loss: -17.536614\n",
      "\n",
      "Train set: Average log loss: -17.5499\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4132\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2616 [0/90000 (0%)]\tlog Loss: -17.536920\n",
      "\n",
      "Train set: Average log loss: -17.5499\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4133\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2617 [0/90000 (0%)]\tlog Loss: -17.536873\n",
      "\n",
      "Train set: Average log loss: -17.5500\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4133\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2618 [0/90000 (0%)]\tlog Loss: -17.537056\n",
      "\n",
      "Train set: Average log loss: -17.5501\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4134\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2619 [0/90000 (0%)]\tlog Loss: -17.537141\n",
      "\n",
      "Train set: Average log loss: -17.5502\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4135\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2620 [0/90000 (0%)]\tlog Loss: -17.537231\n",
      "\n",
      "Train set: Average log loss: -17.5503\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4135\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2621 [0/90000 (0%)]\tlog Loss: -17.537238\n",
      "\n",
      "Train set: Average log loss: -17.5504\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4136\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2622 [0/90000 (0%)]\tlog Loss: -17.537278\n",
      "\n",
      "Train set: Average log loss: -17.5504\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4136\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2623 [0/90000 (0%)]\tlog Loss: -17.537303\n",
      "\n",
      "Train set: Average log loss: -17.5505\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4138\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2624 [0/90000 (0%)]\tlog Loss: -17.537449\n",
      "\n",
      "Train set: Average log loss: -17.5506\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4139\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2625 [0/90000 (0%)]\tlog Loss: -17.537683\n",
      "\n",
      "Train set: Average log loss: -17.5507\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4140\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2626 [0/90000 (0%)]\tlog Loss: -17.537924\n",
      "\n",
      "Train set: Average log loss: -17.5508\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4141\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2627 [0/90000 (0%)]\tlog Loss: -17.537868\n",
      "\n",
      "Train set: Average log loss: -17.5509\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4141\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2628 [0/90000 (0%)]\tlog Loss: -17.538041\n",
      "\n",
      "Train set: Average log loss: -17.5509\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4143\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2629 [0/90000 (0%)]\tlog Loss: -17.538115\n",
      "\n",
      "Train set: Average log loss: -17.5510\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4143\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2630 [0/90000 (0%)]\tlog Loss: -17.538201\n",
      "\n",
      "Train set: Average log loss: -17.5511\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4144\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2631 [0/90000 (0%)]\tlog Loss: -17.538246\n",
      "\n",
      "Train set: Average log loss: -17.5511\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4144\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2632 [0/90000 (0%)]\tlog Loss: -17.538394\n",
      "\n",
      "Train set: Average log loss: -17.5512\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4144\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2633 [0/90000 (0%)]\tlog Loss: -17.538403\n",
      "\n",
      "Train set: Average log loss: -17.5513\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4145\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2634 [0/90000 (0%)]\tlog Loss: -17.538505\n",
      "\n",
      "Train set: Average log loss: -17.5514\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4146\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2635 [0/90000 (0%)]\tlog Loss: -17.538649\n",
      "\n",
      "Train set: Average log loss: -17.5514\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4147\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2636 [0/90000 (0%)]\tlog Loss: -17.538709\n",
      "\n",
      "Train set: Average log loss: -17.5515\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4147\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2637 [0/90000 (0%)]\tlog Loss: -17.538792\n",
      "\n",
      "Train set: Average log loss: -17.5516\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4148\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2638 [0/90000 (0%)]\tlog Loss: -17.538911\n",
      "\n",
      "Train set: Average log loss: -17.5517\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4149\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2639 [0/90000 (0%)]\tlog Loss: -17.538939\n",
      "\n",
      "Train set: Average log loss: -17.5518\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4150\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2640 [0/90000 (0%)]\tlog Loss: -17.539158\n",
      "\n",
      "Train set: Average log loss: -17.5519\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4150\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2641 [0/90000 (0%)]\tlog Loss: -17.539191\n",
      "\n",
      "Train set: Average log loss: -17.5519\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4151\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2642 [0/90000 (0%)]\tlog Loss: -17.539180\n",
      "\n",
      "Train set: Average log loss: -17.5520\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4152\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2643 [0/90000 (0%)]\tlog Loss: -17.539336\n",
      "\n",
      "Train set: Average log loss: -17.5521\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4153\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2644 [0/90000 (0%)]\tlog Loss: -17.539582\n",
      "\n",
      "Train set: Average log loss: -17.5522\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4153\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2645 [0/90000 (0%)]\tlog Loss: -17.539645\n",
      "\n",
      "Train set: Average log loss: -17.5522\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4154\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2646 [0/90000 (0%)]\tlog Loss: -17.539709\n",
      "\n",
      "Train set: Average log loss: -17.5523\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4155\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2647 [0/90000 (0%)]\tlog Loss: -17.539831\n",
      "\n",
      "Train set: Average log loss: -17.5524\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4154\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2648 [0/90000 (0%)]\tlog Loss: -17.539759\n",
      "\n",
      "Train set: Average log loss: -17.5524\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4154\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2649 [0/90000 (0%)]\tlog Loss: -17.539839\n",
      "\n",
      "Train set: Average log loss: -17.5525\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4156\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2650 [0/90000 (0%)]\tlog Loss: -17.539887\n",
      "\n",
      "Train set: Average log loss: -17.5526\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4156\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2651 [0/90000 (0%)]\tlog Loss: -17.540009\n",
      "\n",
      "Train set: Average log loss: -17.5527\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4156\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2652 [0/90000 (0%)]\tlog Loss: -17.540003\n",
      "\n",
      "Train set: Average log loss: -17.5528\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4157\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2653 [0/90000 (0%)]\tlog Loss: -17.540164\n",
      "\n",
      "Train set: Average log loss: -17.5529\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4158\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2654 [0/90000 (0%)]\tlog Loss: -17.540309\n",
      "\n",
      "Train set: Average log loss: -17.5529\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4159\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2655 [0/90000 (0%)]\tlog Loss: -17.540212\n",
      "\n",
      "Train set: Average log loss: -17.5530\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4160\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2656 [0/90000 (0%)]\tlog Loss: -17.540492\n",
      "\n",
      "Train set: Average log loss: -17.5531\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4160\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2657 [0/90000 (0%)]\tlog Loss: -17.540407\n",
      "\n",
      "Train set: Average log loss: -17.5532\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4160\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2658 [0/90000 (0%)]\tlog Loss: -17.540531\n",
      "\n",
      "Train set: Average log loss: -17.5532\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4161\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2659 [0/90000 (0%)]\tlog Loss: -17.540499\n",
      "\n",
      "Train set: Average log loss: -17.5533\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4161\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2660 [0/90000 (0%)]\tlog Loss: -17.540628\n",
      "\n",
      "Train set: Average log loss: -17.5534\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4162\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2661 [0/90000 (0%)]\tlog Loss: -17.540718\n",
      "\n",
      "Train set: Average log loss: -17.5535\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4162\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2662 [0/90000 (0%)]\tlog Loss: -17.540709\n",
      "\n",
      "Train set: Average log loss: -17.5535\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4163\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2663 [0/90000 (0%)]\tlog Loss: -17.540814\n",
      "\n",
      "Train set: Average log loss: -17.5536\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4162\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2664 [0/90000 (0%)]\tlog Loss: -17.540799\n",
      "\n",
      "Train set: Average log loss: -17.5536\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4163\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2665 [0/90000 (0%)]\tlog Loss: -17.540879\n",
      "\n",
      "Train set: Average log loss: -17.5538\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4164\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2666 [0/90000 (0%)]\tlog Loss: -17.540890\n",
      "\n",
      "Train set: Average log loss: -17.5539\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4165\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2667 [0/90000 (0%)]\tlog Loss: -17.541026\n",
      "\n",
      "Train set: Average log loss: -17.5540\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4166\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2668 [0/90000 (0%)]\tlog Loss: -17.541279\n",
      "\n",
      "Train set: Average log loss: -17.5540\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4166\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2669 [0/90000 (0%)]\tlog Loss: -17.541305\n",
      "\n",
      "Train set: Average log loss: -17.5541\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4167\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2670 [0/90000 (0%)]\tlog Loss: -17.541396\n",
      "\n",
      "Train set: Average log loss: -17.5541\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4167\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2671 [0/90000 (0%)]\tlog Loss: -17.541397\n",
      "\n",
      "Train set: Average log loss: -17.5543\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4167\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2672 [0/90000 (0%)]\tlog Loss: -17.541508\n",
      "\n",
      "Train set: Average log loss: -17.5543\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4169\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2673 [0/90000 (0%)]\tlog Loss: -17.541438\n",
      "\n",
      "Train set: Average log loss: -17.5544\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4169\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2674 [0/90000 (0%)]\tlog Loss: -17.541823\n",
      "\n",
      "Train set: Average log loss: -17.5545\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4171\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2675 [0/90000 (0%)]\tlog Loss: -17.541830\n",
      "\n",
      "Train set: Average log loss: -17.5546\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4172\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2676 [0/90000 (0%)]\tlog Loss: -17.541921\n",
      "\n",
      "Train set: Average log loss: -17.5547\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4172\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2677 [0/90000 (0%)]\tlog Loss: -17.542079\n",
      "\n",
      "Train set: Average log loss: -17.5547\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4173\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2678 [0/90000 (0%)]\tlog Loss: -17.542111\n",
      "\n",
      "Train set: Average log loss: -17.5548\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4173\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2679 [0/90000 (0%)]\tlog Loss: -17.542161\n",
      "\n",
      "Train set: Average log loss: -17.5549\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4174\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2680 [0/90000 (0%)]\tlog Loss: -17.542186\n",
      "\n",
      "Train set: Average log loss: -17.5550\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4175\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2681 [0/90000 (0%)]\tlog Loss: -17.542375\n",
      "\n",
      "Train set: Average log loss: -17.5551\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4176\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2682 [0/90000 (0%)]\tlog Loss: -17.542549\n",
      "\n",
      "Train set: Average log loss: -17.5552\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4177\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2683 [0/90000 (0%)]\tlog Loss: -17.542702\n",
      "\n",
      "Train set: Average log loss: -17.5552\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4178\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2684 [0/90000 (0%)]\tlog Loss: -17.542748\n",
      "\n",
      "Train set: Average log loss: -17.5554\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4179\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2685 [0/90000 (0%)]\tlog Loss: -17.542874\n",
      "\n",
      "Train set: Average log loss: -17.5555\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4180\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2686 [0/90000 (0%)]\tlog Loss: -17.543073\n",
      "\n",
      "Train set: Average log loss: -17.5555\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4182\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2687 [0/90000 (0%)]\tlog Loss: -17.543220\n",
      "\n",
      "Train set: Average log loss: -17.5556\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4184\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2688 [0/90000 (0%)]\tlog Loss: -17.543406\n",
      "\n",
      "Train set: Average log loss: -17.5557\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4185\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2689 [0/90000 (0%)]\tlog Loss: -17.543509\n",
      "\n",
      "Train set: Average log loss: -17.5557\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4185\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2690 [0/90000 (0%)]\tlog Loss: -17.543498\n",
      "\n",
      "Train set: Average log loss: -17.5558\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4186\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2691 [0/90000 (0%)]\tlog Loss: -17.543652\n",
      "\n",
      "Train set: Average log loss: -17.5559\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4187\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2692 [0/90000 (0%)]\tlog Loss: -17.543742\n",
      "\n",
      "Train set: Average log loss: -17.5560\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4188\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2693 [0/90000 (0%)]\tlog Loss: -17.543959\n",
      "\n",
      "Train set: Average log loss: -17.5561\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4189\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2694 [0/90000 (0%)]\tlog Loss: -17.544107\n",
      "\n",
      "Train set: Average log loss: -17.5562\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4190\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2695 [0/90000 (0%)]\tlog Loss: -17.544135\n",
      "\n",
      "Train set: Average log loss: -17.5562\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4191\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2696 [0/90000 (0%)]\tlog Loss: -17.544332\n",
      "\n",
      "Train set: Average log loss: -17.5563\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4192\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2697 [0/90000 (0%)]\tlog Loss: -17.544382\n",
      "\n",
      "Train set: Average log loss: -17.5564\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4192\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2698 [0/90000 (0%)]\tlog Loss: -17.544388\n",
      "\n",
      "Train set: Average log loss: -17.5564\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4194\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2699 [0/90000 (0%)]\tlog Loss: -17.544533\n",
      "\n",
      "Train set: Average log loss: -17.5566\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4195\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2700 [0/90000 (0%)]\tlog Loss: -17.544606\n",
      "\n",
      "Train set: Average log loss: -17.5566\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4194\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2701 [0/90000 (0%)]\tlog Loss: -17.544601\n",
      "\n",
      "Train set: Average log loss: -17.5567\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4196\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2702 [0/90000 (0%)]\tlog Loss: -17.544738\n",
      "\n",
      "Train set: Average log loss: -17.5568\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4197\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2703 [0/90000 (0%)]\tlog Loss: -17.544962\n",
      "\n",
      "Train set: Average log loss: -17.5569\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4197\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2704 [0/90000 (0%)]\tlog Loss: -17.545122\n",
      "\n",
      "Train set: Average log loss: -17.5570\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4198\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2705 [0/90000 (0%)]\tlog Loss: -17.545082\n",
      "\n",
      "Train set: Average log loss: -17.5570\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4199\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2706 [0/90000 (0%)]\tlog Loss: -17.545251\n",
      "\n",
      "Train set: Average log loss: -17.5571\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4200\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2707 [0/90000 (0%)]\tlog Loss: -17.545306\n",
      "\n",
      "Train set: Average log loss: -17.5572\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4200\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2708 [0/90000 (0%)]\tlog Loss: -17.545404\n",
      "\n",
      "Train set: Average log loss: -17.5572\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4201\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2709 [0/90000 (0%)]\tlog Loss: -17.545433\n",
      "\n",
      "Train set: Average log loss: -17.5573\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4202\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2710 [0/90000 (0%)]\tlog Loss: -17.545474\n",
      "\n",
      "Train set: Average log loss: -17.5574\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4202\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2711 [0/90000 (0%)]\tlog Loss: -17.545473\n",
      "\n",
      "Train set: Average log loss: -17.5575\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4203\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2712 [0/90000 (0%)]\tlog Loss: -17.545547\n",
      "\n",
      "Train set: Average log loss: -17.5576\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4205\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2713 [0/90000 (0%)]\tlog Loss: -17.545774\n",
      "\n",
      "Train set: Average log loss: -17.5577\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4205\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2714 [0/90000 (0%)]\tlog Loss: -17.545779\n",
      "\n",
      "Train set: Average log loss: -17.5577\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4206\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2715 [0/90000 (0%)]\tlog Loss: -17.546012\n",
      "\n",
      "Train set: Average log loss: -17.5578\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4206\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2716 [0/90000 (0%)]\tlog Loss: -17.545907\n",
      "\n",
      "Train set: Average log loss: -17.5579\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4207\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2717 [0/90000 (0%)]\tlog Loss: -17.546111\n",
      "\n",
      "Train set: Average log loss: -17.5580\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4208\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2718 [0/90000 (0%)]\tlog Loss: -17.546271\n",
      "\n",
      "Train set: Average log loss: -17.5580\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4209\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2719 [0/90000 (0%)]\tlog Loss: -17.546159\n",
      "\n",
      "Train set: Average log loss: -17.5581\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4210\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2720 [0/90000 (0%)]\tlog Loss: -17.546385\n",
      "\n",
      "Train set: Average log loss: -17.5582\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4211\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2721 [0/90000 (0%)]\tlog Loss: -17.546449\n",
      "\n",
      "Train set: Average log loss: -17.5583\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4212\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2722 [0/90000 (0%)]\tlog Loss: -17.546632\n",
      "\n",
      "Train set: Average log loss: -17.5583\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4212\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2723 [0/90000 (0%)]\tlog Loss: -17.546535\n",
      "\n",
      "Train set: Average log loss: -17.5584\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4213\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2724 [0/90000 (0%)]\tlog Loss: -17.546701\n",
      "\n",
      "Train set: Average log loss: -17.5585\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4214\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2725 [0/90000 (0%)]\tlog Loss: -17.546742\n",
      "\n",
      "Train set: Average log loss: -17.5586\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4215\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2726 [0/90000 (0%)]\tlog Loss: -17.546828\n",
      "\n",
      "Train set: Average log loss: -17.5587\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4216\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2727 [0/90000 (0%)]\tlog Loss: -17.547043\n",
      "\n",
      "Train set: Average log loss: -17.5588\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4218\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2728 [0/90000 (0%)]\tlog Loss: -17.547213\n",
      "\n",
      "Train set: Average log loss: -17.5589\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4219\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2729 [0/90000 (0%)]\tlog Loss: -17.547277\n",
      "\n",
      "Train set: Average log loss: -17.5590\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4220\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2730 [0/90000 (0%)]\tlog Loss: -17.547389\n",
      "\n",
      "Train set: Average log loss: -17.5591\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4221\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2731 [0/90000 (0%)]\tlog Loss: -17.547415\n",
      "\n",
      "Train set: Average log loss: -17.5591\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4222\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2732 [0/90000 (0%)]\tlog Loss: -17.547670\n",
      "\n",
      "Train set: Average log loss: -17.5592\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4224\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2733 [0/90000 (0%)]\tlog Loss: -17.547828\n",
      "\n",
      "Train set: Average log loss: -17.5593\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4225\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2734 [0/90000 (0%)]\tlog Loss: -17.547926\n",
      "\n",
      "Train set: Average log loss: -17.5594\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4227\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2735 [0/90000 (0%)]\tlog Loss: -17.548153\n",
      "\n",
      "Train set: Average log loss: -17.5596\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4229\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2736 [0/90000 (0%)]\tlog Loss: -17.548415\n",
      "\n",
      "Train set: Average log loss: -17.5596\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4230\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2737 [0/90000 (0%)]\tlog Loss: -17.548608\n",
      "\n",
      "Train set: Average log loss: -17.5597\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4231\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2738 [0/90000 (0%)]\tlog Loss: -17.548578\n",
      "\n",
      "Train set: Average log loss: -17.5597\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4232\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2739 [0/90000 (0%)]\tlog Loss: -17.548781\n",
      "\n",
      "Train set: Average log loss: -17.5598\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4233\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2740 [0/90000 (0%)]\tlog Loss: -17.548820\n",
      "\n",
      "Train set: Average log loss: -17.5599\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4235\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2741 [0/90000 (0%)]\tlog Loss: -17.549066\n",
      "\n",
      "Train set: Average log loss: -17.5600\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4237\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2742 [0/90000 (0%)]\tlog Loss: -17.549296\n",
      "\n",
      "Train set: Average log loss: -17.5601\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4237\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2743 [0/90000 (0%)]\tlog Loss: -17.549442\n",
      "\n",
      "Train set: Average log loss: -17.5602\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4239\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2744 [0/90000 (0%)]\tlog Loss: -17.549572\n",
      "\n",
      "Train set: Average log loss: -17.5602\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4239\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2745 [0/90000 (0%)]\tlog Loss: -17.549658\n",
      "\n",
      "Train set: Average log loss: -17.5603\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4240\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2746 [0/90000 (0%)]\tlog Loss: -17.549718\n",
      "\n",
      "Train set: Average log loss: -17.5604\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4241\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2747 [0/90000 (0%)]\tlog Loss: -17.549704\n",
      "\n",
      "Train set: Average log loss: -17.5605\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4242\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2748 [0/90000 (0%)]\tlog Loss: -17.550008\n",
      "\n",
      "Train set: Average log loss: -17.5606\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4243\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2749 [0/90000 (0%)]\tlog Loss: -17.549998\n",
      "\n",
      "Train set: Average log loss: -17.5606\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4245\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2750 [0/90000 (0%)]\tlog Loss: -17.550270\n",
      "\n",
      "Train set: Average log loss: -17.5607\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4246\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2751 [0/90000 (0%)]\tlog Loss: -17.550347\n",
      "\n",
      "Train set: Average log loss: -17.5608\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4246\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2752 [0/90000 (0%)]\tlog Loss: -17.550374\n",
      "\n",
      "Train set: Average log loss: -17.5609\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4247\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2753 [0/90000 (0%)]\tlog Loss: -17.550426\n",
      "\n",
      "Train set: Average log loss: -17.5609\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4248\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2754 [0/90000 (0%)]\tlog Loss: -17.550647\n",
      "\n",
      "Train set: Average log loss: -17.5610\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4249\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2755 [0/90000 (0%)]\tlog Loss: -17.550629\n",
      "\n",
      "Train set: Average log loss: -17.5611\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4250\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2756 [0/90000 (0%)]\tlog Loss: -17.550905\n",
      "\n",
      "Train set: Average log loss: -17.5612\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4251\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2757 [0/90000 (0%)]\tlog Loss: -17.551043\n",
      "\n",
      "Train set: Average log loss: -17.5613\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4252\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2758 [0/90000 (0%)]\tlog Loss: -17.551172\n",
      "\n",
      "Train set: Average log loss: -17.5613\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4253\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2759 [0/90000 (0%)]\tlog Loss: -17.551308\n",
      "\n",
      "Train set: Average log loss: -17.5614\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4253\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2760 [0/90000 (0%)]\tlog Loss: -17.551350\n",
      "\n",
      "Train set: Average log loss: -17.5615\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4256\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2761 [0/90000 (0%)]\tlog Loss: -17.551679\n",
      "\n",
      "Train set: Average log loss: -17.5616\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4256\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2762 [0/90000 (0%)]\tlog Loss: -17.551683\n",
      "\n",
      "Train set: Average log loss: -17.5616\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4256\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2763 [0/90000 (0%)]\tlog Loss: -17.551821\n",
      "\n",
      "Train set: Average log loss: -17.5617\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4258\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2764 [0/90000 (0%)]\tlog Loss: -17.551898\n",
      "\n",
      "Train set: Average log loss: -17.5618\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4258\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2765 [0/90000 (0%)]\tlog Loss: -17.552056\n",
      "\n",
      "Train set: Average log loss: -17.5619\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4259\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2766 [0/90000 (0%)]\tlog Loss: -17.552066\n",
      "\n",
      "Train set: Average log loss: -17.5620\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4260\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2767 [0/90000 (0%)]\tlog Loss: -17.552266\n",
      "\n",
      "Train set: Average log loss: -17.5620\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4261\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2768 [0/90000 (0%)]\tlog Loss: -17.552298\n",
      "\n",
      "Train set: Average log loss: -17.5621\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4261\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2769 [0/90000 (0%)]\tlog Loss: -17.552398\n",
      "\n",
      "Train set: Average log loss: -17.5622\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4262\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2770 [0/90000 (0%)]\tlog Loss: -17.552341\n",
      "\n",
      "Train set: Average log loss: -17.5623\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4263\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2771 [0/90000 (0%)]\tlog Loss: -17.552433\n",
      "\n",
      "Train set: Average log loss: -17.5623\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4263\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2772 [0/90000 (0%)]\tlog Loss: -17.552530\n",
      "\n",
      "Train set: Average log loss: -17.5624\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4264\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2773 [0/90000 (0%)]\tlog Loss: -17.552683\n",
      "\n",
      "Train set: Average log loss: -17.5625\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4265\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2774 [0/90000 (0%)]\tlog Loss: -17.552703\n",
      "\n",
      "Train set: Average log loss: -17.5626\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4266\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2775 [0/90000 (0%)]\tlog Loss: -17.552858\n",
      "\n",
      "Train set: Average log loss: -17.5626\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4266\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2776 [0/90000 (0%)]\tlog Loss: -17.552900\n",
      "\n",
      "Train set: Average log loss: -17.5627\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4267\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2777 [0/90000 (0%)]\tlog Loss: -17.553019\n",
      "\n",
      "Train set: Average log loss: -17.5628\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4268\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2778 [0/90000 (0%)]\tlog Loss: -17.553074\n",
      "\n",
      "Train set: Average log loss: -17.5629\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4268\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2779 [0/90000 (0%)]\tlog Loss: -17.553150\n",
      "\n",
      "Train set: Average log loss: -17.5629\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4269\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2780 [0/90000 (0%)]\tlog Loss: -17.553178\n",
      "\n",
      "Train set: Average log loss: -17.5630\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4269\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2781 [0/90000 (0%)]\tlog Loss: -17.553154\n",
      "\n",
      "Train set: Average log loss: -17.5631\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4270\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2782 [0/90000 (0%)]\tlog Loss: -17.553276\n",
      "\n",
      "Train set: Average log loss: -17.5632\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4270\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2783 [0/90000 (0%)]\tlog Loss: -17.553368\n",
      "\n",
      "Train set: Average log loss: -17.5632\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4271\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2784 [0/90000 (0%)]\tlog Loss: -17.553531\n",
      "\n",
      "Train set: Average log loss: -17.5633\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4272\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2785 [0/90000 (0%)]\tlog Loss: -17.553561\n",
      "\n",
      "Train set: Average log loss: -17.5634\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4272\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2786 [0/90000 (0%)]\tlog Loss: -17.553543\n",
      "\n",
      "Train set: Average log loss: -17.5634\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4273\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2787 [0/90000 (0%)]\tlog Loss: -17.553500\n",
      "\n",
      "Train set: Average log loss: -17.5636\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4273\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2788 [0/90000 (0%)]\tlog Loss: -17.553707\n",
      "\n",
      "Train set: Average log loss: -17.5636\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4274\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2789 [0/90000 (0%)]\tlog Loss: -17.553704\n",
      "\n",
      "Train set: Average log loss: -17.5636\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4274\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2790 [0/90000 (0%)]\tlog Loss: -17.553649\n",
      "\n",
      "Train set: Average log loss: -17.5637\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4274\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2791 [0/90000 (0%)]\tlog Loss: -17.553801\n",
      "\n",
      "Train set: Average log loss: -17.5638\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4275\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2792 [0/90000 (0%)]\tlog Loss: -17.553833\n",
      "\n",
      "Train set: Average log loss: -17.5639\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4275\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2793 [0/90000 (0%)]\tlog Loss: -17.553915\n",
      "\n",
      "Train set: Average log loss: -17.5640\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4276\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2794 [0/90000 (0%)]\tlog Loss: -17.554032\n",
      "\n",
      "Train set: Average log loss: -17.5641\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4277\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2795 [0/90000 (0%)]\tlog Loss: -17.554186\n",
      "\n",
      "Train set: Average log loss: -17.5642\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4278\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2796 [0/90000 (0%)]\tlog Loss: -17.554214\n",
      "\n",
      "Train set: Average log loss: -17.5642\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4279\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2797 [0/90000 (0%)]\tlog Loss: -17.554375\n",
      "\n",
      "Train set: Average log loss: -17.5643\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4279\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2798 [0/90000 (0%)]\tlog Loss: -17.554401\n",
      "\n",
      "Train set: Average log loss: -17.5644\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4279\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2799 [0/90000 (0%)]\tlog Loss: -17.554528\n",
      "\n",
      "Train set: Average log loss: -17.5644\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4280\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2800 [0/90000 (0%)]\tlog Loss: -17.554504\n",
      "\n",
      "Train set: Average log loss: -17.5645\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4280\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2801 [0/90000 (0%)]\tlog Loss: -17.554587\n",
      "\n",
      "Train set: Average log loss: -17.5646\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4282\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2802 [0/90000 (0%)]\tlog Loss: -17.554611\n",
      "\n",
      "Train set: Average log loss: -17.5646\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4283\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2803 [0/90000 (0%)]\tlog Loss: -17.554782\n",
      "\n",
      "Train set: Average log loss: -17.5648\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4283\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2804 [0/90000 (0%)]\tlog Loss: -17.554850\n",
      "\n",
      "Train set: Average log loss: -17.5648\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4284\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2805 [0/90000 (0%)]\tlog Loss: -17.554832\n",
      "\n",
      "Train set: Average log loss: -17.5649\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4284\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2806 [0/90000 (0%)]\tlog Loss: -17.554892\n",
      "\n",
      "Train set: Average log loss: -17.5650\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4285\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2807 [0/90000 (0%)]\tlog Loss: -17.555014\n",
      "\n",
      "Train set: Average log loss: -17.5650\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4285\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2808 [0/90000 (0%)]\tlog Loss: -17.555013\n",
      "\n",
      "Train set: Average log loss: -17.5651\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4285\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2809 [0/90000 (0%)]\tlog Loss: -17.555019\n",
      "\n",
      "Train set: Average log loss: -17.5652\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4285\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2810 [0/90000 (0%)]\tlog Loss: -17.555220\n",
      "\n",
      "Train set: Average log loss: -17.5652\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4286\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2811 [0/90000 (0%)]\tlog Loss: -17.555285\n",
      "\n",
      "Train set: Average log loss: -17.5654\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4287\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2812 [0/90000 (0%)]\tlog Loss: -17.555351\n",
      "\n",
      "Train set: Average log loss: -17.5654\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4288\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2813 [0/90000 (0%)]\tlog Loss: -17.555416\n",
      "\n",
      "Train set: Average log loss: -17.5655\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4288\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2814 [0/90000 (0%)]\tlog Loss: -17.555450\n",
      "\n",
      "Train set: Average log loss: -17.5656\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4289\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2815 [0/90000 (0%)]\tlog Loss: -17.555731\n",
      "\n",
      "Train set: Average log loss: -17.5657\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4290\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2816 [0/90000 (0%)]\tlog Loss: -17.555832\n",
      "\n",
      "Train set: Average log loss: -17.5657\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4292\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2817 [0/90000 (0%)]\tlog Loss: -17.555865\n",
      "\n",
      "Train set: Average log loss: -17.5658\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4292\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2818 [0/90000 (0%)]\tlog Loss: -17.556041\n",
      "\n",
      "Train set: Average log loss: -17.5659\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4293\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2819 [0/90000 (0%)]\tlog Loss: -17.556207\n",
      "\n",
      "Train set: Average log loss: -17.5660\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4294\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2820 [0/90000 (0%)]\tlog Loss: -17.556197\n",
      "\n",
      "Train set: Average log loss: -17.5660\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4295\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2821 [0/90000 (0%)]\tlog Loss: -17.556304\n",
      "\n",
      "Train set: Average log loss: -17.5661\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4295\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2822 [0/90000 (0%)]\tlog Loss: -17.556471\n",
      "\n",
      "Train set: Average log loss: -17.5662\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4297\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2823 [0/90000 (0%)]\tlog Loss: -17.556540\n",
      "\n",
      "Train set: Average log loss: -17.5663\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4298\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2824 [0/90000 (0%)]\tlog Loss: -17.556598\n",
      "\n",
      "Train set: Average log loss: -17.5664\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4298\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2825 [0/90000 (0%)]\tlog Loss: -17.556764\n",
      "\n",
      "Train set: Average log loss: -17.5664\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4299\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2826 [0/90000 (0%)]\tlog Loss: -17.556887\n",
      "\n",
      "Train set: Average log loss: -17.5665\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4300\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2827 [0/90000 (0%)]\tlog Loss: -17.556904\n",
      "\n",
      "Train set: Average log loss: -17.5666\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4301\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2828 [0/90000 (0%)]\tlog Loss: -17.557036\n",
      "\n",
      "Train set: Average log loss: -17.5667\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4301\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2829 [0/90000 (0%)]\tlog Loss: -17.557132\n",
      "\n",
      "Train set: Average log loss: -17.5667\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4302\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2830 [0/90000 (0%)]\tlog Loss: -17.557216\n",
      "\n",
      "Train set: Average log loss: -17.5668\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4303\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2831 [0/90000 (0%)]\tlog Loss: -17.557354\n",
      "\n",
      "Train set: Average log loss: -17.5669\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4303\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2832 [0/90000 (0%)]\tlog Loss: -17.557473\n",
      "\n",
      "Train set: Average log loss: -17.5669\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4304\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2833 [0/90000 (0%)]\tlog Loss: -17.557417\n",
      "\n",
      "Train set: Average log loss: -17.5670\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4305\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2834 [0/90000 (0%)]\tlog Loss: -17.557640\n",
      "\n",
      "Train set: Average log loss: -17.5671\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4305\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2835 [0/90000 (0%)]\tlog Loss: -17.557637\n",
      "\n",
      "Train set: Average log loss: -17.5672\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4307\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2836 [0/90000 (0%)]\tlog Loss: -17.557793\n",
      "\n",
      "Train set: Average log loss: -17.5673\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4307\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2837 [0/90000 (0%)]\tlog Loss: -17.557906\n",
      "\n",
      "Train set: Average log loss: -17.5673\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4309\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2838 [0/90000 (0%)]\tlog Loss: -17.558115\n",
      "\n",
      "Train set: Average log loss: -17.5674\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4310\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2839 [0/90000 (0%)]\tlog Loss: -17.558097\n",
      "\n",
      "Train set: Average log loss: -17.5675\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4311\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2840 [0/90000 (0%)]\tlog Loss: -17.558223\n",
      "\n",
      "Train set: Average log loss: -17.5676\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4311\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2841 [0/90000 (0%)]\tlog Loss: -17.558327\n",
      "\n",
      "Train set: Average log loss: -17.5677\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4312\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2842 [0/90000 (0%)]\tlog Loss: -17.558424\n",
      "\n",
      "Train set: Average log loss: -17.5677\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4313\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2843 [0/90000 (0%)]\tlog Loss: -17.558507\n",
      "\n",
      "Train set: Average log loss: -17.5679\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4315\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2844 [0/90000 (0%)]\tlog Loss: -17.558820\n",
      "\n",
      "Train set: Average log loss: -17.5679\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4315\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2845 [0/90000 (0%)]\tlog Loss: -17.558930\n",
      "\n",
      "Train set: Average log loss: -17.5680\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4317\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2846 [0/90000 (0%)]\tlog Loss: -17.559085\n",
      "\n",
      "Train set: Average log loss: -17.5681\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4318\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2847 [0/90000 (0%)]\tlog Loss: -17.559129\n",
      "\n",
      "Train set: Average log loss: -17.5682\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4319\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2848 [0/90000 (0%)]\tlog Loss: -17.559260\n",
      "\n",
      "Train set: Average log loss: -17.5683\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4321\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2849 [0/90000 (0%)]\tlog Loss: -17.559426\n",
      "\n",
      "Train set: Average log loss: -17.5684\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4322\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2850 [0/90000 (0%)]\tlog Loss: -17.559530\n",
      "\n",
      "Train set: Average log loss: -17.5684\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4323\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2851 [0/90000 (0%)]\tlog Loss: -17.559625\n",
      "\n",
      "Train set: Average log loss: -17.5685\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4324\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2852 [0/90000 (0%)]\tlog Loss: -17.559770\n",
      "\n",
      "Train set: Average log loss: -17.5686\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4325\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2853 [0/90000 (0%)]\tlog Loss: -17.559912\n",
      "\n",
      "Train set: Average log loss: -17.5687\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4326\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2854 [0/90000 (0%)]\tlog Loss: -17.559952\n",
      "\n",
      "Train set: Average log loss: -17.5687\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4327\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2855 [0/90000 (0%)]\tlog Loss: -17.560059\n",
      "\n",
      "Train set: Average log loss: -17.5688\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4327\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2856 [0/90000 (0%)]\tlog Loss: -17.560234\n",
      "\n",
      "Train set: Average log loss: -17.5689\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4329\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2857 [0/90000 (0%)]\tlog Loss: -17.560442\n",
      "\n",
      "Train set: Average log loss: -17.5690\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4329\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2858 [0/90000 (0%)]\tlog Loss: -17.560400\n",
      "\n",
      "Train set: Average log loss: -17.5690\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4331\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2859 [0/90000 (0%)]\tlog Loss: -17.560609\n",
      "\n",
      "Train set: Average log loss: -17.5691\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4332\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2860 [0/90000 (0%)]\tlog Loss: -17.560728\n",
      "\n",
      "Train set: Average log loss: -17.5692\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4332\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2861 [0/90000 (0%)]\tlog Loss: -17.560823\n",
      "\n",
      "Train set: Average log loss: -17.5692\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4333\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2862 [0/90000 (0%)]\tlog Loss: -17.560861\n",
      "\n",
      "Train set: Average log loss: -17.5693\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4334\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2863 [0/90000 (0%)]\tlog Loss: -17.561020\n",
      "\n",
      "Train set: Average log loss: -17.5694\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4335\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2864 [0/90000 (0%)]\tlog Loss: -17.561178\n",
      "\n",
      "Train set: Average log loss: -17.5695\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4336\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2865 [0/90000 (0%)]\tlog Loss: -17.561266\n",
      "\n",
      "Train set: Average log loss: -17.5695\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4336\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2866 [0/90000 (0%)]\tlog Loss: -17.561341\n",
      "\n",
      "Train set: Average log loss: -17.5696\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4337\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2867 [0/90000 (0%)]\tlog Loss: -17.561505\n",
      "\n",
      "Train set: Average log loss: -17.5697\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4338\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2868 [0/90000 (0%)]\tlog Loss: -17.561625\n",
      "\n",
      "Train set: Average log loss: -17.5698\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4339\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2869 [0/90000 (0%)]\tlog Loss: -17.561660\n",
      "\n",
      "Train set: Average log loss: -17.5699\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4340\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2870 [0/90000 (0%)]\tlog Loss: -17.561781\n",
      "\n",
      "Train set: Average log loss: -17.5699\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4340\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2871 [0/90000 (0%)]\tlog Loss: -17.561905\n",
      "\n",
      "Train set: Average log loss: -17.5700\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4340\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2872 [0/90000 (0%)]\tlog Loss: -17.561808\n",
      "\n",
      "Train set: Average log loss: -17.5700\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4342\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2873 [0/90000 (0%)]\tlog Loss: -17.562110\n",
      "\n",
      "Train set: Average log loss: -17.5701\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4342\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2874 [0/90000 (0%)]\tlog Loss: -17.562074\n",
      "\n",
      "Train set: Average log loss: -17.5702\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4343\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2875 [0/90000 (0%)]\tlog Loss: -17.562197\n",
      "\n",
      "Train set: Average log loss: -17.5703\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4343\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2876 [0/90000 (0%)]\tlog Loss: -17.562219\n",
      "\n",
      "Train set: Average log loss: -17.5703\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4345\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2877 [0/90000 (0%)]\tlog Loss: -17.562444\n",
      "\n",
      "Train set: Average log loss: -17.5704\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2878 [0/90000 (0%)]\tlog Loss: -17.562550\n",
      "\n",
      "Train set: Average log loss: -17.5705\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2879 [0/90000 (0%)]\tlog Loss: -17.562602\n",
      "\n",
      "Train set: Average log loss: -17.5706\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2880 [0/90000 (0%)]\tlog Loss: -17.562481\n",
      "\n",
      "Train set: Average log loss: -17.5706\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4347\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2881 [0/90000 (0%)]\tlog Loss: -17.562661\n",
      "\n",
      "Train set: Average log loss: -17.5707\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4347\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2882 [0/90000 (0%)]\tlog Loss: -17.562753\n",
      "\n",
      "Train set: Average log loss: -17.5708\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4348\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2883 [0/90000 (0%)]\tlog Loss: -17.562736\n",
      "\n",
      "Train set: Average log loss: -17.5709\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4348\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2884 [0/90000 (0%)]\tlog Loss: -17.562879\n",
      "\n",
      "Train set: Average log loss: -17.5709\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4349\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2885 [0/90000 (0%)]\tlog Loss: -17.562995\n",
      "\n",
      "Train set: Average log loss: -17.5710\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4350\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2886 [0/90000 (0%)]\tlog Loss: -17.563180\n",
      "\n",
      "Train set: Average log loss: -17.5711\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4351\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2887 [0/90000 (0%)]\tlog Loss: -17.563211\n",
      "\n",
      "Train set: Average log loss: -17.5712\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4353\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2888 [0/90000 (0%)]\tlog Loss: -17.563297\n",
      "\n",
      "Train set: Average log loss: -17.5712\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4353\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2889 [0/90000 (0%)]\tlog Loss: -17.563349\n",
      "\n",
      "Train set: Average log loss: -17.5713\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4354\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2890 [0/90000 (0%)]\tlog Loss: -17.563483\n",
      "\n",
      "Train set: Average log loss: -17.5714\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4354\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2891 [0/90000 (0%)]\tlog Loss: -17.563475\n",
      "\n",
      "Train set: Average log loss: -17.5715\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4355\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2892 [0/90000 (0%)]\tlog Loss: -17.563523\n",
      "\n",
      "Train set: Average log loss: -17.5715\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4356\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2893 [0/90000 (0%)]\tlog Loss: -17.563674\n",
      "\n",
      "Train set: Average log loss: -17.5716\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4357\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2894 [0/90000 (0%)]\tlog Loss: -17.563772\n",
      "\n",
      "Train set: Average log loss: -17.5717\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4358\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2895 [0/90000 (0%)]\tlog Loss: -17.563729\n",
      "\n",
      "Train set: Average log loss: -17.5718\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4358\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2896 [0/90000 (0%)]\tlog Loss: -17.563816\n",
      "\n",
      "Train set: Average log loss: -17.5719\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4359\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2897 [0/90000 (0%)]\tlog Loss: -17.563957\n",
      "\n",
      "Train set: Average log loss: -17.5719\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4360\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2898 [0/90000 (0%)]\tlog Loss: -17.564013\n",
      "\n",
      "Train set: Average log loss: -17.5720\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4360\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2899 [0/90000 (0%)]\tlog Loss: -17.564030\n",
      "\n",
      "Train set: Average log loss: -17.5721\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4361\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2900 [0/90000 (0%)]\tlog Loss: -17.564097\n",
      "\n",
      "Train set: Average log loss: -17.5721\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4361\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2901 [0/90000 (0%)]\tlog Loss: -17.564034\n",
      "\n",
      "Train set: Average log loss: -17.5722\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4362\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2902 [0/90000 (0%)]\tlog Loss: -17.564237\n",
      "\n",
      "Train set: Average log loss: -17.5723\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4362\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2903 [0/90000 (0%)]\tlog Loss: -17.564200\n",
      "\n",
      "Train set: Average log loss: -17.5724\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4363\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2904 [0/90000 (0%)]\tlog Loss: -17.564300\n",
      "\n",
      "Train set: Average log loss: -17.5724\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4363\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2905 [0/90000 (0%)]\tlog Loss: -17.564372\n",
      "\n",
      "Train set: Average log loss: -17.5725\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2906 [0/90000 (0%)]\tlog Loss: -17.564410\n",
      "\n",
      "Train set: Average log loss: -17.5726\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4365\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2907 [0/90000 (0%)]\tlog Loss: -17.564425\n",
      "\n",
      "Train set: Average log loss: -17.5726\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4366\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2908 [0/90000 (0%)]\tlog Loss: -17.564545\n",
      "\n",
      "Train set: Average log loss: -17.5727\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4366\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2909 [0/90000 (0%)]\tlog Loss: -17.564610\n",
      "\n",
      "Train set: Average log loss: -17.5728\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4367\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2910 [0/90000 (0%)]\tlog Loss: -17.564723\n",
      "\n",
      "Train set: Average log loss: -17.5729\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4368\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2911 [0/90000 (0%)]\tlog Loss: -17.564765\n",
      "\n",
      "Train set: Average log loss: -17.5730\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4368\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2912 [0/90000 (0%)]\tlog Loss: -17.564798\n",
      "\n",
      "Train set: Average log loss: -17.5730\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4370\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2913 [0/90000 (0%)]\tlog Loss: -17.564888\n",
      "\n",
      "Train set: Average log loss: -17.5731\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4371\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2914 [0/90000 (0%)]\tlog Loss: -17.565030\n",
      "\n",
      "Train set: Average log loss: -17.5732\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4371\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2915 [0/90000 (0%)]\tlog Loss: -17.565158\n",
      "\n",
      "Train set: Average log loss: -17.5733\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4372\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2916 [0/90000 (0%)]\tlog Loss: -17.565177\n",
      "\n",
      "Train set: Average log loss: -17.5733\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4373\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2917 [0/90000 (0%)]\tlog Loss: -17.565264\n",
      "\n",
      "Train set: Average log loss: -17.5734\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4373\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2918 [0/90000 (0%)]\tlog Loss: -17.565307\n",
      "\n",
      "Train set: Average log loss: -17.5735\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4374\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2919 [0/90000 (0%)]\tlog Loss: -17.565432\n",
      "\n",
      "Train set: Average log loss: -17.5736\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4375\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2920 [0/90000 (0%)]\tlog Loss: -17.565497\n",
      "\n",
      "Train set: Average log loss: -17.5736\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4375\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2921 [0/90000 (0%)]\tlog Loss: -17.565625\n",
      "\n",
      "Train set: Average log loss: -17.5737\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4377\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2922 [0/90000 (0%)]\tlog Loss: -17.565626\n",
      "\n",
      "Train set: Average log loss: -17.5738\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4378\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2923 [0/90000 (0%)]\tlog Loss: -17.565766\n",
      "\n",
      "Train set: Average log loss: -17.5739\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4378\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2924 [0/90000 (0%)]\tlog Loss: -17.565915\n",
      "\n",
      "Train set: Average log loss: -17.5739\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4379\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2925 [0/90000 (0%)]\tlog Loss: -17.565964\n",
      "\n",
      "Train set: Average log loss: -17.5740\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4379\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2926 [0/90000 (0%)]\tlog Loss: -17.565930\n",
      "\n",
      "Train set: Average log loss: -17.5741\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4380\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2927 [0/90000 (0%)]\tlog Loss: -17.566085\n",
      "\n",
      "Train set: Average log loss: -17.5741\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4381\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2928 [0/90000 (0%)]\tlog Loss: -17.566187\n",
      "\n",
      "Train set: Average log loss: -17.5742\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4381\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2929 [0/90000 (0%)]\tlog Loss: -17.566270\n",
      "\n",
      "Train set: Average log loss: -17.5743\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4381\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2930 [0/90000 (0%)]\tlog Loss: -17.566257\n",
      "\n",
      "Train set: Average log loss: -17.5744\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4383\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2931 [0/90000 (0%)]\tlog Loss: -17.566324\n",
      "\n",
      "Train set: Average log loss: -17.5745\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4383\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2932 [0/90000 (0%)]\tlog Loss: -17.566395\n",
      "\n",
      "Train set: Average log loss: -17.5746\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4384\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2933 [0/90000 (0%)]\tlog Loss: -17.566468\n",
      "\n",
      "Train set: Average log loss: -17.5746\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4385\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2934 [0/90000 (0%)]\tlog Loss: -17.566607\n",
      "\n",
      "Train set: Average log loss: -17.5747\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4386\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2935 [0/90000 (0%)]\tlog Loss: -17.566680\n",
      "\n",
      "Train set: Average log loss: -17.5748\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4387\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2936 [0/90000 (0%)]\tlog Loss: -17.566720\n",
      "\n",
      "Train set: Average log loss: -17.5749\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4388\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2937 [0/90000 (0%)]\tlog Loss: -17.566936\n",
      "\n",
      "Train set: Average log loss: -17.5750\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4389\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2938 [0/90000 (0%)]\tlog Loss: -17.567085\n",
      "\n",
      "Train set: Average log loss: -17.5751\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4391\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2939 [0/90000 (0%)]\tlog Loss: -17.567209\n",
      "\n",
      "Train set: Average log loss: -17.5752\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4392\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2940 [0/90000 (0%)]\tlog Loss: -17.567307\n",
      "\n",
      "Train set: Average log loss: -17.5753\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4393\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2941 [0/90000 (0%)]\tlog Loss: -17.567539\n",
      "\n",
      "Train set: Average log loss: -17.5754\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4395\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2942 [0/90000 (0%)]\tlog Loss: -17.567646\n",
      "\n",
      "Train set: Average log loss: -17.5755\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4396\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2943 [0/90000 (0%)]\tlog Loss: -17.567858\n",
      "\n",
      "Train set: Average log loss: -17.5755\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4398\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2944 [0/90000 (0%)]\tlog Loss: -17.568046\n",
      "\n",
      "Train set: Average log loss: -17.5757\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4400\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2945 [0/90000 (0%)]\tlog Loss: -17.568357\n",
      "\n",
      "Train set: Average log loss: -17.5757\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4401\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2946 [0/90000 (0%)]\tlog Loss: -17.568423\n",
      "\n",
      "Train set: Average log loss: -17.5758\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4403\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2947 [0/90000 (0%)]\tlog Loss: -17.568676\n",
      "\n",
      "Train set: Average log loss: -17.5759\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4405\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2948 [0/90000 (0%)]\tlog Loss: -17.568854\n",
      "\n",
      "Train set: Average log loss: -17.5761\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4407\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2949 [0/90000 (0%)]\tlog Loss: -17.569096\n",
      "\n",
      "Train set: Average log loss: -17.5761\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4409\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2950 [0/90000 (0%)]\tlog Loss: -17.569201\n",
      "\n",
      "Train set: Average log loss: -17.5762\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4411\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2951 [0/90000 (0%)]\tlog Loss: -17.569552\n",
      "\n",
      "Train set: Average log loss: -17.5763\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4412\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2952 [0/90000 (0%)]\tlog Loss: -17.569678\n",
      "\n",
      "Train set: Average log loss: -17.5764\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4414\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2953 [0/90000 (0%)]\tlog Loss: -17.569900\n",
      "\n",
      "Train set: Average log loss: -17.5765\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4416\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2954 [0/90000 (0%)]\tlog Loss: -17.570059\n",
      "\n",
      "Train set: Average log loss: -17.5766\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4418\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2955 [0/90000 (0%)]\tlog Loss: -17.570304\n",
      "\n",
      "Train set: Average log loss: -17.5767\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4420\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2956 [0/90000 (0%)]\tlog Loss: -17.570469\n",
      "\n",
      "Train set: Average log loss: -17.5767\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4423\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2957 [0/90000 (0%)]\tlog Loss: -17.570774\n",
      "\n",
      "Train set: Average log loss: -17.5768\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4424\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2958 [0/90000 (0%)]\tlog Loss: -17.570961\n",
      "\n",
      "Train set: Average log loss: -17.5769\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4426\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2959 [0/90000 (0%)]\tlog Loss: -17.571314\n",
      "\n",
      "Train set: Average log loss: -17.5770\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4427\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2960 [0/90000 (0%)]\tlog Loss: -17.571413\n",
      "\n",
      "Train set: Average log loss: -17.5771\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4430\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2961 [0/90000 (0%)]\tlog Loss: -17.571690\n",
      "\n",
      "Train set: Average log loss: -17.5772\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4432\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2962 [0/90000 (0%)]\tlog Loss: -17.571884\n",
      "\n",
      "Train set: Average log loss: -17.5773\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4433\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2963 [0/90000 (0%)]\tlog Loss: -17.572043\n",
      "\n",
      "Train set: Average log loss: -17.5774\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4434\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2964 [0/90000 (0%)]\tlog Loss: -17.572225\n",
      "\n",
      "Train set: Average log loss: -17.5774\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4437\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2965 [0/90000 (0%)]\tlog Loss: -17.572483\n",
      "\n",
      "Train set: Average log loss: -17.5776\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4438\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2966 [0/90000 (0%)]\tlog Loss: -17.572556\n",
      "\n",
      "Train set: Average log loss: -17.5776\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4440\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2967 [0/90000 (0%)]\tlog Loss: -17.572878\n",
      "\n",
      "Train set: Average log loss: -17.5777\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4441\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2968 [0/90000 (0%)]\tlog Loss: -17.573099\n",
      "\n",
      "Train set: Average log loss: -17.5778\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4444\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2969 [0/90000 (0%)]\tlog Loss: -17.573230\n",
      "\n",
      "Train set: Average log loss: -17.5779\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4446\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2970 [0/90000 (0%)]\tlog Loss: -17.573589\n",
      "\n",
      "Train set: Average log loss: -17.5780\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4447\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2971 [0/90000 (0%)]\tlog Loss: -17.573650\n",
      "\n",
      "Train set: Average log loss: -17.5781\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4449\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2972 [0/90000 (0%)]\tlog Loss: -17.574013\n",
      "\n",
      "Train set: Average log loss: -17.5782\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4451\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2973 [0/90000 (0%)]\tlog Loss: -17.574067\n",
      "\n",
      "Train set: Average log loss: -17.5783\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4453\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2974 [0/90000 (0%)]\tlog Loss: -17.574355\n",
      "\n",
      "Train set: Average log loss: -17.5784\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4455\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2975 [0/90000 (0%)]\tlog Loss: -17.574634\n",
      "\n",
      "Train set: Average log loss: -17.5784\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4457\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2976 [0/90000 (0%)]\tlog Loss: -17.574804\n",
      "\n",
      "Train set: Average log loss: -17.5786\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4459\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2977 [0/90000 (0%)]\tlog Loss: -17.575008\n",
      "\n",
      "Train set: Average log loss: -17.5786\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4461\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2978 [0/90000 (0%)]\tlog Loss: -17.575179\n",
      "\n",
      "Train set: Average log loss: -17.5788\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4463\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2979 [0/90000 (0%)]\tlog Loss: -17.575628\n",
      "\n",
      "Train set: Average log loss: -17.5788\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4466\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2980 [0/90000 (0%)]\tlog Loss: -17.575844\n",
      "\n",
      "Train set: Average log loss: -17.5789\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4467\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2981 [0/90000 (0%)]\tlog Loss: -17.576014\n",
      "\n",
      "Train set: Average log loss: -17.5790\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4470\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2982 [0/90000 (0%)]\tlog Loss: -17.576350\n",
      "\n",
      "Train set: Average log loss: -17.5791\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4471\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2983 [0/90000 (0%)]\tlog Loss: -17.576468\n",
      "\n",
      "Train set: Average log loss: -17.5792\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4474\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2984 [0/90000 (0%)]\tlog Loss: -17.576758\n",
      "\n",
      "Train set: Average log loss: -17.5793\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4476\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2985 [0/90000 (0%)]\tlog Loss: -17.576957\n",
      "\n",
      "Train set: Average log loss: -17.5795\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4479\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2986 [0/90000 (0%)]\tlog Loss: -17.577303\n",
      "\n",
      "Train set: Average log loss: -17.5796\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4480\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2987 [0/90000 (0%)]\tlog Loss: -17.577593\n",
      "\n",
      "Train set: Average log loss: -17.5797\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4483\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2988 [0/90000 (0%)]\tlog Loss: -17.577795\n",
      "\n",
      "Train set: Average log loss: -17.5798\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4486\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2989 [0/90000 (0%)]\tlog Loss: -17.578044\n",
      "\n",
      "Train set: Average log loss: -17.5799\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4489\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2990 [0/90000 (0%)]\tlog Loss: -17.578457\n",
      "\n",
      "Train set: Average log loss: -17.5800\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4491\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2991 [0/90000 (0%)]\tlog Loss: -17.578712\n",
      "\n",
      "Train set: Average log loss: -17.5801\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4495\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2992 [0/90000 (0%)]\tlog Loss: -17.579059\n",
      "\n",
      "Train set: Average log loss: -17.5803\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4497\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2993 [0/90000 (0%)]\tlog Loss: -17.579458\n",
      "\n",
      "Train set: Average log loss: -17.5804\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4501\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2994 [0/90000 (0%)]\tlog Loss: -17.579838\n",
      "\n",
      "Train set: Average log loss: -17.5805\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4504\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2995 [0/90000 (0%)]\tlog Loss: -17.580101\n",
      "\n",
      "Train set: Average log loss: -17.5807\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4507\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2996 [0/90000 (0%)]\tlog Loss: -17.580571\n",
      "\n",
      "Train set: Average log loss: -17.5808\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4511\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2997 [0/90000 (0%)]\tlog Loss: -17.581016\n",
      "\n",
      "Train set: Average log loss: -17.5810\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4515\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2998 [0/90000 (0%)]\tlog Loss: -17.581449\n",
      "\n",
      "Train set: Average log loss: -17.5812\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4519\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2999 [0/90000 (0%)]\tlog Loss: -17.581888\n",
      "\n",
      "Train set: Average log loss: -17.5814\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4523\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 3000 [0/90000 (0%)]\tlog Loss: -17.582344\n",
      "\n",
      "Train set: Average log loss: -17.5816\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.4527\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "args =  {\"batch_size\": 1024,\n",
    "         \"test_batch_size\": 4048,\n",
    "         \"epochs\" : 3*10**3,\n",
    "         \"lr\": 1e-4,\n",
    "         \"gamma\": .1,\n",
    "         \"no_cuda\" : False,\n",
    "         \"run_dry\": False,\n",
    "         \"seed\": 0,\n",
    "         \"log_interval\" : 100,\n",
    "         \"dry_run\" : False,\n",
    "         \"save_model\": True}\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "\n",
    "# Loading Train / Test Data\n",
    "bs_dataset = bs_LHS_data_generator(n = 10**5)\n",
    "train_size, test_size = int(bs_dataset.shape[0]*0.9), int(bs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(bs_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])\n",
    "\n",
    "\n",
    "\n",
    "print(device)\n",
    "model = BS_ANN().to(device)\n",
    "# Adam is found to be the best optimizer in the article\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"]) \n",
    "# Deacreses the Learning rate by .1 every 1000 epoch\n",
    "scheduler = StepLR(optimizer, step_size=10**3, gamma=args[\"gamma\"]) \n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "# Training loop\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "  train_loss = model.train_model(args, device, train_loader, optimizer, epoch)\n",
    "  test_loss = model.test_model(device, test_loader)\n",
    "  print(\"Current Learning rate {}\".format(scheduler.get_last_lr()[0]))\n",
    "  scheduler.step()\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "if args[\"save_model\"] :\n",
    "  torch.save(model.state_dict(), \"BS_ANN.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "Pw0Dw-vnm72M",
    "outputId": "7f700b9b-efd2-4606-f505-e0e32a86686e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WIDE 2.314319972608953e-08 VS in the article 8.04e-09\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the Log(MSE) with respect to the epoch\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot( np.log(test_losses), label = \"test\")\n",
    "plt.plot( np.log(train_losses), label = \"train\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log(LSE)\")\n",
    "plt.title(\"MSE over Epoch\" )\n",
    "plt.grid()\n",
    "print(\"Training WIDE {} VS in the article 8.04e-09\".format(train_losses[-1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2ImhvUYv-4O",
    "outputId": "35b137c2-26d9-4782-b5fe-7aa611394283"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BS_ANN(\n",
       "  (fc1): Linear(in_features=4, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc3): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc4): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc5): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model from saved parameters\n",
    "loaded_BS_ANN = BS_ANN().to(device)\n",
    "PATH = \"/content/BS_ANN.pt\"\n",
    "loaded_BS_ANN.load_state_dict(torch.load(PATH))\n",
    "loaded_BS_ANN.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "0oYKldqCw3dy",
    "outputId": "e15a1cdb-3f59-400d-e6c3-6776d4f55e3e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX  0.0007829666\n",
      "MIN  -0.0007711351\n"
     ]
    }
   ],
   "source": [
    "# Plotting erros histogram\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_BS_ANN(data)\n",
    "    errors.append((target-output).data.cpu().numpy())\n",
    "  errors = np.concatenate(errors)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(errors,50,fill=False)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Difference\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "print(\"MAX \",errors.max())\n",
    "print(\"MIN \",errors.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f44A-qviNxBr",
    "outputId": "66050728-c44a-4306-fc89-abb627493e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Train scores----------------\n",
      "MSE 2.624483e-08\n",
      "MAE 1.200234e-04\n",
      "MAPE 1.243891e-01\n",
      "R2 9.999995e-01\n",
      "-----------Test scores----------------\n",
      "MSE 2.597518e-08\n",
      "MAE 1.204123e-04\n",
      "MAPE 1.166314e-01\n",
      "R2 9.999995e-01\n"
     ]
    }
   ],
   "source": [
    "# Printing train and test MSE, MAE, MAPE and R2\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_BS_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Train scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )\n",
    "\n",
    "\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_BS_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Test scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJsHimJgXAic",
    "outputId": "4800b612-2436-411d-a430-2cc9952b25ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016116817303673826"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(2.597518e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "BL1Caoa46NXA",
    "outputId": "8b977eda-117a-453c-d2a5-c10d224e8623"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace this with a matrix\n",
    "moyeness = np.linspace(0.4,1.4)\n",
    "tau, r, sigma = 1, .02, .2\n",
    "prices = bs_moyeness_call(moyeness, tau, r, sigma)\n",
    "\n",
    "plot_dateset = np.ones((50,4))\n",
    "plot_dateset[:,0] = moyeness\n",
    "plot_dateset[:,1] = plot_dateset[:,1]*tau\n",
    "plot_dateset[:,2] = plot_dateset[:,2]*r\n",
    "plot_dateset[:,3] = plot_dateset[:,3]*sigma\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(moyeness, prices,label=\"Ground truth\",linewidth=5 )\n",
    "\n",
    "\n",
    "plot_dateset = torch.from_numpy(plot_dateset).type(torch.FloatTensor).to(device)\n",
    "output = loaded_BS_ANN(plot_dateset)\n",
    "plt.plot(moyeness, output.data.cpu().numpy(),\"--\",label=\"BS-ANN\",linewidth=5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"S/K\")\n",
    "plt.ylabel(\"P/K\")\n",
    "plt.title(\"Truth vs ANN\" )\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glh6JAzfpWHm"
   },
   "source": [
    "### Training BS-ANN with DecayLR from $10^{-4}$ to $10^{-6}$ Narrow dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tU85O5ppY2G",
    "outputId": "2c5e01f3-7782-4d3d-b13e-1b9ad1e2c1da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mLe flux de sortie a t tronqu et ne contient que les 5000dernires lignes.\u001b[0m\n",
      "Train Epoch: 2376 [0/90000 (0%)]\tlog Loss: -18.105247\n",
      "\n",
      "Train set: Average log loss: -18.0353\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9279\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2377 [0/90000 (0%)]\tlog Loss: -18.105351\n",
      "\n",
      "Train set: Average log loss: -18.0354\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9281\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2378 [0/90000 (0%)]\tlog Loss: -18.105520\n",
      "\n",
      "Train set: Average log loss: -18.0355\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9281\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2379 [0/90000 (0%)]\tlog Loss: -18.105535\n",
      "\n",
      "Train set: Average log loss: -18.0356\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9282\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2380 [0/90000 (0%)]\tlog Loss: -18.105639\n",
      "\n",
      "Train set: Average log loss: -18.0356\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9282\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2381 [0/90000 (0%)]\tlog Loss: -18.105760\n",
      "\n",
      "Train set: Average log loss: -18.0357\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9283\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2382 [0/90000 (0%)]\tlog Loss: -18.105791\n",
      "\n",
      "Train set: Average log loss: -18.0358\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9284\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2383 [0/90000 (0%)]\tlog Loss: -18.105912\n",
      "\n",
      "Train set: Average log loss: -18.0359\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9286\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2384 [0/90000 (0%)]\tlog Loss: -18.106115\n",
      "\n",
      "Train set: Average log loss: -18.0360\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9286\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2385 [0/90000 (0%)]\tlog Loss: -18.106177\n",
      "\n",
      "Train set: Average log loss: -18.0361\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9287\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2386 [0/90000 (0%)]\tlog Loss: -18.106115\n",
      "\n",
      "Train set: Average log loss: -18.0362\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9288\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2387 [0/90000 (0%)]\tlog Loss: -18.106203\n",
      "\n",
      "Train set: Average log loss: -18.0362\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9288\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2388 [0/90000 (0%)]\tlog Loss: -18.106180\n",
      "\n",
      "Train set: Average log loss: -18.0363\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9289\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2389 [0/90000 (0%)]\tlog Loss: -18.106345\n",
      "\n",
      "Train set: Average log loss: -18.0364\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9290\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2390 [0/90000 (0%)]\tlog Loss: -18.106482\n",
      "\n",
      "Train set: Average log loss: -18.0365\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9292\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2391 [0/90000 (0%)]\tlog Loss: -18.106567\n",
      "\n",
      "Train set: Average log loss: -18.0366\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9292\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2392 [0/90000 (0%)]\tlog Loss: -18.106541\n",
      "\n",
      "Train set: Average log loss: -18.0367\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9294\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2393 [0/90000 (0%)]\tlog Loss: -18.106803\n",
      "\n",
      "Train set: Average log loss: -18.0368\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9295\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2394 [0/90000 (0%)]\tlog Loss: -18.106822\n",
      "\n",
      "Train set: Average log loss: -18.0369\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9295\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2395 [0/90000 (0%)]\tlog Loss: -18.106995\n",
      "\n",
      "Train set: Average log loss: -18.0370\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9296\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2396 [0/90000 (0%)]\tlog Loss: -18.107041\n",
      "\n",
      "Train set: Average log loss: -18.0370\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9296\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2397 [0/90000 (0%)]\tlog Loss: -18.107091\n",
      "\n",
      "Train set: Average log loss: -18.0371\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9298\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2398 [0/90000 (0%)]\tlog Loss: -18.107140\n",
      "\n",
      "Train set: Average log loss: -18.0372\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9299\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2399 [0/90000 (0%)]\tlog Loss: -18.107332\n",
      "\n",
      "Train set: Average log loss: -18.0373\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9299\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2400 [0/90000 (0%)]\tlog Loss: -18.107475\n",
      "\n",
      "Train set: Average log loss: -18.0374\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9300\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2401 [0/90000 (0%)]\tlog Loss: -18.107544\n",
      "\n",
      "Train set: Average log loss: -18.0375\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9301\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2402 [0/90000 (0%)]\tlog Loss: -18.107501\n",
      "\n",
      "Train set: Average log loss: -18.0375\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9302\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2403 [0/90000 (0%)]\tlog Loss: -18.107615\n",
      "\n",
      "Train set: Average log loss: -18.0376\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9302\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2404 [0/90000 (0%)]\tlog Loss: -18.107719\n",
      "\n",
      "Train set: Average log loss: -18.0377\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9303\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2405 [0/90000 (0%)]\tlog Loss: -18.107771\n",
      "\n",
      "Train set: Average log loss: -18.0378\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9304\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2406 [0/90000 (0%)]\tlog Loss: -18.107948\n",
      "\n",
      "Train set: Average log loss: -18.0379\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9305\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2407 [0/90000 (0%)]\tlog Loss: -18.108033\n",
      "\n",
      "Train set: Average log loss: -18.0380\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9306\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2408 [0/90000 (0%)]\tlog Loss: -18.108167\n",
      "\n",
      "Train set: Average log loss: -18.0381\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9307\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2409 [0/90000 (0%)]\tlog Loss: -18.108352\n",
      "\n",
      "Train set: Average log loss: -18.0382\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9307\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2410 [0/90000 (0%)]\tlog Loss: -18.108321\n",
      "\n",
      "Train set: Average log loss: -18.0382\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9308\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2411 [0/90000 (0%)]\tlog Loss: -18.108409\n",
      "\n",
      "Train set: Average log loss: -18.0383\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9309\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2412 [0/90000 (0%)]\tlog Loss: -18.108577\n",
      "\n",
      "Train set: Average log loss: -18.0384\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9310\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2413 [0/90000 (0%)]\tlog Loss: -18.108683\n",
      "\n",
      "Train set: Average log loss: -18.0385\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9311\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2414 [0/90000 (0%)]\tlog Loss: -18.108816\n",
      "\n",
      "Train set: Average log loss: -18.0386\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9312\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2415 [0/90000 (0%)]\tlog Loss: -18.108852\n",
      "\n",
      "Train set: Average log loss: -18.0386\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9312\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2416 [0/90000 (0%)]\tlog Loss: -18.108916\n",
      "\n",
      "Train set: Average log loss: -18.0387\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9312\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2417 [0/90000 (0%)]\tlog Loss: -18.109140\n",
      "\n",
      "Train set: Average log loss: -18.0388\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9314\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2418 [0/90000 (0%)]\tlog Loss: -18.109217\n",
      "\n",
      "Train set: Average log loss: -18.0389\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9314\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2419 [0/90000 (0%)]\tlog Loss: -18.109411\n",
      "\n",
      "Train set: Average log loss: -18.0390\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9315\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2420 [0/90000 (0%)]\tlog Loss: -18.109413\n",
      "\n",
      "Train set: Average log loss: -18.0391\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9316\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2421 [0/90000 (0%)]\tlog Loss: -18.109541\n",
      "\n",
      "Train set: Average log loss: -18.0391\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9316\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2422 [0/90000 (0%)]\tlog Loss: -18.109585\n",
      "\n",
      "Train set: Average log loss: -18.0392\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9317\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2423 [0/90000 (0%)]\tlog Loss: -18.109721\n",
      "\n",
      "Train set: Average log loss: -18.0393\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9318\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2424 [0/90000 (0%)]\tlog Loss: -18.109872\n",
      "\n",
      "Train set: Average log loss: -18.0394\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9319\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2425 [0/90000 (0%)]\tlog Loss: -18.109925\n",
      "\n",
      "Train set: Average log loss: -18.0395\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9320\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2426 [0/90000 (0%)]\tlog Loss: -18.110051\n",
      "\n",
      "Train set: Average log loss: -18.0396\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9321\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2427 [0/90000 (0%)]\tlog Loss: -18.110162\n",
      "\n",
      "Train set: Average log loss: -18.0396\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9322\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2428 [0/90000 (0%)]\tlog Loss: -18.110253\n",
      "\n",
      "Train set: Average log loss: -18.0397\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9323\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2429 [0/90000 (0%)]\tlog Loss: -18.110307\n",
      "\n",
      "Train set: Average log loss: -18.0398\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9324\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2430 [0/90000 (0%)]\tlog Loss: -18.110431\n",
      "\n",
      "Train set: Average log loss: -18.0399\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9324\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2431 [0/90000 (0%)]\tlog Loss: -18.110399\n",
      "\n",
      "Train set: Average log loss: -18.0399\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9325\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2432 [0/90000 (0%)]\tlog Loss: -18.110624\n",
      "\n",
      "Train set: Average log loss: -18.0401\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9326\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2433 [0/90000 (0%)]\tlog Loss: -18.110766\n",
      "\n",
      "Train set: Average log loss: -18.0402\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9326\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2434 [0/90000 (0%)]\tlog Loss: -18.110640\n",
      "\n",
      "Train set: Average log loss: -18.0402\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9327\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2435 [0/90000 (0%)]\tlog Loss: -18.110810\n",
      "\n",
      "Train set: Average log loss: -18.0403\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9328\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2436 [0/90000 (0%)]\tlog Loss: -18.110898\n",
      "\n",
      "Train set: Average log loss: -18.0404\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9329\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2437 [0/90000 (0%)]\tlog Loss: -18.110945\n",
      "\n",
      "Train set: Average log loss: -18.0405\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9330\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2438 [0/90000 (0%)]\tlog Loss: -18.111079\n",
      "\n",
      "Train set: Average log loss: -18.0406\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9331\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2439 [0/90000 (0%)]\tlog Loss: -18.111104\n",
      "\n",
      "Train set: Average log loss: -18.0406\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9332\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2440 [0/90000 (0%)]\tlog Loss: -18.111152\n",
      "\n",
      "Train set: Average log loss: -18.0407\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9332\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2441 [0/90000 (0%)]\tlog Loss: -18.111436\n",
      "\n",
      "Train set: Average log loss: -18.0408\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9333\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2442 [0/90000 (0%)]\tlog Loss: -18.111438\n",
      "\n",
      "Train set: Average log loss: -18.0409\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9334\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2443 [0/90000 (0%)]\tlog Loss: -18.111581\n",
      "\n",
      "Train set: Average log loss: -18.0410\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9334\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2444 [0/90000 (0%)]\tlog Loss: -18.111653\n",
      "\n",
      "Train set: Average log loss: -18.0411\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9335\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2445 [0/90000 (0%)]\tlog Loss: -18.111779\n",
      "\n",
      "Train set: Average log loss: -18.0412\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9335\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2446 [0/90000 (0%)]\tlog Loss: -18.111658\n",
      "\n",
      "Train set: Average log loss: -18.0412\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9336\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2447 [0/90000 (0%)]\tlog Loss: -18.111745\n",
      "\n",
      "Train set: Average log loss: -18.0413\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9337\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2448 [0/90000 (0%)]\tlog Loss: -18.111912\n",
      "\n",
      "Train set: Average log loss: -18.0414\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9338\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2449 [0/90000 (0%)]\tlog Loss: -18.111959\n",
      "\n",
      "Train set: Average log loss: -18.0415\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9338\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2450 [0/90000 (0%)]\tlog Loss: -18.112012\n",
      "\n",
      "Train set: Average log loss: -18.0415\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9339\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2451 [0/90000 (0%)]\tlog Loss: -18.112160\n",
      "\n",
      "Train set: Average log loss: -18.0416\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9339\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2452 [0/90000 (0%)]\tlog Loss: -18.112179\n",
      "\n",
      "Train set: Average log loss: -18.0417\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9340\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2453 [0/90000 (0%)]\tlog Loss: -18.112285\n",
      "\n",
      "Train set: Average log loss: -18.0418\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9341\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2454 [0/90000 (0%)]\tlog Loss: -18.112402\n",
      "\n",
      "Train set: Average log loss: -18.0419\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9342\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2455 [0/90000 (0%)]\tlog Loss: -18.112635\n",
      "\n",
      "Train set: Average log loss: -18.0420\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9343\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2456 [0/90000 (0%)]\tlog Loss: -18.112562\n",
      "\n",
      "Train set: Average log loss: -18.0421\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9344\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2457 [0/90000 (0%)]\tlog Loss: -18.112556\n",
      "\n",
      "Train set: Average log loss: -18.0421\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9344\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2458 [0/90000 (0%)]\tlog Loss: -18.112648\n",
      "\n",
      "Train set: Average log loss: -18.0422\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9345\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2459 [0/90000 (0%)]\tlog Loss: -18.112730\n",
      "\n",
      "Train set: Average log loss: -18.0423\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2460 [0/90000 (0%)]\tlog Loss: -18.112762\n",
      "\n",
      "Train set: Average log loss: -18.0423\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9347\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2461 [0/90000 (0%)]\tlog Loss: -18.113096\n",
      "\n",
      "Train set: Average log loss: -18.0425\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9348\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2462 [0/90000 (0%)]\tlog Loss: -18.113065\n",
      "\n",
      "Train set: Average log loss: -18.0425\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9349\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2463 [0/90000 (0%)]\tlog Loss: -18.113049\n",
      "\n",
      "Train set: Average log loss: -18.0426\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9350\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2464 [0/90000 (0%)]\tlog Loss: -18.113270\n",
      "\n",
      "Train set: Average log loss: -18.0427\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9351\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2465 [0/90000 (0%)]\tlog Loss: -18.113340\n",
      "\n",
      "Train set: Average log loss: -18.0428\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9352\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2466 [0/90000 (0%)]\tlog Loss: -18.113352\n",
      "\n",
      "Train set: Average log loss: -18.0429\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9352\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2467 [0/90000 (0%)]\tlog Loss: -18.113440\n",
      "\n",
      "Train set: Average log loss: -18.0429\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9353\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2468 [0/90000 (0%)]\tlog Loss: -18.113466\n",
      "\n",
      "Train set: Average log loss: -18.0430\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9354\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2469 [0/90000 (0%)]\tlog Loss: -18.113644\n",
      "\n",
      "Train set: Average log loss: -18.0431\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9355\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2470 [0/90000 (0%)]\tlog Loss: -18.113709\n",
      "\n",
      "Train set: Average log loss: -18.0432\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9355\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2471 [0/90000 (0%)]\tlog Loss: -18.113821\n",
      "\n",
      "Train set: Average log loss: -18.0433\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9356\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2472 [0/90000 (0%)]\tlog Loss: -18.113847\n",
      "\n",
      "Train set: Average log loss: -18.0434\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9357\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2473 [0/90000 (0%)]\tlog Loss: -18.113999\n",
      "\n",
      "Train set: Average log loss: -18.0434\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9358\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2474 [0/90000 (0%)]\tlog Loss: -18.114167\n",
      "\n",
      "Train set: Average log loss: -18.0435\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9359\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2475 [0/90000 (0%)]\tlog Loss: -18.114256\n",
      "\n",
      "Train set: Average log loss: -18.0436\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9359\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2476 [0/90000 (0%)]\tlog Loss: -18.114350\n",
      "\n",
      "Train set: Average log loss: -18.0437\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9360\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2477 [0/90000 (0%)]\tlog Loss: -18.114347\n",
      "\n",
      "Train set: Average log loss: -18.0438\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9361\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2478 [0/90000 (0%)]\tlog Loss: -18.114418\n",
      "\n",
      "Train set: Average log loss: -18.0439\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9361\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2479 [0/90000 (0%)]\tlog Loss: -18.114357\n",
      "\n",
      "Train set: Average log loss: -18.0439\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9363\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2480 [0/90000 (0%)]\tlog Loss: -18.114765\n",
      "\n",
      "Train set: Average log loss: -18.0440\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9363\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2481 [0/90000 (0%)]\tlog Loss: -18.114607\n",
      "\n",
      "Train set: Average log loss: -18.0441\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2482 [0/90000 (0%)]\tlog Loss: -18.114878\n",
      "\n",
      "Train set: Average log loss: -18.0442\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2483 [0/90000 (0%)]\tlog Loss: -18.114715\n",
      "\n",
      "Train set: Average log loss: -18.0443\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9365\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2484 [0/90000 (0%)]\tlog Loss: -18.115022\n",
      "\n",
      "Train set: Average log loss: -18.0443\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9366\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2485 [0/90000 (0%)]\tlog Loss: -18.115075\n",
      "\n",
      "Train set: Average log loss: -18.0444\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9368\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2486 [0/90000 (0%)]\tlog Loss: -18.115158\n",
      "\n",
      "Train set: Average log loss: -18.0445\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9369\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2487 [0/90000 (0%)]\tlog Loss: -18.115359\n",
      "\n",
      "Train set: Average log loss: -18.0446\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9370\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2488 [0/90000 (0%)]\tlog Loss: -18.115544\n",
      "\n",
      "Train set: Average log loss: -18.0447\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9370\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2489 [0/90000 (0%)]\tlog Loss: -18.115340\n",
      "\n",
      "Train set: Average log loss: -18.0448\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9371\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2490 [0/90000 (0%)]\tlog Loss: -18.115647\n",
      "\n",
      "Train set: Average log loss: -18.0448\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9371\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2491 [0/90000 (0%)]\tlog Loss: -18.115591\n",
      "\n",
      "Train set: Average log loss: -18.0449\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9373\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2492 [0/90000 (0%)]\tlog Loss: -18.115815\n",
      "\n",
      "Train set: Average log loss: -18.0450\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9374\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2493 [0/90000 (0%)]\tlog Loss: -18.116065\n",
      "\n",
      "Train set: Average log loss: -18.0451\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9374\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2494 [0/90000 (0%)]\tlog Loss: -18.115888\n",
      "\n",
      "Train set: Average log loss: -18.0452\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9375\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2495 [0/90000 (0%)]\tlog Loss: -18.116010\n",
      "\n",
      "Train set: Average log loss: -18.0453\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9375\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2496 [0/90000 (0%)]\tlog Loss: -18.116243\n",
      "\n",
      "Train set: Average log loss: -18.0453\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9376\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2497 [0/90000 (0%)]\tlog Loss: -18.116304\n",
      "\n",
      "Train set: Average log loss: -18.0455\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9377\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2498 [0/90000 (0%)]\tlog Loss: -18.116484\n",
      "\n",
      "Train set: Average log loss: -18.0455\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9377\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2499 [0/90000 (0%)]\tlog Loss: -18.116378\n",
      "\n",
      "Train set: Average log loss: -18.0456\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9379\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2500 [0/90000 (0%)]\tlog Loss: -18.116546\n",
      "\n",
      "Train set: Average log loss: -18.0457\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9379\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2501 [0/90000 (0%)]\tlog Loss: -18.116681\n",
      "\n",
      "Train set: Average log loss: -18.0458\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9379\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2502 [0/90000 (0%)]\tlog Loss: -18.116639\n",
      "\n",
      "Train set: Average log loss: -18.0458\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9380\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2503 [0/90000 (0%)]\tlog Loss: -18.116704\n",
      "\n",
      "Train set: Average log loss: -18.0459\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9382\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2504 [0/90000 (0%)]\tlog Loss: -18.116904\n",
      "\n",
      "Train set: Average log loss: -18.0460\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9382\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2505 [0/90000 (0%)]\tlog Loss: -18.117052\n",
      "\n",
      "Train set: Average log loss: -18.0461\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9383\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2506 [0/90000 (0%)]\tlog Loss: -18.117082\n",
      "\n",
      "Train set: Average log loss: -18.0462\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9384\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2507 [0/90000 (0%)]\tlog Loss: -18.117212\n",
      "\n",
      "Train set: Average log loss: -18.0463\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9384\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2508 [0/90000 (0%)]\tlog Loss: -18.117300\n",
      "\n",
      "Train set: Average log loss: -18.0463\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9386\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2509 [0/90000 (0%)]\tlog Loss: -18.117478\n",
      "\n",
      "Train set: Average log loss: -18.0464\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9386\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2510 [0/90000 (0%)]\tlog Loss: -18.117508\n",
      "\n",
      "Train set: Average log loss: -18.0465\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9387\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2511 [0/90000 (0%)]\tlog Loss: -18.117636\n",
      "\n",
      "Train set: Average log loss: -18.0466\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9388\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2512 [0/90000 (0%)]\tlog Loss: -18.117738\n",
      "\n",
      "Train set: Average log loss: -18.0467\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9389\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2513 [0/90000 (0%)]\tlog Loss: -18.117763\n",
      "\n",
      "Train set: Average log loss: -18.0468\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9390\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2514 [0/90000 (0%)]\tlog Loss: -18.117876\n",
      "\n",
      "Train set: Average log loss: -18.0468\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9391\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2515 [0/90000 (0%)]\tlog Loss: -18.117941\n",
      "\n",
      "Train set: Average log loss: -18.0469\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9391\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2516 [0/90000 (0%)]\tlog Loss: -18.118038\n",
      "\n",
      "Train set: Average log loss: -18.0470\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9392\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2517 [0/90000 (0%)]\tlog Loss: -18.118043\n",
      "\n",
      "Train set: Average log loss: -18.0471\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9393\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2518 [0/90000 (0%)]\tlog Loss: -18.118251\n",
      "\n",
      "Train set: Average log loss: -18.0472\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9394\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2519 [0/90000 (0%)]\tlog Loss: -18.118354\n",
      "\n",
      "Train set: Average log loss: -18.0473\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9395\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2520 [0/90000 (0%)]\tlog Loss: -18.118521\n",
      "\n",
      "Train set: Average log loss: -18.0473\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9396\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2521 [0/90000 (0%)]\tlog Loss: -18.118562\n",
      "\n",
      "Train set: Average log loss: -18.0474\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9397\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2522 [0/90000 (0%)]\tlog Loss: -18.118650\n",
      "\n",
      "Train set: Average log loss: -18.0475\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9397\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2523 [0/90000 (0%)]\tlog Loss: -18.118834\n",
      "\n",
      "Train set: Average log loss: -18.0476\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9398\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2524 [0/90000 (0%)]\tlog Loss: -18.118766\n",
      "\n",
      "Train set: Average log loss: -18.0476\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9398\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2525 [0/90000 (0%)]\tlog Loss: -18.118826\n",
      "\n",
      "Train set: Average log loss: -18.0477\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9399\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2526 [0/90000 (0%)]\tlog Loss: -18.119161\n",
      "\n",
      "Train set: Average log loss: -18.0478\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9400\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2527 [0/90000 (0%)]\tlog Loss: -18.119122\n",
      "\n",
      "Train set: Average log loss: -18.0479\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9401\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2528 [0/90000 (0%)]\tlog Loss: -18.119235\n",
      "\n",
      "Train set: Average log loss: -18.0480\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9401\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2529 [0/90000 (0%)]\tlog Loss: -18.119314\n",
      "\n",
      "Train set: Average log loss: -18.0481\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9402\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2530 [0/90000 (0%)]\tlog Loss: -18.119452\n",
      "\n",
      "Train set: Average log loss: -18.0481\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9404\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2531 [0/90000 (0%)]\tlog Loss: -18.119601\n",
      "\n",
      "Train set: Average log loss: -18.0482\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9405\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2532 [0/90000 (0%)]\tlog Loss: -18.119637\n",
      "\n",
      "Train set: Average log loss: -18.0483\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9405\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2533 [0/90000 (0%)]\tlog Loss: -18.119795\n",
      "\n",
      "Train set: Average log loss: -18.0484\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9406\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2534 [0/90000 (0%)]\tlog Loss: -18.119804\n",
      "\n",
      "Train set: Average log loss: -18.0485\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9407\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2535 [0/90000 (0%)]\tlog Loss: -18.119810\n",
      "\n",
      "Train set: Average log loss: -18.0486\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9408\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2536 [0/90000 (0%)]\tlog Loss: -18.120138\n",
      "\n",
      "Train set: Average log loss: -18.0487\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9409\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2537 [0/90000 (0%)]\tlog Loss: -18.120055\n",
      "\n",
      "Train set: Average log loss: -18.0487\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9409\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2538 [0/90000 (0%)]\tlog Loss: -18.119917\n",
      "\n",
      "Train set: Average log loss: -18.0488\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9410\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2539 [0/90000 (0%)]\tlog Loss: -18.120234\n",
      "\n",
      "Train set: Average log loss: -18.0489\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9411\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2540 [0/90000 (0%)]\tlog Loss: -18.120320\n",
      "\n",
      "Train set: Average log loss: -18.0490\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9412\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2541 [0/90000 (0%)]\tlog Loss: -18.120335\n",
      "\n",
      "Train set: Average log loss: -18.0491\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9414\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2542 [0/90000 (0%)]\tlog Loss: -18.120685\n",
      "\n",
      "Train set: Average log loss: -18.0492\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9415\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2543 [0/90000 (0%)]\tlog Loss: -18.120724\n",
      "\n",
      "Train set: Average log loss: -18.0492\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9415\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2544 [0/90000 (0%)]\tlog Loss: -18.120653\n",
      "\n",
      "Train set: Average log loss: -18.0493\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9416\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2545 [0/90000 (0%)]\tlog Loss: -18.120880\n",
      "\n",
      "Train set: Average log loss: -18.0494\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9417\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2546 [0/90000 (0%)]\tlog Loss: -18.120948\n",
      "\n",
      "Train set: Average log loss: -18.0495\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9418\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2547 [0/90000 (0%)]\tlog Loss: -18.121019\n",
      "\n",
      "Train set: Average log loss: -18.0495\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9419\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2548 [0/90000 (0%)]\tlog Loss: -18.121146\n",
      "\n",
      "Train set: Average log loss: -18.0496\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9419\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2549 [0/90000 (0%)]\tlog Loss: -18.121308\n",
      "\n",
      "Train set: Average log loss: -18.0497\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9420\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2550 [0/90000 (0%)]\tlog Loss: -18.121177\n",
      "\n",
      "Train set: Average log loss: -18.0498\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9421\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2551 [0/90000 (0%)]\tlog Loss: -18.121443\n",
      "\n",
      "Train set: Average log loss: -18.0499\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9422\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2552 [0/90000 (0%)]\tlog Loss: -18.121514\n",
      "\n",
      "Train set: Average log loss: -18.0500\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9423\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2553 [0/90000 (0%)]\tlog Loss: -18.121577\n",
      "\n",
      "Train set: Average log loss: -18.0500\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9422\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2554 [0/90000 (0%)]\tlog Loss: -18.121705\n",
      "\n",
      "Train set: Average log loss: -18.0501\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9424\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2555 [0/90000 (0%)]\tlog Loss: -18.121780\n",
      "\n",
      "Train set: Average log loss: -18.0502\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9425\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2556 [0/90000 (0%)]\tlog Loss: -18.121834\n",
      "\n",
      "Train set: Average log loss: -18.0503\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9425\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2557 [0/90000 (0%)]\tlog Loss: -18.122057\n",
      "\n",
      "Train set: Average log loss: -18.0503\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9426\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2558 [0/90000 (0%)]\tlog Loss: -18.121995\n",
      "\n",
      "Train set: Average log loss: -18.0504\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9427\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2559 [0/90000 (0%)]\tlog Loss: -18.122013\n",
      "\n",
      "Train set: Average log loss: -18.0505\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9427\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2560 [0/90000 (0%)]\tlog Loss: -18.122200\n",
      "\n",
      "Train set: Average log loss: -18.0506\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9429\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2561 [0/90000 (0%)]\tlog Loss: -18.122155\n",
      "\n",
      "Train set: Average log loss: -18.0507\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9429\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2562 [0/90000 (0%)]\tlog Loss: -18.122264\n",
      "\n",
      "Train set: Average log loss: -18.0507\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9429\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2563 [0/90000 (0%)]\tlog Loss: -18.122325\n",
      "\n",
      "Train set: Average log loss: -18.0508\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9429\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2564 [0/90000 (0%)]\tlog Loss: -18.122308\n",
      "\n",
      "Train set: Average log loss: -18.0509\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9431\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2565 [0/90000 (0%)]\tlog Loss: -18.122372\n",
      "\n",
      "Train set: Average log loss: -18.0510\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9431\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2566 [0/90000 (0%)]\tlog Loss: -18.122433\n",
      "\n",
      "Train set: Average log loss: -18.0511\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9432\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2567 [0/90000 (0%)]\tlog Loss: -18.122583\n",
      "\n",
      "Train set: Average log loss: -18.0512\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9433\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2568 [0/90000 (0%)]\tlog Loss: -18.122497\n",
      "\n",
      "Train set: Average log loss: -18.0512\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9434\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2569 [0/90000 (0%)]\tlog Loss: -18.122722\n",
      "\n",
      "Train set: Average log loss: -18.0513\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9434\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2570 [0/90000 (0%)]\tlog Loss: -18.122810\n",
      "\n",
      "Train set: Average log loss: -18.0514\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9435\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2571 [0/90000 (0%)]\tlog Loss: -18.122911\n",
      "\n",
      "Train set: Average log loss: -18.0515\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9436\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2572 [0/90000 (0%)]\tlog Loss: -18.122950\n",
      "\n",
      "Train set: Average log loss: -18.0516\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9436\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2573 [0/90000 (0%)]\tlog Loss: -18.123043\n",
      "\n",
      "Train set: Average log loss: -18.0516\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9436\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2574 [0/90000 (0%)]\tlog Loss: -18.123009\n",
      "\n",
      "Train set: Average log loss: -18.0517\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9438\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2575 [0/90000 (0%)]\tlog Loss: -18.123304\n",
      "\n",
      "Train set: Average log loss: -18.0518\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9438\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2576 [0/90000 (0%)]\tlog Loss: -18.123184\n",
      "\n",
      "Train set: Average log loss: -18.0519\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9439\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2577 [0/90000 (0%)]\tlog Loss: -18.123411\n",
      "\n",
      "Train set: Average log loss: -18.0520\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9440\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2578 [0/90000 (0%)]\tlog Loss: -18.123450\n",
      "\n",
      "Train set: Average log loss: -18.0521\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9440\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2579 [0/90000 (0%)]\tlog Loss: -18.123437\n",
      "\n",
      "Train set: Average log loss: -18.0521\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9440\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2580 [0/90000 (0%)]\tlog Loss: -18.123463\n",
      "\n",
      "Train set: Average log loss: -18.0522\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9441\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2581 [0/90000 (0%)]\tlog Loss: -18.123682\n",
      "\n",
      "Train set: Average log loss: -18.0523\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9442\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2582 [0/90000 (0%)]\tlog Loss: -18.123812\n",
      "\n",
      "Train set: Average log loss: -18.0524\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9442\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2583 [0/90000 (0%)]\tlog Loss: -18.123987\n",
      "\n",
      "Train set: Average log loss: -18.0525\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9443\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2584 [0/90000 (0%)]\tlog Loss: -18.124087\n",
      "\n",
      "Train set: Average log loss: -18.0526\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9443\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2585 [0/90000 (0%)]\tlog Loss: -18.124081\n",
      "\n",
      "Train set: Average log loss: -18.0526\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9445\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2586 [0/90000 (0%)]\tlog Loss: -18.124332\n",
      "\n",
      "Train set: Average log loss: -18.0527\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9446\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2587 [0/90000 (0%)]\tlog Loss: -18.124359\n",
      "\n",
      "Train set: Average log loss: -18.0528\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9447\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2588 [0/90000 (0%)]\tlog Loss: -18.124454\n",
      "\n",
      "Train set: Average log loss: -18.0529\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9447\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2589 [0/90000 (0%)]\tlog Loss: -18.124484\n",
      "\n",
      "Train set: Average log loss: -18.0529\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9448\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2590 [0/90000 (0%)]\tlog Loss: -18.124567\n",
      "\n",
      "Train set: Average log loss: -18.0530\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9448\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2591 [0/90000 (0%)]\tlog Loss: -18.124613\n",
      "\n",
      "Train set: Average log loss: -18.0531\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9450\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2592 [0/90000 (0%)]\tlog Loss: -18.124931\n",
      "\n",
      "Train set: Average log loss: -18.0532\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9450\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2593 [0/90000 (0%)]\tlog Loss: -18.125029\n",
      "\n",
      "Train set: Average log loss: -18.0533\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9451\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2594 [0/90000 (0%)]\tlog Loss: -18.125014\n",
      "\n",
      "Train set: Average log loss: -18.0534\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9452\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2595 [0/90000 (0%)]\tlog Loss: -18.125279\n",
      "\n",
      "Train set: Average log loss: -18.0534\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9452\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2596 [0/90000 (0%)]\tlog Loss: -18.125246\n",
      "\n",
      "Train set: Average log loss: -18.0535\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9453\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2597 [0/90000 (0%)]\tlog Loss: -18.125401\n",
      "\n",
      "Train set: Average log loss: -18.0536\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9454\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2598 [0/90000 (0%)]\tlog Loss: -18.125318\n",
      "\n",
      "Train set: Average log loss: -18.0537\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9455\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2599 [0/90000 (0%)]\tlog Loss: -18.125399\n",
      "\n",
      "Train set: Average log loss: -18.0538\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9456\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2600 [0/90000 (0%)]\tlog Loss: -18.125467\n",
      "\n",
      "Train set: Average log loss: -18.0539\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9458\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2601 [0/90000 (0%)]\tlog Loss: -18.125620\n",
      "\n",
      "Train set: Average log loss: -18.0539\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9458\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2602 [0/90000 (0%)]\tlog Loss: -18.125776\n",
      "\n",
      "Train set: Average log loss: -18.0540\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9458\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2603 [0/90000 (0%)]\tlog Loss: -18.125709\n",
      "\n",
      "Train set: Average log loss: -18.0541\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9459\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2604 [0/90000 (0%)]\tlog Loss: -18.125946\n",
      "\n",
      "Train set: Average log loss: -18.0542\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9460\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2605 [0/90000 (0%)]\tlog Loss: -18.126022\n",
      "\n",
      "Train set: Average log loss: -18.0543\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9461\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2606 [0/90000 (0%)]\tlog Loss: -18.126110\n",
      "\n",
      "Train set: Average log loss: -18.0543\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9462\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2607 [0/90000 (0%)]\tlog Loss: -18.126176\n",
      "\n",
      "Train set: Average log loss: -18.0545\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9464\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2608 [0/90000 (0%)]\tlog Loss: -18.126417\n",
      "\n",
      "Train set: Average log loss: -18.0545\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9465\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2609 [0/90000 (0%)]\tlog Loss: -18.126547\n",
      "\n",
      "Train set: Average log loss: -18.0546\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9465\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2610 [0/90000 (0%)]\tlog Loss: -18.126465\n",
      "\n",
      "Train set: Average log loss: -18.0547\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9466\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2611 [0/90000 (0%)]\tlog Loss: -18.126632\n",
      "\n",
      "Train set: Average log loss: -18.0548\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9467\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2612 [0/90000 (0%)]\tlog Loss: -18.126770\n",
      "\n",
      "Train set: Average log loss: -18.0549\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9468\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2613 [0/90000 (0%)]\tlog Loss: -18.126851\n",
      "\n",
      "Train set: Average log loss: -18.0549\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9468\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2614 [0/90000 (0%)]\tlog Loss: -18.127032\n",
      "\n",
      "Train set: Average log loss: -18.0550\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9470\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2615 [0/90000 (0%)]\tlog Loss: -18.127143\n",
      "\n",
      "Train set: Average log loss: -18.0551\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9471\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2616 [0/90000 (0%)]\tlog Loss: -18.127172\n",
      "\n",
      "Train set: Average log loss: -18.0552\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9472\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2617 [0/90000 (0%)]\tlog Loss: -18.127229\n",
      "\n",
      "Train set: Average log loss: -18.0553\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9472\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2618 [0/90000 (0%)]\tlog Loss: -18.127313\n",
      "\n",
      "Train set: Average log loss: -18.0553\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9473\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2619 [0/90000 (0%)]\tlog Loss: -18.127340\n",
      "\n",
      "Train set: Average log loss: -18.0554\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9475\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2620 [0/90000 (0%)]\tlog Loss: -18.127625\n",
      "\n",
      "Train set: Average log loss: -18.0555\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9476\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2621 [0/90000 (0%)]\tlog Loss: -18.127566\n",
      "\n",
      "Train set: Average log loss: -18.0556\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9477\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2622 [0/90000 (0%)]\tlog Loss: -18.127911\n",
      "\n",
      "Train set: Average log loss: -18.0557\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9478\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2623 [0/90000 (0%)]\tlog Loss: -18.127975\n",
      "\n",
      "Train set: Average log loss: -18.0558\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9479\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2624 [0/90000 (0%)]\tlog Loss: -18.128103\n",
      "\n",
      "Train set: Average log loss: -18.0558\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9480\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2625 [0/90000 (0%)]\tlog Loss: -18.128131\n",
      "\n",
      "Train set: Average log loss: -18.0559\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9480\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2626 [0/90000 (0%)]\tlog Loss: -18.128273\n",
      "\n",
      "Train set: Average log loss: -18.0560\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9481\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2627 [0/90000 (0%)]\tlog Loss: -18.128343\n",
      "\n",
      "Train set: Average log loss: -18.0561\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9482\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2628 [0/90000 (0%)]\tlog Loss: -18.128378\n",
      "\n",
      "Train set: Average log loss: -18.0562\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9484\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2629 [0/90000 (0%)]\tlog Loss: -18.128491\n",
      "\n",
      "Train set: Average log loss: -18.0562\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9483\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2630 [0/90000 (0%)]\tlog Loss: -18.128477\n",
      "\n",
      "Train set: Average log loss: -18.0563\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9485\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2631 [0/90000 (0%)]\tlog Loss: -18.128829\n",
      "\n",
      "Train set: Average log loss: -18.0564\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9485\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2632 [0/90000 (0%)]\tlog Loss: -18.128661\n",
      "\n",
      "Train set: Average log loss: -18.0565\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9486\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2633 [0/90000 (0%)]\tlog Loss: -18.128984\n",
      "\n",
      "Train set: Average log loss: -18.0566\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9487\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2634 [0/90000 (0%)]\tlog Loss: -18.128856\n",
      "\n",
      "Train set: Average log loss: -18.0567\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9489\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2635 [0/90000 (0%)]\tlog Loss: -18.129133\n",
      "\n",
      "Train set: Average log loss: -18.0567\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9489\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2636 [0/90000 (0%)]\tlog Loss: -18.129052\n",
      "\n",
      "Train set: Average log loss: -18.0568\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9490\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2637 [0/90000 (0%)]\tlog Loss: -18.129193\n",
      "\n",
      "Train set: Average log loss: -18.0569\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9492\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2638 [0/90000 (0%)]\tlog Loss: -18.129454\n",
      "\n",
      "Train set: Average log loss: -18.0570\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9492\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2639 [0/90000 (0%)]\tlog Loss: -18.129487\n",
      "\n",
      "Train set: Average log loss: -18.0571\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9494\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2640 [0/90000 (0%)]\tlog Loss: -18.129549\n",
      "\n",
      "Train set: Average log loss: -18.0571\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9494\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2641 [0/90000 (0%)]\tlog Loss: -18.129730\n",
      "\n",
      "Train set: Average log loss: -18.0572\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9495\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2642 [0/90000 (0%)]\tlog Loss: -18.129693\n",
      "\n",
      "Train set: Average log loss: -18.0573\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9496\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2643 [0/90000 (0%)]\tlog Loss: -18.130032\n",
      "\n",
      "Train set: Average log loss: -18.0574\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9497\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2644 [0/90000 (0%)]\tlog Loss: -18.129988\n",
      "\n",
      "Train set: Average log loss: -18.0574\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9498\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2645 [0/90000 (0%)]\tlog Loss: -18.129976\n",
      "\n",
      "Train set: Average log loss: -18.0575\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9498\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2646 [0/90000 (0%)]\tlog Loss: -18.130009\n",
      "\n",
      "Train set: Average log loss: -18.0576\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9500\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2647 [0/90000 (0%)]\tlog Loss: -18.130311\n",
      "\n",
      "Train set: Average log loss: -18.0577\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9500\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2648 [0/90000 (0%)]\tlog Loss: -18.130331\n",
      "\n",
      "Train set: Average log loss: -18.0578\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9501\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2649 [0/90000 (0%)]\tlog Loss: -18.130286\n",
      "\n",
      "Train set: Average log loss: -18.0578\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9502\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2650 [0/90000 (0%)]\tlog Loss: -18.130434\n",
      "\n",
      "Train set: Average log loss: -18.0579\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9502\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2651 [0/90000 (0%)]\tlog Loss: -18.130485\n",
      "\n",
      "Train set: Average log loss: -18.0580\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9503\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2652 [0/90000 (0%)]\tlog Loss: -18.130453\n",
      "\n",
      "Train set: Average log loss: -18.0581\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9503\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2653 [0/90000 (0%)]\tlog Loss: -18.130574\n",
      "\n",
      "Train set: Average log loss: -18.0582\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9505\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2654 [0/90000 (0%)]\tlog Loss: -18.130608\n",
      "\n",
      "Train set: Average log loss: -18.0582\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9505\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2655 [0/90000 (0%)]\tlog Loss: -18.130635\n",
      "\n",
      "Train set: Average log loss: -18.0583\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9506\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2656 [0/90000 (0%)]\tlog Loss: -18.130892\n",
      "\n",
      "Train set: Average log loss: -18.0584\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9506\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2657 [0/90000 (0%)]\tlog Loss: -18.130811\n",
      "\n",
      "Train set: Average log loss: -18.0585\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9507\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2658 [0/90000 (0%)]\tlog Loss: -18.130978\n",
      "\n",
      "Train set: Average log loss: -18.0585\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9508\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2659 [0/90000 (0%)]\tlog Loss: -18.130950\n",
      "\n",
      "Train set: Average log loss: -18.0586\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9509\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2660 [0/90000 (0%)]\tlog Loss: -18.131117\n",
      "\n",
      "Train set: Average log loss: -18.0587\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9510\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2661 [0/90000 (0%)]\tlog Loss: -18.131147\n",
      "\n",
      "Train set: Average log loss: -18.0588\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9510\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2662 [0/90000 (0%)]\tlog Loss: -18.131139\n",
      "\n",
      "Train set: Average log loss: -18.0589\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9512\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2663 [0/90000 (0%)]\tlog Loss: -18.131408\n",
      "\n",
      "Train set: Average log loss: -18.0589\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9512\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2664 [0/90000 (0%)]\tlog Loss: -18.131328\n",
      "\n",
      "Train set: Average log loss: -18.0590\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9514\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2665 [0/90000 (0%)]\tlog Loss: -18.131531\n",
      "\n",
      "Train set: Average log loss: -18.0591\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9513\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2666 [0/90000 (0%)]\tlog Loss: -18.131416\n",
      "\n",
      "Train set: Average log loss: -18.0592\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9515\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2667 [0/90000 (0%)]\tlog Loss: -18.131617\n",
      "\n",
      "Train set: Average log loss: -18.0593\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9514\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2668 [0/90000 (0%)]\tlog Loss: -18.131487\n",
      "\n",
      "Train set: Average log loss: -18.0593\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9515\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2669 [0/90000 (0%)]\tlog Loss: -18.131718\n",
      "\n",
      "Train set: Average log loss: -18.0594\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9517\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2670 [0/90000 (0%)]\tlog Loss: -18.131673\n",
      "\n",
      "Train set: Average log loss: -18.0595\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9518\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2671 [0/90000 (0%)]\tlog Loss: -18.131982\n",
      "\n",
      "Train set: Average log loss: -18.0596\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9520\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2672 [0/90000 (0%)]\tlog Loss: -18.131999\n",
      "\n",
      "Train set: Average log loss: -18.0597\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9520\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2673 [0/90000 (0%)]\tlog Loss: -18.131908\n",
      "\n",
      "Train set: Average log loss: -18.0598\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9521\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2674 [0/90000 (0%)]\tlog Loss: -18.132014\n",
      "\n",
      "Train set: Average log loss: -18.0599\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9521\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2675 [0/90000 (0%)]\tlog Loss: -18.131972\n",
      "\n",
      "Train set: Average log loss: -18.0599\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9523\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2676 [0/90000 (0%)]\tlog Loss: -18.132298\n",
      "\n",
      "Train set: Average log loss: -18.0600\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9524\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2677 [0/90000 (0%)]\tlog Loss: -18.132430\n",
      "\n",
      "Train set: Average log loss: -18.0601\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9524\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2678 [0/90000 (0%)]\tlog Loss: -18.132392\n",
      "\n",
      "Train set: Average log loss: -18.0602\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9525\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2679 [0/90000 (0%)]\tlog Loss: -18.132575\n",
      "\n",
      "Train set: Average log loss: -18.0603\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9526\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2680 [0/90000 (0%)]\tlog Loss: -18.132657\n",
      "\n",
      "Train set: Average log loss: -18.0603\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9526\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2681 [0/90000 (0%)]\tlog Loss: -18.132745\n",
      "\n",
      "Train set: Average log loss: -18.0604\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9528\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2682 [0/90000 (0%)]\tlog Loss: -18.132825\n",
      "\n",
      "Train set: Average log loss: -18.0605\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9528\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2683 [0/90000 (0%)]\tlog Loss: -18.132950\n",
      "\n",
      "Train set: Average log loss: -18.0606\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9529\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2684 [0/90000 (0%)]\tlog Loss: -18.133012\n",
      "\n",
      "Train set: Average log loss: -18.0607\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9529\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2685 [0/90000 (0%)]\tlog Loss: -18.133079\n",
      "\n",
      "Train set: Average log loss: -18.0607\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9530\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2686 [0/90000 (0%)]\tlog Loss: -18.133073\n",
      "\n",
      "Train set: Average log loss: -18.0608\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9531\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2687 [0/90000 (0%)]\tlog Loss: -18.133220\n",
      "\n",
      "Train set: Average log loss: -18.0609\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9531\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2688 [0/90000 (0%)]\tlog Loss: -18.133236\n",
      "\n",
      "Train set: Average log loss: -18.0610\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9533\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2689 [0/90000 (0%)]\tlog Loss: -18.133453\n",
      "\n",
      "Train set: Average log loss: -18.0611\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9534\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2690 [0/90000 (0%)]\tlog Loss: -18.133482\n",
      "\n",
      "Train set: Average log loss: -18.0611\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9534\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2691 [0/90000 (0%)]\tlog Loss: -18.133582\n",
      "\n",
      "Train set: Average log loss: -18.0612\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9535\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2692 [0/90000 (0%)]\tlog Loss: -18.133746\n",
      "\n",
      "Train set: Average log loss: -18.0613\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9536\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2693 [0/90000 (0%)]\tlog Loss: -18.133849\n",
      "\n",
      "Train set: Average log loss: -18.0614\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9537\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2694 [0/90000 (0%)]\tlog Loss: -18.133803\n",
      "\n",
      "Train set: Average log loss: -18.0615\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9538\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2695 [0/90000 (0%)]\tlog Loss: -18.133915\n",
      "\n",
      "Train set: Average log loss: -18.0615\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9538\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2696 [0/90000 (0%)]\tlog Loss: -18.133942\n",
      "\n",
      "Train set: Average log loss: -18.0616\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9540\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2697 [0/90000 (0%)]\tlog Loss: -18.134113\n",
      "\n",
      "Train set: Average log loss: -18.0617\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9540\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2698 [0/90000 (0%)]\tlog Loss: -18.134100\n",
      "\n",
      "Train set: Average log loss: -18.0618\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9541\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2699 [0/90000 (0%)]\tlog Loss: -18.134232\n",
      "\n",
      "Train set: Average log loss: -18.0619\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9542\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2700 [0/90000 (0%)]\tlog Loss: -18.134283\n",
      "\n",
      "Train set: Average log loss: -18.0620\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9543\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2701 [0/90000 (0%)]\tlog Loss: -18.134556\n",
      "\n",
      "Train set: Average log loss: -18.0620\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9544\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2702 [0/90000 (0%)]\tlog Loss: -18.134480\n",
      "\n",
      "Train set: Average log loss: -18.0621\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9545\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2703 [0/90000 (0%)]\tlog Loss: -18.134809\n",
      "\n",
      "Train set: Average log loss: -18.0622\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9547\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2704 [0/90000 (0%)]\tlog Loss: -18.135079\n",
      "\n",
      "Train set: Average log loss: -18.0623\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9548\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2705 [0/90000 (0%)]\tlog Loss: -18.135072\n",
      "\n",
      "Train set: Average log loss: -18.0624\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9550\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2706 [0/90000 (0%)]\tlog Loss: -18.135216\n",
      "\n",
      "Train set: Average log loss: -18.0624\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9550\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2707 [0/90000 (0%)]\tlog Loss: -18.135257\n",
      "\n",
      "Train set: Average log loss: -18.0625\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9552\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2708 [0/90000 (0%)]\tlog Loss: -18.135479\n",
      "\n",
      "Train set: Average log loss: -18.0626\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9552\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2709 [0/90000 (0%)]\tlog Loss: -18.135544\n",
      "\n",
      "Train set: Average log loss: -18.0627\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9554\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2710 [0/90000 (0%)]\tlog Loss: -18.135663\n",
      "\n",
      "Train set: Average log loss: -18.0628\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9555\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2711 [0/90000 (0%)]\tlog Loss: -18.135819\n",
      "\n",
      "Train set: Average log loss: -18.0628\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9556\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2712 [0/90000 (0%)]\tlog Loss: -18.135879\n",
      "\n",
      "Train set: Average log loss: -18.0629\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9556\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2713 [0/90000 (0%)]\tlog Loss: -18.136105\n",
      "\n",
      "Train set: Average log loss: -18.0630\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9558\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2714 [0/90000 (0%)]\tlog Loss: -18.136049\n",
      "\n",
      "Train set: Average log loss: -18.0631\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9558\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2715 [0/90000 (0%)]\tlog Loss: -18.136320\n",
      "\n",
      "Train set: Average log loss: -18.0632\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9560\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2716 [0/90000 (0%)]\tlog Loss: -18.136282\n",
      "\n",
      "Train set: Average log loss: -18.0633\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9561\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2717 [0/90000 (0%)]\tlog Loss: -18.136406\n",
      "\n",
      "Train set: Average log loss: -18.0633\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9562\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2718 [0/90000 (0%)]\tlog Loss: -18.136437\n",
      "\n",
      "Train set: Average log loss: -18.0634\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9562\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2719 [0/90000 (0%)]\tlog Loss: -18.136484\n",
      "\n",
      "Train set: Average log loss: -18.0635\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9563\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2720 [0/90000 (0%)]\tlog Loss: -18.136713\n",
      "\n",
      "Train set: Average log loss: -18.0636\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9564\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2721 [0/90000 (0%)]\tlog Loss: -18.136825\n",
      "\n",
      "Train set: Average log loss: -18.0636\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9565\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2722 [0/90000 (0%)]\tlog Loss: -18.136925\n",
      "\n",
      "Train set: Average log loss: -18.0637\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9566\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2723 [0/90000 (0%)]\tlog Loss: -18.137261\n",
      "\n",
      "Train set: Average log loss: -18.0638\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9567\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2724 [0/90000 (0%)]\tlog Loss: -18.137231\n",
      "\n",
      "Train set: Average log loss: -18.0639\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9569\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2725 [0/90000 (0%)]\tlog Loss: -18.137357\n",
      "\n",
      "Train set: Average log loss: -18.0640\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9569\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2726 [0/90000 (0%)]\tlog Loss: -18.137537\n",
      "\n",
      "Train set: Average log loss: -18.0640\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9571\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2727 [0/90000 (0%)]\tlog Loss: -18.137588\n",
      "\n",
      "Train set: Average log loss: -18.0641\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9571\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2728 [0/90000 (0%)]\tlog Loss: -18.137716\n",
      "\n",
      "Train set: Average log loss: -18.0642\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9573\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2729 [0/90000 (0%)]\tlog Loss: -18.137901\n",
      "\n",
      "Train set: Average log loss: -18.0643\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9573\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2730 [0/90000 (0%)]\tlog Loss: -18.137961\n",
      "\n",
      "Train set: Average log loss: -18.0643\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9575\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2731 [0/90000 (0%)]\tlog Loss: -18.138109\n",
      "\n",
      "Train set: Average log loss: -18.0645\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9576\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2732 [0/90000 (0%)]\tlog Loss: -18.138241\n",
      "\n",
      "Train set: Average log loss: -18.0646\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9578\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2733 [0/90000 (0%)]\tlog Loss: -18.138463\n",
      "\n",
      "Train set: Average log loss: -18.0646\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9579\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2734 [0/90000 (0%)]\tlog Loss: -18.138641\n",
      "\n",
      "Train set: Average log loss: -18.0647\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9580\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2735 [0/90000 (0%)]\tlog Loss: -18.138761\n",
      "\n",
      "Train set: Average log loss: -18.0648\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9581\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2736 [0/90000 (0%)]\tlog Loss: -18.138798\n",
      "\n",
      "Train set: Average log loss: -18.0649\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9583\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2737 [0/90000 (0%)]\tlog Loss: -18.139099\n",
      "\n",
      "Train set: Average log loss: -18.0650\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9584\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2738 [0/90000 (0%)]\tlog Loss: -18.139270\n",
      "\n",
      "Train set: Average log loss: -18.0650\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9585\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2739 [0/90000 (0%)]\tlog Loss: -18.139244\n",
      "\n",
      "Train set: Average log loss: -18.0651\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9586\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2740 [0/90000 (0%)]\tlog Loss: -18.139531\n",
      "\n",
      "Train set: Average log loss: -18.0652\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9587\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2741 [0/90000 (0%)]\tlog Loss: -18.139534\n",
      "\n",
      "Train set: Average log loss: -18.0653\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9588\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2742 [0/90000 (0%)]\tlog Loss: -18.139739\n",
      "\n",
      "Train set: Average log loss: -18.0653\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9589\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2743 [0/90000 (0%)]\tlog Loss: -18.139891\n",
      "\n",
      "Train set: Average log loss: -18.0654\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9589\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2744 [0/90000 (0%)]\tlog Loss: -18.139889\n",
      "\n",
      "Train set: Average log loss: -18.0655\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9590\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2745 [0/90000 (0%)]\tlog Loss: -18.140072\n",
      "\n",
      "Train set: Average log loss: -18.0656\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9591\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2746 [0/90000 (0%)]\tlog Loss: -18.140131\n",
      "\n",
      "Train set: Average log loss: -18.0656\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9592\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2747 [0/90000 (0%)]\tlog Loss: -18.140416\n",
      "\n",
      "Train set: Average log loss: -18.0657\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9592\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2748 [0/90000 (0%)]\tlog Loss: -18.140349\n",
      "\n",
      "Train set: Average log loss: -18.0658\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9594\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2749 [0/90000 (0%)]\tlog Loss: -18.140579\n",
      "\n",
      "Train set: Average log loss: -18.0659\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9593\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2750 [0/90000 (0%)]\tlog Loss: -18.140376\n",
      "\n",
      "Train set: Average log loss: -18.0659\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9594\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2751 [0/90000 (0%)]\tlog Loss: -18.140704\n",
      "\n",
      "Train set: Average log loss: -18.0660\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9595\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2752 [0/90000 (0%)]\tlog Loss: -18.140654\n",
      "\n",
      "Train set: Average log loss: -18.0661\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9595\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2753 [0/90000 (0%)]\tlog Loss: -18.140844\n",
      "\n",
      "Train set: Average log loss: -18.0662\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9596\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2754 [0/90000 (0%)]\tlog Loss: -18.140779\n",
      "\n",
      "Train set: Average log loss: -18.0662\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9597\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2755 [0/90000 (0%)]\tlog Loss: -18.140947\n",
      "\n",
      "Train set: Average log loss: -18.0663\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9598\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2756 [0/90000 (0%)]\tlog Loss: -18.140856\n",
      "\n",
      "Train set: Average log loss: -18.0664\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9598\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2757 [0/90000 (0%)]\tlog Loss: -18.141070\n",
      "\n",
      "Train set: Average log loss: -18.0665\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9600\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2758 [0/90000 (0%)]\tlog Loss: -18.141153\n",
      "\n",
      "Train set: Average log loss: -18.0666\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9600\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2759 [0/90000 (0%)]\tlog Loss: -18.141244\n",
      "\n",
      "Train set: Average log loss: -18.0667\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9601\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2760 [0/90000 (0%)]\tlog Loss: -18.141297\n",
      "\n",
      "Train set: Average log loss: -18.0667\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9602\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2761 [0/90000 (0%)]\tlog Loss: -18.141465\n",
      "\n",
      "Train set: Average log loss: -18.0668\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9602\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2762 [0/90000 (0%)]\tlog Loss: -18.141598\n",
      "\n",
      "Train set: Average log loss: -18.0669\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9604\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2763 [0/90000 (0%)]\tlog Loss: -18.141646\n",
      "\n",
      "Train set: Average log loss: -18.0670\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9604\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2764 [0/90000 (0%)]\tlog Loss: -18.141726\n",
      "\n",
      "Train set: Average log loss: -18.0671\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9606\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2765 [0/90000 (0%)]\tlog Loss: -18.141941\n",
      "\n",
      "Train set: Average log loss: -18.0671\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9606\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2766 [0/90000 (0%)]\tlog Loss: -18.141802\n",
      "\n",
      "Train set: Average log loss: -18.0672\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9607\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2767 [0/90000 (0%)]\tlog Loss: -18.142180\n",
      "\n",
      "Train set: Average log loss: -18.0673\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9608\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2768 [0/90000 (0%)]\tlog Loss: -18.142091\n",
      "\n",
      "Train set: Average log loss: -18.0674\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9609\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2769 [0/90000 (0%)]\tlog Loss: -18.142147\n",
      "\n",
      "Train set: Average log loss: -18.0675\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9610\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2770 [0/90000 (0%)]\tlog Loss: -18.142369\n",
      "\n",
      "Train set: Average log loss: -18.0675\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9611\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2771 [0/90000 (0%)]\tlog Loss: -18.142410\n",
      "\n",
      "Train set: Average log loss: -18.0677\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9613\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2772 [0/90000 (0%)]\tlog Loss: -18.142672\n",
      "\n",
      "Train set: Average log loss: -18.0677\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9613\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2773 [0/90000 (0%)]\tlog Loss: -18.142762\n",
      "\n",
      "Train set: Average log loss: -18.0678\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9614\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2774 [0/90000 (0%)]\tlog Loss: -18.142830\n",
      "\n",
      "Train set: Average log loss: -18.0679\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9615\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2775 [0/90000 (0%)]\tlog Loss: -18.142961\n",
      "\n",
      "Train set: Average log loss: -18.0679\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9617\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2776 [0/90000 (0%)]\tlog Loss: -18.143254\n",
      "\n",
      "Train set: Average log loss: -18.0680\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9617\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2777 [0/90000 (0%)]\tlog Loss: -18.143209\n",
      "\n",
      "Train set: Average log loss: -18.0681\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9618\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2778 [0/90000 (0%)]\tlog Loss: -18.143255\n",
      "\n",
      "Train set: Average log loss: -18.0682\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9619\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2779 [0/90000 (0%)]\tlog Loss: -18.143550\n",
      "\n",
      "Train set: Average log loss: -18.0683\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9621\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2780 [0/90000 (0%)]\tlog Loss: -18.143632\n",
      "\n",
      "Train set: Average log loss: -18.0684\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9621\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2781 [0/90000 (0%)]\tlog Loss: -18.143719\n",
      "\n",
      "Train set: Average log loss: -18.0684\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9622\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2782 [0/90000 (0%)]\tlog Loss: -18.143857\n",
      "\n",
      "Train set: Average log loss: -18.0685\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9623\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2783 [0/90000 (0%)]\tlog Loss: -18.143993\n",
      "\n",
      "Train set: Average log loss: -18.0686\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9625\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2784 [0/90000 (0%)]\tlog Loss: -18.144214\n",
      "\n",
      "Train set: Average log loss: -18.0687\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9625\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2785 [0/90000 (0%)]\tlog Loss: -18.144075\n",
      "\n",
      "Train set: Average log loss: -18.0687\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9626\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2786 [0/90000 (0%)]\tlog Loss: -18.144279\n",
      "\n",
      "Train set: Average log loss: -18.0688\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9627\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2787 [0/90000 (0%)]\tlog Loss: -18.144358\n",
      "\n",
      "Train set: Average log loss: -18.0689\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9628\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2788 [0/90000 (0%)]\tlog Loss: -18.144412\n",
      "\n",
      "Train set: Average log loss: -18.0690\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9629\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2789 [0/90000 (0%)]\tlog Loss: -18.144517\n",
      "\n",
      "Train set: Average log loss: -18.0690\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9630\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2790 [0/90000 (0%)]\tlog Loss: -18.144554\n",
      "\n",
      "Train set: Average log loss: -18.0691\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9632\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2791 [0/90000 (0%)]\tlog Loss: -18.144717\n",
      "\n",
      "Train set: Average log loss: -18.0692\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9633\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2792 [0/90000 (0%)]\tlog Loss: -18.144928\n",
      "\n",
      "Train set: Average log loss: -18.0693\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9633\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2793 [0/90000 (0%)]\tlog Loss: -18.144887\n",
      "\n",
      "Train set: Average log loss: -18.0694\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9635\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2794 [0/90000 (0%)]\tlog Loss: -18.145003\n",
      "\n",
      "Train set: Average log loss: -18.0694\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9636\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2795 [0/90000 (0%)]\tlog Loss: -18.145145\n",
      "\n",
      "Train set: Average log loss: -18.0695\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9637\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2796 [0/90000 (0%)]\tlog Loss: -18.145390\n",
      "\n",
      "Train set: Average log loss: -18.0696\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9638\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2797 [0/90000 (0%)]\tlog Loss: -18.145413\n",
      "\n",
      "Train set: Average log loss: -18.0697\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9639\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2798 [0/90000 (0%)]\tlog Loss: -18.145459\n",
      "\n",
      "Train set: Average log loss: -18.0698\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9640\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2799 [0/90000 (0%)]\tlog Loss: -18.145715\n",
      "\n",
      "Train set: Average log loss: -18.0699\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9641\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2800 [0/90000 (0%)]\tlog Loss: -18.145690\n",
      "\n",
      "Train set: Average log loss: -18.0699\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9642\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2801 [0/90000 (0%)]\tlog Loss: -18.145903\n",
      "\n",
      "Train set: Average log loss: -18.0700\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9642\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2802 [0/90000 (0%)]\tlog Loss: -18.145797\n",
      "\n",
      "Train set: Average log loss: -18.0701\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9644\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2803 [0/90000 (0%)]\tlog Loss: -18.146076\n",
      "\n",
      "Train set: Average log loss: -18.0702\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9644\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2804 [0/90000 (0%)]\tlog Loss: -18.146014\n",
      "\n",
      "Train set: Average log loss: -18.0702\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9645\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2805 [0/90000 (0%)]\tlog Loss: -18.146193\n",
      "\n",
      "Train set: Average log loss: -18.0703\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9646\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2806 [0/90000 (0%)]\tlog Loss: -18.146228\n",
      "\n",
      "Train set: Average log loss: -18.0704\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9648\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2807 [0/90000 (0%)]\tlog Loss: -18.146358\n",
      "\n",
      "Train set: Average log loss: -18.0705\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9648\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2808 [0/90000 (0%)]\tlog Loss: -18.146559\n",
      "\n",
      "Train set: Average log loss: -18.0706\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9648\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2809 [0/90000 (0%)]\tlog Loss: -18.146561\n",
      "\n",
      "Train set: Average log loss: -18.0706\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9650\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2810 [0/90000 (0%)]\tlog Loss: -18.146658\n",
      "\n",
      "Train set: Average log loss: -18.0707\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9650\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2811 [0/90000 (0%)]\tlog Loss: -18.146760\n",
      "\n",
      "Train set: Average log loss: -18.0708\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9651\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2812 [0/90000 (0%)]\tlog Loss: -18.146983\n",
      "\n",
      "Train set: Average log loss: -18.0709\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9652\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2813 [0/90000 (0%)]\tlog Loss: -18.147008\n",
      "\n",
      "Train set: Average log loss: -18.0709\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9653\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2814 [0/90000 (0%)]\tlog Loss: -18.147060\n",
      "\n",
      "Train set: Average log loss: -18.0710\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9653\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2815 [0/90000 (0%)]\tlog Loss: -18.147268\n",
      "\n",
      "Train set: Average log loss: -18.0711\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9654\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2816 [0/90000 (0%)]\tlog Loss: -18.147237\n",
      "\n",
      "Train set: Average log loss: -18.0712\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9654\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2817 [0/90000 (0%)]\tlog Loss: -18.147264\n",
      "\n",
      "Train set: Average log loss: -18.0712\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9655\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2818 [0/90000 (0%)]\tlog Loss: -18.147311\n",
      "\n",
      "Train set: Average log loss: -18.0713\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9656\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2819 [0/90000 (0%)]\tlog Loss: -18.147470\n",
      "\n",
      "Train set: Average log loss: -18.0714\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9656\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2820 [0/90000 (0%)]\tlog Loss: -18.147464\n",
      "\n",
      "Train set: Average log loss: -18.0714\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9658\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2821 [0/90000 (0%)]\tlog Loss: -18.147691\n",
      "\n",
      "Train set: Average log loss: -18.0715\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9657\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2822 [0/90000 (0%)]\tlog Loss: -18.147621\n",
      "\n",
      "Train set: Average log loss: -18.0716\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9658\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2823 [0/90000 (0%)]\tlog Loss: -18.147674\n",
      "\n",
      "Train set: Average log loss: -18.0717\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9659\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2824 [0/90000 (0%)]\tlog Loss: -18.147870\n",
      "\n",
      "Train set: Average log loss: -18.0718\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9660\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2825 [0/90000 (0%)]\tlog Loss: -18.147834\n",
      "\n",
      "Train set: Average log loss: -18.0719\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9660\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2826 [0/90000 (0%)]\tlog Loss: -18.147980\n",
      "\n",
      "Train set: Average log loss: -18.0719\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9662\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2827 [0/90000 (0%)]\tlog Loss: -18.148167\n",
      "\n",
      "Train set: Average log loss: -18.0720\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9663\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2828 [0/90000 (0%)]\tlog Loss: -18.148097\n",
      "\n",
      "Train set: Average log loss: -18.0721\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9663\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2829 [0/90000 (0%)]\tlog Loss: -18.148131\n",
      "\n",
      "Train set: Average log loss: -18.0721\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9664\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2830 [0/90000 (0%)]\tlog Loss: -18.148352\n",
      "\n",
      "Train set: Average log loss: -18.0722\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9664\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2831 [0/90000 (0%)]\tlog Loss: -18.148353\n",
      "\n",
      "Train set: Average log loss: -18.0723\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9666\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2832 [0/90000 (0%)]\tlog Loss: -18.148478\n",
      "\n",
      "Train set: Average log loss: -18.0724\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9667\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2833 [0/90000 (0%)]\tlog Loss: -18.148854\n",
      "\n",
      "Train set: Average log loss: -18.0725\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9668\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2834 [0/90000 (0%)]\tlog Loss: -18.148828\n",
      "\n",
      "Train set: Average log loss: -18.0725\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9668\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2835 [0/90000 (0%)]\tlog Loss: -18.148883\n",
      "\n",
      "Train set: Average log loss: -18.0726\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9668\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2836 [0/90000 (0%)]\tlog Loss: -18.148910\n",
      "\n",
      "Train set: Average log loss: -18.0727\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9671\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2837 [0/90000 (0%)]\tlog Loss: -18.149186\n",
      "\n",
      "Train set: Average log loss: -18.0728\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9672\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2838 [0/90000 (0%)]\tlog Loss: -18.149277\n",
      "\n",
      "Train set: Average log loss: -18.0729\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9672\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2839 [0/90000 (0%)]\tlog Loss: -18.149317\n",
      "\n",
      "Train set: Average log loss: -18.0729\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9673\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2840 [0/90000 (0%)]\tlog Loss: -18.149558\n",
      "\n",
      "Train set: Average log loss: -18.0730\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9674\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2841 [0/90000 (0%)]\tlog Loss: -18.149602\n",
      "\n",
      "Train set: Average log loss: -18.0731\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9675\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2842 [0/90000 (0%)]\tlog Loss: -18.149774\n",
      "\n",
      "Train set: Average log loss: -18.0732\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9676\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2843 [0/90000 (0%)]\tlog Loss: -18.149828\n",
      "\n",
      "Train set: Average log loss: -18.0733\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9676\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2844 [0/90000 (0%)]\tlog Loss: -18.149854\n",
      "\n",
      "Train set: Average log loss: -18.0733\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9678\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2845 [0/90000 (0%)]\tlog Loss: -18.150138\n",
      "\n",
      "Train set: Average log loss: -18.0734\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9678\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2846 [0/90000 (0%)]\tlog Loss: -18.150132\n",
      "\n",
      "Train set: Average log loss: -18.0735\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9680\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2847 [0/90000 (0%)]\tlog Loss: -18.150287\n",
      "\n",
      "Train set: Average log loss: -18.0736\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9681\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2848 [0/90000 (0%)]\tlog Loss: -18.150502\n",
      "\n",
      "Train set: Average log loss: -18.0736\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9682\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2849 [0/90000 (0%)]\tlog Loss: -18.150565\n",
      "\n",
      "Train set: Average log loss: -18.0737\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9682\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2850 [0/90000 (0%)]\tlog Loss: -18.150661\n",
      "\n",
      "Train set: Average log loss: -18.0738\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9683\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2851 [0/90000 (0%)]\tlog Loss: -18.150782\n",
      "\n",
      "Train set: Average log loss: -18.0739\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9684\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2852 [0/90000 (0%)]\tlog Loss: -18.150925\n",
      "\n",
      "Train set: Average log loss: -18.0740\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9685\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2853 [0/90000 (0%)]\tlog Loss: -18.150925\n",
      "\n",
      "Train set: Average log loss: -18.0740\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9686\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2854 [0/90000 (0%)]\tlog Loss: -18.151109\n",
      "\n",
      "Train set: Average log loss: -18.0741\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9687\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2855 [0/90000 (0%)]\tlog Loss: -18.151229\n",
      "\n",
      "Train set: Average log loss: -18.0742\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9688\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2856 [0/90000 (0%)]\tlog Loss: -18.151278\n",
      "\n",
      "Train set: Average log loss: -18.0743\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9688\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2857 [0/90000 (0%)]\tlog Loss: -18.151317\n",
      "\n",
      "Train set: Average log loss: -18.0744\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9689\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2858 [0/90000 (0%)]\tlog Loss: -18.151543\n",
      "\n",
      "Train set: Average log loss: -18.0744\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9690\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2859 [0/90000 (0%)]\tlog Loss: -18.151581\n",
      "\n",
      "Train set: Average log loss: -18.0745\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9691\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2860 [0/90000 (0%)]\tlog Loss: -18.151582\n",
      "\n",
      "Train set: Average log loss: -18.0746\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9692\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2861 [0/90000 (0%)]\tlog Loss: -18.151784\n",
      "\n",
      "Train set: Average log loss: -18.0747\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9693\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2862 [0/90000 (0%)]\tlog Loss: -18.151885\n",
      "\n",
      "Train set: Average log loss: -18.0747\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9694\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2863 [0/90000 (0%)]\tlog Loss: -18.151977\n",
      "\n",
      "Train set: Average log loss: -18.0748\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9694\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2864 [0/90000 (0%)]\tlog Loss: -18.151862\n",
      "\n",
      "Train set: Average log loss: -18.0749\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9695\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2865 [0/90000 (0%)]\tlog Loss: -18.152012\n",
      "\n",
      "Train set: Average log loss: -18.0749\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9696\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2866 [0/90000 (0%)]\tlog Loss: -18.152059\n",
      "\n",
      "Train set: Average log loss: -18.0750\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9696\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2867 [0/90000 (0%)]\tlog Loss: -18.152117\n",
      "\n",
      "Train set: Average log loss: -18.0751\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9697\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2868 [0/90000 (0%)]\tlog Loss: -18.152125\n",
      "\n",
      "Train set: Average log loss: -18.0752\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9698\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2869 [0/90000 (0%)]\tlog Loss: -18.152248\n",
      "\n",
      "Train set: Average log loss: -18.0752\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9699\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2870 [0/90000 (0%)]\tlog Loss: -18.152412\n",
      "\n",
      "Train set: Average log loss: -18.0753\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9700\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2871 [0/90000 (0%)]\tlog Loss: -18.152422\n",
      "\n",
      "Train set: Average log loss: -18.0754\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9701\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2872 [0/90000 (0%)]\tlog Loss: -18.152442\n",
      "\n",
      "Train set: Average log loss: -18.0755\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9700\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2873 [0/90000 (0%)]\tlog Loss: -18.152410\n",
      "\n",
      "Train set: Average log loss: -18.0755\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9702\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2874 [0/90000 (0%)]\tlog Loss: -18.152552\n",
      "\n",
      "Train set: Average log loss: -18.0756\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9702\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2875 [0/90000 (0%)]\tlog Loss: -18.152563\n",
      "\n",
      "Train set: Average log loss: -18.0757\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9703\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2876 [0/90000 (0%)]\tlog Loss: -18.152625\n",
      "\n",
      "Train set: Average log loss: -18.0758\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9704\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2877 [0/90000 (0%)]\tlog Loss: -18.152625\n",
      "\n",
      "Train set: Average log loss: -18.0758\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9705\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2878 [0/90000 (0%)]\tlog Loss: -18.152818\n",
      "\n",
      "Train set: Average log loss: -18.0759\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9705\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2879 [0/90000 (0%)]\tlog Loss: -18.152782\n",
      "\n",
      "Train set: Average log loss: -18.0760\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9707\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2880 [0/90000 (0%)]\tlog Loss: -18.152877\n",
      "\n",
      "Train set: Average log loss: -18.0761\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9706\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2881 [0/90000 (0%)]\tlog Loss: -18.152812\n",
      "\n",
      "Train set: Average log loss: -18.0762\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9708\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2882 [0/90000 (0%)]\tlog Loss: -18.152922\n",
      "\n",
      "Train set: Average log loss: -18.0762\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9709\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2883 [0/90000 (0%)]\tlog Loss: -18.152971\n",
      "\n",
      "Train set: Average log loss: -18.0763\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9709\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2884 [0/90000 (0%)]\tlog Loss: -18.152833\n",
      "\n",
      "Train set: Average log loss: -18.0764\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9709\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2885 [0/90000 (0%)]\tlog Loss: -18.152951\n",
      "\n",
      "Train set: Average log loss: -18.0764\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9710\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2886 [0/90000 (0%)]\tlog Loss: -18.152919\n",
      "\n",
      "Train set: Average log loss: -18.0765\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9711\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2887 [0/90000 (0%)]\tlog Loss: -18.153174\n",
      "\n",
      "Train set: Average log loss: -18.0766\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9712\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2888 [0/90000 (0%)]\tlog Loss: -18.153182\n",
      "\n",
      "Train set: Average log loss: -18.0767\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9713\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2889 [0/90000 (0%)]\tlog Loss: -18.153278\n",
      "\n",
      "Train set: Average log loss: -18.0768\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9713\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2890 [0/90000 (0%)]\tlog Loss: -18.153272\n",
      "\n",
      "Train set: Average log loss: -18.0768\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9714\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2891 [0/90000 (0%)]\tlog Loss: -18.153368\n",
      "\n",
      "Train set: Average log loss: -18.0769\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9715\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2892 [0/90000 (0%)]\tlog Loss: -18.153549\n",
      "\n",
      "Train set: Average log loss: -18.0770\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9715\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2893 [0/90000 (0%)]\tlog Loss: -18.153359\n",
      "\n",
      "Train set: Average log loss: -18.0770\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9715\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2894 [0/90000 (0%)]\tlog Loss: -18.153490\n",
      "\n",
      "Train set: Average log loss: -18.0771\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9717\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2895 [0/90000 (0%)]\tlog Loss: -18.153660\n",
      "\n",
      "Train set: Average log loss: -18.0772\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9718\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2896 [0/90000 (0%)]\tlog Loss: -18.153702\n",
      "\n",
      "Train set: Average log loss: -18.0773\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9718\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2897 [0/90000 (0%)]\tlog Loss: -18.153789\n",
      "\n",
      "Train set: Average log loss: -18.0773\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9718\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2898 [0/90000 (0%)]\tlog Loss: -18.153770\n",
      "\n",
      "Train set: Average log loss: -18.0774\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9719\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2899 [0/90000 (0%)]\tlog Loss: -18.153772\n",
      "\n",
      "Train set: Average log loss: -18.0775\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9720\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2900 [0/90000 (0%)]\tlog Loss: -18.153934\n",
      "\n",
      "Train set: Average log loss: -18.0776\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9720\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2901 [0/90000 (0%)]\tlog Loss: -18.154026\n",
      "\n",
      "Train set: Average log loss: -18.0776\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9720\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2902 [0/90000 (0%)]\tlog Loss: -18.153835\n",
      "\n",
      "Train set: Average log loss: -18.0777\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9722\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2903 [0/90000 (0%)]\tlog Loss: -18.154093\n",
      "\n",
      "Train set: Average log loss: -18.0778\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9722\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2904 [0/90000 (0%)]\tlog Loss: -18.153976\n",
      "\n",
      "Train set: Average log loss: -18.0779\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9722\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2905 [0/90000 (0%)]\tlog Loss: -18.154114\n",
      "\n",
      "Train set: Average log loss: -18.0780\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9722\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2906 [0/90000 (0%)]\tlog Loss: -18.154059\n",
      "\n",
      "Train set: Average log loss: -18.0780\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9723\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2907 [0/90000 (0%)]\tlog Loss: -18.154133\n",
      "\n",
      "Train set: Average log loss: -18.0781\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9723\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2908 [0/90000 (0%)]\tlog Loss: -18.154113\n",
      "\n",
      "Train set: Average log loss: -18.0782\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9724\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2909 [0/90000 (0%)]\tlog Loss: -18.154239\n",
      "\n",
      "Train set: Average log loss: -18.0782\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9724\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2910 [0/90000 (0%)]\tlog Loss: -18.154162\n",
      "\n",
      "Train set: Average log loss: -18.0783\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9725\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2911 [0/90000 (0%)]\tlog Loss: -18.154322\n",
      "\n",
      "Train set: Average log loss: -18.0784\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9725\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2912 [0/90000 (0%)]\tlog Loss: -18.154418\n",
      "\n",
      "Train set: Average log loss: -18.0785\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9726\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2913 [0/90000 (0%)]\tlog Loss: -18.154370\n",
      "\n",
      "Train set: Average log loss: -18.0785\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9726\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2914 [0/90000 (0%)]\tlog Loss: -18.154288\n",
      "\n",
      "Train set: Average log loss: -18.0786\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9727\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2915 [0/90000 (0%)]\tlog Loss: -18.154465\n",
      "\n",
      "Train set: Average log loss: -18.0787\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9727\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2916 [0/90000 (0%)]\tlog Loss: -18.154362\n",
      "\n",
      "Train set: Average log loss: -18.0788\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9728\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2917 [0/90000 (0%)]\tlog Loss: -18.154345\n",
      "\n",
      "Train set: Average log loss: -18.0788\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9729\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2918 [0/90000 (0%)]\tlog Loss: -18.154425\n",
      "\n",
      "Train set: Average log loss: -18.0789\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9731\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2919 [0/90000 (0%)]\tlog Loss: -18.154594\n",
      "\n",
      "Train set: Average log loss: -18.0790\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9731\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2920 [0/90000 (0%)]\tlog Loss: -18.154607\n",
      "\n",
      "Train set: Average log loss: -18.0790\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9731\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2921 [0/90000 (0%)]\tlog Loss: -18.154743\n",
      "\n",
      "Train set: Average log loss: -18.0791\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9732\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2922 [0/90000 (0%)]\tlog Loss: -18.154759\n",
      "\n",
      "Train set: Average log loss: -18.0792\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9733\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2923 [0/90000 (0%)]\tlog Loss: -18.154764\n",
      "\n",
      "Train set: Average log loss: -18.0793\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9732\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2924 [0/90000 (0%)]\tlog Loss: -18.154819\n",
      "\n",
      "Train set: Average log loss: -18.0793\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9734\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2925 [0/90000 (0%)]\tlog Loss: -18.154899\n",
      "\n",
      "Train set: Average log loss: -18.0794\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9735\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2926 [0/90000 (0%)]\tlog Loss: -18.155073\n",
      "\n",
      "Train set: Average log loss: -18.0795\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9735\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2927 [0/90000 (0%)]\tlog Loss: -18.154988\n",
      "\n",
      "Train set: Average log loss: -18.0796\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9735\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2928 [0/90000 (0%)]\tlog Loss: -18.155149\n",
      "\n",
      "Train set: Average log loss: -18.0796\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9736\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2929 [0/90000 (0%)]\tlog Loss: -18.155314\n",
      "\n",
      "Train set: Average log loss: -18.0797\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9736\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2930 [0/90000 (0%)]\tlog Loss: -18.155339\n",
      "\n",
      "Train set: Average log loss: -18.0798\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9737\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2931 [0/90000 (0%)]\tlog Loss: -18.155351\n",
      "\n",
      "Train set: Average log loss: -18.0799\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9738\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2932 [0/90000 (0%)]\tlog Loss: -18.155338\n",
      "\n",
      "Train set: Average log loss: -18.0800\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9739\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2933 [0/90000 (0%)]\tlog Loss: -18.155584\n",
      "\n",
      "Train set: Average log loss: -18.0800\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9740\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2934 [0/90000 (0%)]\tlog Loss: -18.155643\n",
      "\n",
      "Train set: Average log loss: -18.0801\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9740\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2935 [0/90000 (0%)]\tlog Loss: -18.155780\n",
      "\n",
      "Train set: Average log loss: -18.0802\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9742\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2936 [0/90000 (0%)]\tlog Loss: -18.155860\n",
      "\n",
      "Train set: Average log loss: -18.0803\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9742\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2937 [0/90000 (0%)]\tlog Loss: -18.155953\n",
      "\n",
      "Train set: Average log loss: -18.0803\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9743\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2938 [0/90000 (0%)]\tlog Loss: -18.156225\n",
      "\n",
      "Train set: Average log loss: -18.0804\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9745\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2939 [0/90000 (0%)]\tlog Loss: -18.156332\n",
      "\n",
      "Train set: Average log loss: -18.0805\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9745\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2940 [0/90000 (0%)]\tlog Loss: -18.156324\n",
      "\n",
      "Train set: Average log loss: -18.0806\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9747\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2941 [0/90000 (0%)]\tlog Loss: -18.156447\n",
      "\n",
      "Train set: Average log loss: -18.0807\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9748\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2942 [0/90000 (0%)]\tlog Loss: -18.156693\n",
      "\n",
      "Train set: Average log loss: -18.0807\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9749\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2943 [0/90000 (0%)]\tlog Loss: -18.156633\n",
      "\n",
      "Train set: Average log loss: -18.0808\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9749\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2944 [0/90000 (0%)]\tlog Loss: -18.156633\n",
      "\n",
      "Train set: Average log loss: -18.0809\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9750\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2945 [0/90000 (0%)]\tlog Loss: -18.156780\n",
      "\n",
      "Train set: Average log loss: -18.0810\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9752\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2946 [0/90000 (0%)]\tlog Loss: -18.157014\n",
      "\n",
      "Train set: Average log loss: -18.0810\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9753\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2947 [0/90000 (0%)]\tlog Loss: -18.157035\n",
      "\n",
      "Train set: Average log loss: -18.0811\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9753\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2948 [0/90000 (0%)]\tlog Loss: -18.157204\n",
      "\n",
      "Train set: Average log loss: -18.0812\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9755\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2949 [0/90000 (0%)]\tlog Loss: -18.157276\n",
      "\n",
      "Train set: Average log loss: -18.0813\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9755\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2950 [0/90000 (0%)]\tlog Loss: -18.157338\n",
      "\n",
      "Train set: Average log loss: -18.0813\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9756\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2951 [0/90000 (0%)]\tlog Loss: -18.157460\n",
      "\n",
      "Train set: Average log loss: -18.0814\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9757\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2952 [0/90000 (0%)]\tlog Loss: -18.157588\n",
      "\n",
      "Train set: Average log loss: -18.0815\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9759\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2953 [0/90000 (0%)]\tlog Loss: -18.157703\n",
      "\n",
      "Train set: Average log loss: -18.0816\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9760\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2954 [0/90000 (0%)]\tlog Loss: -18.157858\n",
      "\n",
      "Train set: Average log loss: -18.0817\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9760\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2955 [0/90000 (0%)]\tlog Loss: -18.157843\n",
      "\n",
      "Train set: Average log loss: -18.0817\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9762\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2956 [0/90000 (0%)]\tlog Loss: -18.157998\n",
      "\n",
      "Train set: Average log loss: -18.0818\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9762\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2957 [0/90000 (0%)]\tlog Loss: -18.158046\n",
      "\n",
      "Train set: Average log loss: -18.0819\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9764\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2958 [0/90000 (0%)]\tlog Loss: -18.158172\n",
      "\n",
      "Train set: Average log loss: -18.0820\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9766\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2959 [0/90000 (0%)]\tlog Loss: -18.158436\n",
      "\n",
      "Train set: Average log loss: -18.0820\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9767\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2960 [0/90000 (0%)]\tlog Loss: -18.158485\n",
      "\n",
      "Train set: Average log loss: -18.0821\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9767\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2961 [0/90000 (0%)]\tlog Loss: -18.158607\n",
      "\n",
      "Train set: Average log loss: -18.0822\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9768\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2962 [0/90000 (0%)]\tlog Loss: -18.158492\n",
      "\n",
      "Train set: Average log loss: -18.0823\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9769\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2963 [0/90000 (0%)]\tlog Loss: -18.158598\n",
      "\n",
      "Train set: Average log loss: -18.0823\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9770\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2964 [0/90000 (0%)]\tlog Loss: -18.158783\n",
      "\n",
      "Train set: Average log loss: -18.0824\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9770\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2965 [0/90000 (0%)]\tlog Loss: -18.158735\n",
      "\n",
      "Train set: Average log loss: -18.0825\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9771\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2966 [0/90000 (0%)]\tlog Loss: -18.159015\n",
      "\n",
      "Train set: Average log loss: -18.0825\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9772\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2967 [0/90000 (0%)]\tlog Loss: -18.158952\n",
      "\n",
      "Train set: Average log loss: -18.0826\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9773\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2968 [0/90000 (0%)]\tlog Loss: -18.159225\n",
      "\n",
      "Train set: Average log loss: -18.0827\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9773\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2969 [0/90000 (0%)]\tlog Loss: -18.159104\n",
      "\n",
      "Train set: Average log loss: -18.0828\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9774\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2970 [0/90000 (0%)]\tlog Loss: -18.159425\n",
      "\n",
      "Train set: Average log loss: -18.0828\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9775\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2971 [0/90000 (0%)]\tlog Loss: -18.159228\n",
      "\n",
      "Train set: Average log loss: -18.0829\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9775\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2972 [0/90000 (0%)]\tlog Loss: -18.159423\n",
      "\n",
      "Train set: Average log loss: -18.0830\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9776\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2973 [0/90000 (0%)]\tlog Loss: -18.159392\n",
      "\n",
      "Train set: Average log loss: -18.0830\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9776\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2974 [0/90000 (0%)]\tlog Loss: -18.159524\n",
      "\n",
      "Train set: Average log loss: -18.0831\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9776\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2975 [0/90000 (0%)]\tlog Loss: -18.159415\n",
      "\n",
      "Train set: Average log loss: -18.0832\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9777\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2976 [0/90000 (0%)]\tlog Loss: -18.159619\n",
      "\n",
      "Train set: Average log loss: -18.0832\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9777\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2977 [0/90000 (0%)]\tlog Loss: -18.159628\n",
      "\n",
      "Train set: Average log loss: -18.0833\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9778\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2978 [0/90000 (0%)]\tlog Loss: -18.159584\n",
      "\n",
      "Train set: Average log loss: -18.0834\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9779\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2979 [0/90000 (0%)]\tlog Loss: -18.159739\n",
      "\n",
      "Train set: Average log loss: -18.0835\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9780\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2980 [0/90000 (0%)]\tlog Loss: -18.159737\n",
      "\n",
      "Train set: Average log loss: -18.0836\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9780\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2981 [0/90000 (0%)]\tlog Loss: -18.159869\n",
      "\n",
      "Train set: Average log loss: -18.0836\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9781\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2982 [0/90000 (0%)]\tlog Loss: -18.159922\n",
      "\n",
      "Train set: Average log loss: -18.0837\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9781\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2983 [0/90000 (0%)]\tlog Loss: -18.159976\n",
      "\n",
      "Train set: Average log loss: -18.0838\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9782\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2984 [0/90000 (0%)]\tlog Loss: -18.159905\n",
      "\n",
      "Train set: Average log loss: -18.0838\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9783\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2985 [0/90000 (0%)]\tlog Loss: -18.160020\n",
      "\n",
      "Train set: Average log loss: -18.0839\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9782\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2986 [0/90000 (0%)]\tlog Loss: -18.160093\n",
      "\n",
      "Train set: Average log loss: -18.0840\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9784\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2987 [0/90000 (0%)]\tlog Loss: -18.160156\n",
      "\n",
      "Train set: Average log loss: -18.0841\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9784\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2988 [0/90000 (0%)]\tlog Loss: -18.160153\n",
      "\n",
      "Train set: Average log loss: -18.0842\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9786\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2989 [0/90000 (0%)]\tlog Loss: -18.160462\n",
      "\n",
      "Train set: Average log loss: -18.0842\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9787\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2990 [0/90000 (0%)]\tlog Loss: -18.160425\n",
      "\n",
      "Train set: Average log loss: -18.0843\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9786\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2991 [0/90000 (0%)]\tlog Loss: -18.160449\n",
      "\n",
      "Train set: Average log loss: -18.0843\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9788\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2992 [0/90000 (0%)]\tlog Loss: -18.160413\n",
      "\n",
      "Train set: Average log loss: -18.0844\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9789\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2993 [0/90000 (0%)]\tlog Loss: -18.160658\n",
      "\n",
      "Train set: Average log loss: -18.0845\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9789\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2994 [0/90000 (0%)]\tlog Loss: -18.160621\n",
      "\n",
      "Train set: Average log loss: -18.0846\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9790\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2995 [0/90000 (0%)]\tlog Loss: -18.160760\n",
      "\n",
      "Train set: Average log loss: -18.0847\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9790\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2996 [0/90000 (0%)]\tlog Loss: -18.160790\n",
      "\n",
      "Train set: Average log loss: -18.0847\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9792\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2997 [0/90000 (0%)]\tlog Loss: -18.160888\n",
      "\n",
      "Train set: Average log loss: -18.0848\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9792\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2998 [0/90000 (0%)]\tlog Loss: -18.161033\n",
      "\n",
      "Train set: Average log loss: -18.0849\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9793\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2999 [0/90000 (0%)]\tlog Loss: -18.161088\n",
      "\n",
      "Train set: Average log loss: -18.0850\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9794\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 3000 [0/90000 (0%)]\tlog Loss: -18.160898\n",
      "\n",
      "Train set: Average log loss: -18.0850\n",
      "\n",
      "\n",
      "Test set: Average loss: -17.9795\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "args =  {\"batch_size\": 1024,\n",
    "         \"test_batch_size\": 4048,\n",
    "         \"epochs\" : 3*10**3,\n",
    "         \"lr\": 1e-4,\n",
    "         \"gamma\": .1,\n",
    "         \"no_cuda\" : False,\n",
    "         \"run_dry\": False,\n",
    "         \"seed\": 0,\n",
    "         \"log_interval\" : 100,\n",
    "         \"dry_run\" : False,\n",
    "         \"save_model\": True}\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "\n",
    "# Loading Train / Test Data\n",
    "l_bounds = [.5, .3, .03, .02]\n",
    "u_bounds = [1.5, .95, .08, .9]\n",
    "bs_dataset = bs_LHS_data_generator(n = 10**5,l_bounds=l_bounds, u_bounds=u_bounds)\n",
    "train_size, test_size = int(bs_dataset.shape[0]*0.9), int(bs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(bs_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])\n",
    "\n",
    "\n",
    "\n",
    "print(device)\n",
    "model = BS_ANN().to(device)\n",
    "# Adam is found to be the best optimizer in the article\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"]) \n",
    "# Deacreses the Learning rate by .1 every 1000 epoch\n",
    "scheduler = StepLR(optimizer, step_size=10**3, gamma=args[\"gamma\"]) \n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "# Training loop\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "  train_loss = model.train_model(args, device, train_loader, optimizer, epoch)\n",
    "  test_loss = model.test_model(device, test_loader)\n",
    "  print(\"Current Learning rate {}\".format(scheduler.get_last_lr()[0]))\n",
    "  scheduler.step()\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "if args[\"save_model\"] :\n",
    "  torch.save(model.state_dict(), \"BS_ANN_narrow.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuG_4rSxqapw",
    "outputId": "0be5a5e9-2785-4fd0-9e9a-4e4d7e59f47e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BS_ANN(\n",
       "  (fc1): Linear(in_features=4, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc3): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc4): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc5): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model from saved parameters\n",
    "loaded_BS_ANN = BS_ANN().to(device)\n",
    "PATH = \"/content/BS_ANN_narrow.pt\"\n",
    "loaded_BS_ANN.load_state_dict(torch.load(PATH))\n",
    "loaded_BS_ANN.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "1MoBFR_Oqh4z",
    "outputId": "a79fae8d-a1b0-4f5c-d4e6-9d1274a9d2e8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX  0.0007414818\n",
      "MIN  -0.0007995409\n"
     ]
    }
   ],
   "source": [
    "# Plotting erros histogram\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_BS_ANN(data)\n",
    "    errors.append((target-output).data.cpu().numpy())\n",
    "  errors = np.concatenate(errors)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(errors,50,fill=False)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Difference\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "print(\"MAX \",errors.max())\n",
    "print(\"MIN \",errors.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkXiYbFpqiPu",
    "outputId": "1effb548-fd58-4967-9643-470f9d264256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Train scores----------------\n",
      "MSE 1.390402e-08\n",
      "MAE 8.870737e-05\n",
      "MAPE 8.374511e-02\n",
      "R2 9.999996e-01\n",
      "-----------Test scores----------------\n",
      "MSE 1.554532e-08\n",
      "MAE 9.355514e-05\n",
      "MAPE 2.907670e-01\n",
      "R2 9.999995e-01\n"
     ]
    }
   ],
   "source": [
    "# Printing train and test MSE, MAE, MAPE and R2\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_BS_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Train scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )\n",
    "\n",
    "\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_BS_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Test scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lgRjnSQwHNR",
    "outputId": "0f8812ec-ec88-40d9-833d-a4d6ce9d46d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012468087263088913"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(1.554532e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTZo9gF1GJwq"
   },
   "source": [
    "# 2) Implied volatility network: IV-ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGGuqkhpzjak"
   },
   "source": [
    "### Defining class IV-ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gNI200-zk2d"
   },
   "outputs": [],
   "source": [
    "class IV_ANN(BS_ANN):\n",
    "    # The authors of the paper seem to use\n",
    "    # the same architecture both for BS-ANN and the IV-ANN\n",
    "    # thus the class inherets from BS_ANN\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW3NslRmKsTU"
   },
   "source": [
    "### Data Generation with log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA__rZmyDv7m"
   },
   "source": [
    "Log transformation \n",
    "\n",
    "$\\tilde{V}=V_{t}-\\max \\left(S_{t}-K e^{-r \\tau}, 0\\right)$\n",
    "\n",
    "$\\left\\{\\log (\\tilde{V} / K), S_{0} / K, r, \\tau\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bc2wCF6_Kuij",
    "outputId": "1a052209-0e18-4129-a93e-e6b74681f9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ INPUT ------\n",
      "S/K.......... [0.50 1.40]\n",
      "Tau.......... [0.05 1.00]\n",
      "r............ [0.00 0.10]\n",
      "log(V/K)..... [-16.12 -0.98]\n",
      "------ OUTPUT ------\n",
      "Sigma........ [0.05 1.00]\n"
     ]
    }
   ],
   "source": [
    "def IV_LHS_data_generator(n = 10**6, l_bounds = [.5, .05, .0, .05], u_bounds = [1.4, 1, .1, 1], log_scale = True):\n",
    "  \"\"\"\n",
    "  Generates samples of call prices using Latin hypercube sampling.\n",
    "  Returns a torch float tensor of dimension (n,5) containig the inputs samples and the volatility as the target.\n",
    "  Performs a log transormation of V/K.\n",
    "  The inputs are: Moyeness (S/K), time to maturity (tau), Risk free rate (r), Volatility (sigma).\n",
    "  n: number of samples.\n",
    "  l_bounds: lower bound for the inputs.  \n",
    "  u_bounds: upper bound for the inputs.\n",
    "  log_scale: performs log transformation if True.\n",
    "\n",
    "  \"\"\"\n",
    "  sampler = qmc.LatinHypercube(d=4)\n",
    "  sample = sampler.random(n)\n",
    "  sample = qmc.scale(sample, l_bounds, u_bounds)\n",
    "  bs_prices = bs_moyeness_call(sample[:,0], sample[:,1], sample[:,2], sample[:,3]).reshape((-1,1))\n",
    "\n",
    "\n",
    "  bs_dataset = np.concatenate((sample[:,:-1], bs_prices, sample[:,3].reshape((n,1))),axis=1)\n",
    "  # log transformation as in section 4.3\n",
    "  if log_scale : \n",
    "    bs_dataset[:,3] = bs_dataset[:,3] - np.maximum(bs_dataset[:,0]-np.exp(-bs_dataset[:,1]*bs_dataset[:,2]),0)\n",
    "    indecies = np.argwhere(bs_dataset[:,3]< 1e-7)\n",
    "    bs_dataset = np.delete(bs_dataset, indecies,0)\n",
    "    bs_dataset[:,3] = np.log(bs_dataset[:,3])\n",
    "\n",
    "  print(\"------ INPUT ------\")\n",
    "  print(\"S/K.......... [{:.2f} {:.2f}]\".format(min(bs_dataset[:,0]),max(bs_dataset[:,0])))\n",
    "  print(\"Tau.......... [{:.2f} {:.2f}]\".format(min(bs_dataset[:,1]),max(bs_dataset[:,1])))\n",
    "  print(\"r............ [{:.2f} {:.2f}]\".format(min(bs_dataset[:,2]),max(bs_dataset[:,2])))\n",
    "  if log_scale : \n",
    "    print(\"log(V/K)..... [{:.2f} {:.2f}]\".format(min(bs_dataset[:,3]),max(bs_dataset[:,3])))\n",
    "  else : \n",
    "    print(\"V/K.......... [{:.2f} {:.2f}]\".format(min(bs_dataset[:,3]),max(bs_dataset[:,3])))\n",
    "  print(\"------ OUTPUT ------\")\n",
    "  print(\"Sigma........ [{:.2f} {:.2f}]\".format(min(bs_dataset[:,4]),max(bs_dataset[:,4])))\n",
    "  return torch.FloatTensor(bs_dataset)\n",
    "\n",
    "bs_dataset = IV_LHS_data_generator(log_scale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EImNgxzAbeI"
   },
   "source": [
    "### Training IV-ANN with log transofmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KtbEz_6AeDA",
    "outputId": "dde48922-e0c2-45f7-de60-64ff1cec2c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mLe flux de sortie a t tronqu et ne contient que les 5000dernires lignes.\u001b[0m\n",
      "Train Epoch: 2376 [0/85136 (0%)]\tlog Loss: -14.490517\n",
      "\n",
      "Train set: Average log loss: -14.5737\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2874\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2377 [0/85136 (0%)]\tlog Loss: -14.489299\n",
      "\n",
      "Train set: Average log loss: -14.5737\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2877\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2378 [0/85136 (0%)]\tlog Loss: -14.489865\n",
      "\n",
      "Train set: Average log loss: -14.5738\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2878\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2379 [0/85136 (0%)]\tlog Loss: -14.490235\n",
      "\n",
      "Train set: Average log loss: -14.5740\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2879\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2380 [0/85136 (0%)]\tlog Loss: -14.490352\n",
      "\n",
      "Train set: Average log loss: -14.5741\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2878\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2381 [0/85136 (0%)]\tlog Loss: -14.490228\n",
      "\n",
      "Train set: Average log loss: -14.5741\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2880\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2382 [0/85136 (0%)]\tlog Loss: -14.490507\n",
      "\n",
      "Train set: Average log loss: -14.5742\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2878\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2383 [0/85136 (0%)]\tlog Loss: -14.490343\n",
      "\n",
      "Train set: Average log loss: -14.5742\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2880\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2384 [0/85136 (0%)]\tlog Loss: -14.490377\n",
      "\n",
      "Train set: Average log loss: -14.5742\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2880\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2385 [0/85136 (0%)]\tlog Loss: -14.490465\n",
      "\n",
      "Train set: Average log loss: -14.5744\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2882\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2386 [0/85136 (0%)]\tlog Loss: -14.490278\n",
      "\n",
      "Train set: Average log loss: -14.5745\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2880\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2387 [0/85136 (0%)]\tlog Loss: -14.490771\n",
      "\n",
      "Train set: Average log loss: -14.5746\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2883\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2388 [0/85136 (0%)]\tlog Loss: -14.491042\n",
      "\n",
      "Train set: Average log loss: -14.5747\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2389 [0/85136 (0%)]\tlog Loss: -14.490982\n",
      "\n",
      "Train set: Average log loss: -14.5746\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2878\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2390 [0/85136 (0%)]\tlog Loss: -14.489882\n",
      "\n",
      "Train set: Average log loss: -14.5747\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2883\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2391 [0/85136 (0%)]\tlog Loss: -14.491326\n",
      "\n",
      "Train set: Average log loss: -14.5748\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2883\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2392 [0/85136 (0%)]\tlog Loss: -14.490466\n",
      "\n",
      "Train set: Average log loss: -14.5749\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2884\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2393 [0/85136 (0%)]\tlog Loss: -14.491004\n",
      "\n",
      "Train set: Average log loss: -14.5751\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2879\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2394 [0/85136 (0%)]\tlog Loss: -14.490456\n",
      "\n",
      "Train set: Average log loss: -14.5750\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2884\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2395 [0/85136 (0%)]\tlog Loss: -14.491039\n",
      "\n",
      "Train set: Average log loss: -14.5751\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2396 [0/85136 (0%)]\tlog Loss: -14.491028\n",
      "\n",
      "Train set: Average log loss: -14.5753\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2397 [0/85136 (0%)]\tlog Loss: -14.491288\n",
      "\n",
      "Train set: Average log loss: -14.5753\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2883\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2398 [0/85136 (0%)]\tlog Loss: -14.490809\n",
      "\n",
      "Train set: Average log loss: -14.5753\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2399 [0/85136 (0%)]\tlog Loss: -14.491447\n",
      "\n",
      "Train set: Average log loss: -14.5754\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2888\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2400 [0/85136 (0%)]\tlog Loss: -14.491212\n",
      "\n",
      "Train set: Average log loss: -14.5756\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2883\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2401 [0/85136 (0%)]\tlog Loss: -14.490709\n",
      "\n",
      "Train set: Average log loss: -14.5756\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2890\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2402 [0/85136 (0%)]\tlog Loss: -14.492028\n",
      "\n",
      "Train set: Average log loss: -14.5758\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2886\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2403 [0/85136 (0%)]\tlog Loss: -14.491616\n",
      "\n",
      "Train set: Average log loss: -14.5757\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2404 [0/85136 (0%)]\tlog Loss: -14.491025\n",
      "\n",
      "Train set: Average log loss: -14.5758\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2889\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2405 [0/85136 (0%)]\tlog Loss: -14.491797\n",
      "\n",
      "Train set: Average log loss: -14.5759\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2887\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2406 [0/85136 (0%)]\tlog Loss: -14.491586\n",
      "\n",
      "Train set: Average log loss: -14.5760\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2407 [0/85136 (0%)]\tlog Loss: -14.491333\n",
      "\n",
      "Train set: Average log loss: -14.5760\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2408 [0/85136 (0%)]\tlog Loss: -14.491566\n",
      "\n",
      "Train set: Average log loss: -14.5761\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2893\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2409 [0/85136 (0%)]\tlog Loss: -14.492842\n",
      "\n",
      "Train set: Average log loss: -14.5762\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2888\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2410 [0/85136 (0%)]\tlog Loss: -14.491697\n",
      "\n",
      "Train set: Average log loss: -14.5763\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2411 [0/85136 (0%)]\tlog Loss: -14.492306\n",
      "\n",
      "Train set: Average log loss: -14.5763\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2886\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2412 [0/85136 (0%)]\tlog Loss: -14.491257\n",
      "\n",
      "Train set: Average log loss: -14.5764\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2888\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2413 [0/85136 (0%)]\tlog Loss: -14.492241\n",
      "\n",
      "Train set: Average log loss: -14.5764\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2414 [0/85136 (0%)]\tlog Loss: -14.492229\n",
      "\n",
      "Train set: Average log loss: -14.5766\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2892\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2415 [0/85136 (0%)]\tlog Loss: -14.492562\n",
      "\n",
      "Train set: Average log loss: -14.5766\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2416 [0/85136 (0%)]\tlog Loss: -14.492150\n",
      "\n",
      "Train set: Average log loss: -14.5767\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2417 [0/85136 (0%)]\tlog Loss: -14.492369\n",
      "\n",
      "Train set: Average log loss: -14.5769\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2888\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2418 [0/85136 (0%)]\tlog Loss: -14.492028\n",
      "\n",
      "Train set: Average log loss: -14.5769\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2889\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2419 [0/85136 (0%)]\tlog Loss: -14.491864\n",
      "\n",
      "Train set: Average log loss: -14.5769\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2890\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2420 [0/85136 (0%)]\tlog Loss: -14.492291\n",
      "\n",
      "Train set: Average log loss: -14.5769\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2895\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2421 [0/85136 (0%)]\tlog Loss: -14.493050\n",
      "\n",
      "Train set: Average log loss: -14.5772\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2895\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2422 [0/85136 (0%)]\tlog Loss: -14.492945\n",
      "\n",
      "Train set: Average log loss: -14.5772\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2423 [0/85136 (0%)]\tlog Loss: -14.492089\n",
      "\n",
      "Train set: Average log loss: -14.5772\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2895\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2424 [0/85136 (0%)]\tlog Loss: -14.493047\n",
      "\n",
      "Train set: Average log loss: -14.5774\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2897\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2425 [0/85136 (0%)]\tlog Loss: -14.493320\n",
      "\n",
      "Train set: Average log loss: -14.5775\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2895\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2426 [0/85136 (0%)]\tlog Loss: -14.492816\n",
      "\n",
      "Train set: Average log loss: -14.5775\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2898\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2427 [0/85136 (0%)]\tlog Loss: -14.493199\n",
      "\n",
      "Train set: Average log loss: -14.5777\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2897\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2428 [0/85136 (0%)]\tlog Loss: -14.493537\n",
      "\n",
      "Train set: Average log loss: -14.5778\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2901\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2429 [0/85136 (0%)]\tlog Loss: -14.494020\n",
      "\n",
      "Train set: Average log loss: -14.5777\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2898\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2430 [0/85136 (0%)]\tlog Loss: -14.493354\n",
      "\n",
      "Train set: Average log loss: -14.5779\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2903\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2431 [0/85136 (0%)]\tlog Loss: -14.494401\n",
      "\n",
      "Train set: Average log loss: -14.5780\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2905\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2432 [0/85136 (0%)]\tlog Loss: -14.494605\n",
      "\n",
      "Train set: Average log loss: -14.5781\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2898\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2433 [0/85136 (0%)]\tlog Loss: -14.493481\n",
      "\n",
      "Train set: Average log loss: -14.5780\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2903\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2434 [0/85136 (0%)]\tlog Loss: -14.494250\n",
      "\n",
      "Train set: Average log loss: -14.5781\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2902\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2435 [0/85136 (0%)]\tlog Loss: -14.494203\n",
      "\n",
      "Train set: Average log loss: -14.5783\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2905\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2436 [0/85136 (0%)]\tlog Loss: -14.495060\n",
      "\n",
      "Train set: Average log loss: -14.5783\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2905\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2437 [0/85136 (0%)]\tlog Loss: -14.494607\n",
      "\n",
      "Train set: Average log loss: -14.5785\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2906\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2438 [0/85136 (0%)]\tlog Loss: -14.495092\n",
      "\n",
      "Train set: Average log loss: -14.5784\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2904\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2439 [0/85136 (0%)]\tlog Loss: -14.494649\n",
      "\n",
      "Train set: Average log loss: -14.5785\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2905\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2440 [0/85136 (0%)]\tlog Loss: -14.494684\n",
      "\n",
      "Train set: Average log loss: -14.5786\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2901\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2441 [0/85136 (0%)]\tlog Loss: -14.493955\n",
      "\n",
      "Train set: Average log loss: -14.5786\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2909\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2442 [0/85136 (0%)]\tlog Loss: -14.495518\n",
      "\n",
      "Train set: Average log loss: -14.5788\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2903\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2443 [0/85136 (0%)]\tlog Loss: -14.494667\n",
      "\n",
      "Train set: Average log loss: -14.5788\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2907\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2444 [0/85136 (0%)]\tlog Loss: -14.495235\n",
      "\n",
      "Train set: Average log loss: -14.5790\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2911\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2445 [0/85136 (0%)]\tlog Loss: -14.495883\n",
      "\n",
      "Train set: Average log loss: -14.5790\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2908\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2446 [0/85136 (0%)]\tlog Loss: -14.495385\n",
      "\n",
      "Train set: Average log loss: -14.5790\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2904\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2447 [0/85136 (0%)]\tlog Loss: -14.494617\n",
      "\n",
      "Train set: Average log loss: -14.5790\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2909\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2448 [0/85136 (0%)]\tlog Loss: -14.495322\n",
      "\n",
      "Train set: Average log loss: -14.5791\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2907\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2449 [0/85136 (0%)]\tlog Loss: -14.494927\n",
      "\n",
      "Train set: Average log loss: -14.5793\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2912\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2450 [0/85136 (0%)]\tlog Loss: -14.496366\n",
      "\n",
      "Train set: Average log loss: -14.5794\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2911\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2451 [0/85136 (0%)]\tlog Loss: -14.495864\n",
      "\n",
      "Train set: Average log loss: -14.5793\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2906\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2452 [0/85136 (0%)]\tlog Loss: -14.494988\n",
      "\n",
      "Train set: Average log loss: -14.5795\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2912\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2453 [0/85136 (0%)]\tlog Loss: -14.496479\n",
      "\n",
      "Train set: Average log loss: -14.5795\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2913\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2454 [0/85136 (0%)]\tlog Loss: -14.496024\n",
      "\n",
      "Train set: Average log loss: -14.5796\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2913\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2455 [0/85136 (0%)]\tlog Loss: -14.496191\n",
      "\n",
      "Train set: Average log loss: -14.5796\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2909\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2456 [0/85136 (0%)]\tlog Loss: -14.495440\n",
      "\n",
      "Train set: Average log loss: -14.5798\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2916\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2457 [0/85136 (0%)]\tlog Loss: -14.497041\n",
      "\n",
      "Train set: Average log loss: -14.5799\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2913\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2458 [0/85136 (0%)]\tlog Loss: -14.496110\n",
      "\n",
      "Train set: Average log loss: -14.5799\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2911\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2459 [0/85136 (0%)]\tlog Loss: -14.495841\n",
      "\n",
      "Train set: Average log loss: -14.5798\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2912\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2460 [0/85136 (0%)]\tlog Loss: -14.495868\n",
      "\n",
      "Train set: Average log loss: -14.5802\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2914\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2461 [0/85136 (0%)]\tlog Loss: -14.496539\n",
      "\n",
      "Train set: Average log loss: -14.5801\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2914\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2462 [0/85136 (0%)]\tlog Loss: -14.496500\n",
      "\n",
      "Train set: Average log loss: -14.5802\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2913\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2463 [0/85136 (0%)]\tlog Loss: -14.496279\n",
      "\n",
      "Train set: Average log loss: -14.5803\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2915\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2464 [0/85136 (0%)]\tlog Loss: -14.497121\n",
      "\n",
      "Train set: Average log loss: -14.5803\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2916\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2465 [0/85136 (0%)]\tlog Loss: -14.496820\n",
      "\n",
      "Train set: Average log loss: -14.5805\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2919\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2466 [0/85136 (0%)]\tlog Loss: -14.497450\n",
      "\n",
      "Train set: Average log loss: -14.5805\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2915\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2467 [0/85136 (0%)]\tlog Loss: -14.496488\n",
      "\n",
      "Train set: Average log loss: -14.5805\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2920\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2468 [0/85136 (0%)]\tlog Loss: -14.497154\n",
      "\n",
      "Train set: Average log loss: -14.5807\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2916\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2469 [0/85136 (0%)]\tlog Loss: -14.497062\n",
      "\n",
      "Train set: Average log loss: -14.5808\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2918\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2470 [0/85136 (0%)]\tlog Loss: -14.497133\n",
      "\n",
      "Train set: Average log loss: -14.5808\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2916\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2471 [0/85136 (0%)]\tlog Loss: -14.497025\n",
      "\n",
      "Train set: Average log loss: -14.5808\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2922\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2472 [0/85136 (0%)]\tlog Loss: -14.497912\n",
      "\n",
      "Train set: Average log loss: -14.5809\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2919\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2473 [0/85136 (0%)]\tlog Loss: -14.497229\n",
      "\n",
      "Train set: Average log loss: -14.5811\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2921\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2474 [0/85136 (0%)]\tlog Loss: -14.497719\n",
      "\n",
      "Train set: Average log loss: -14.5812\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2922\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2475 [0/85136 (0%)]\tlog Loss: -14.497768\n",
      "\n",
      "Train set: Average log loss: -14.5813\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2923\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2476 [0/85136 (0%)]\tlog Loss: -14.498005\n",
      "\n",
      "Train set: Average log loss: -14.5813\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2922\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2477 [0/85136 (0%)]\tlog Loss: -14.498001\n",
      "\n",
      "Train set: Average log loss: -14.5815\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2926\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2478 [0/85136 (0%)]\tlog Loss: -14.498523\n",
      "\n",
      "Train set: Average log loss: -14.5814\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2926\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2479 [0/85136 (0%)]\tlog Loss: -14.498124\n",
      "\n",
      "Train set: Average log loss: -14.5816\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2926\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2480 [0/85136 (0%)]\tlog Loss: -14.498337\n",
      "\n",
      "Train set: Average log loss: -14.5817\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2924\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2481 [0/85136 (0%)]\tlog Loss: -14.498236\n",
      "\n",
      "Train set: Average log loss: -14.5817\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2929\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2482 [0/85136 (0%)]\tlog Loss: -14.499063\n",
      "\n",
      "Train set: Average log loss: -14.5817\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2928\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2483 [0/85136 (0%)]\tlog Loss: -14.498529\n",
      "\n",
      "Train set: Average log loss: -14.5821\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2484 [0/85136 (0%)]\tlog Loss: -14.499290\n",
      "\n",
      "Train set: Average log loss: -14.5820\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2485 [0/85136 (0%)]\tlog Loss: -14.498899\n",
      "\n",
      "Train set: Average log loss: -14.5820\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2486 [0/85136 (0%)]\tlog Loss: -14.499978\n",
      "\n",
      "Train set: Average log loss: -14.5822\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2932\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2487 [0/85136 (0%)]\tlog Loss: -14.499069\n",
      "\n",
      "Train set: Average log loss: -14.5822\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2488 [0/85136 (0%)]\tlog Loss: -14.499178\n",
      "\n",
      "Train set: Average log loss: -14.5821\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2932\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2489 [0/85136 (0%)]\tlog Loss: -14.499465\n",
      "\n",
      "Train set: Average log loss: -14.5823\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2931\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2490 [0/85136 (0%)]\tlog Loss: -14.499200\n",
      "\n",
      "Train set: Average log loss: -14.5825\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2933\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2491 [0/85136 (0%)]\tlog Loss: -14.499513\n",
      "\n",
      "Train set: Average log loss: -14.5825\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2492 [0/85136 (0%)]\tlog Loss: -14.499326\n",
      "\n",
      "Train set: Average log loss: -14.5825\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2493 [0/85136 (0%)]\tlog Loss: -14.499981\n",
      "\n",
      "Train set: Average log loss: -14.5827\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2494 [0/85136 (0%)]\tlog Loss: -14.499150\n",
      "\n",
      "Train set: Average log loss: -14.5827\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2495 [0/85136 (0%)]\tlog Loss: -14.500289\n",
      "\n",
      "Train set: Average log loss: -14.5827\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2496 [0/85136 (0%)]\tlog Loss: -14.499791\n",
      "\n",
      "Train set: Average log loss: -14.5829\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2497 [0/85136 (0%)]\tlog Loss: -14.500062\n",
      "\n",
      "Train set: Average log loss: -14.5829\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2498 [0/85136 (0%)]\tlog Loss: -14.499734\n",
      "\n",
      "Train set: Average log loss: -14.5830\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2499 [0/85136 (0%)]\tlog Loss: -14.500692\n",
      "\n",
      "Train set: Average log loss: -14.5832\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2500 [0/85136 (0%)]\tlog Loss: -14.500040\n",
      "\n",
      "Train set: Average log loss: -14.5830\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2501 [0/85136 (0%)]\tlog Loss: -14.500316\n",
      "\n",
      "Train set: Average log loss: -14.5833\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2502 [0/85136 (0%)]\tlog Loss: -14.500517\n",
      "\n",
      "Train set: Average log loss: -14.5834\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2503 [0/85136 (0%)]\tlog Loss: -14.500023\n",
      "\n",
      "Train set: Average log loss: -14.5834\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2504 [0/85136 (0%)]\tlog Loss: -14.500208\n",
      "\n",
      "Train set: Average log loss: -14.5834\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2505 [0/85136 (0%)]\tlog Loss: -14.500403\n",
      "\n",
      "Train set: Average log loss: -14.5835\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2506 [0/85136 (0%)]\tlog Loss: -14.499702\n",
      "\n",
      "Train set: Average log loss: -14.5836\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2940\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2507 [0/85136 (0%)]\tlog Loss: -14.500872\n",
      "\n",
      "Train set: Average log loss: -14.5836\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2508 [0/85136 (0%)]\tlog Loss: -14.500838\n",
      "\n",
      "Train set: Average log loss: -14.5838\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2940\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2509 [0/85136 (0%)]\tlog Loss: -14.500819\n",
      "\n",
      "Train set: Average log loss: -14.5839\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2510 [0/85136 (0%)]\tlog Loss: -14.501189\n",
      "\n",
      "Train set: Average log loss: -14.5839\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2511 [0/85136 (0%)]\tlog Loss: -14.499717\n",
      "\n",
      "Train set: Average log loss: -14.5839\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2512 [0/85136 (0%)]\tlog Loss: -14.500175\n",
      "\n",
      "Train set: Average log loss: -14.5841\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2513 [0/85136 (0%)]\tlog Loss: -14.501477\n",
      "\n",
      "Train set: Average log loss: -14.5842\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2514 [0/85136 (0%)]\tlog Loss: -14.501494\n",
      "\n",
      "Train set: Average log loss: -14.5843\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2515 [0/85136 (0%)]\tlog Loss: -14.501357\n",
      "\n",
      "Train set: Average log loss: -14.5843\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2516 [0/85136 (0%)]\tlog Loss: -14.500837\n",
      "\n",
      "Train set: Average log loss: -14.5844\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2517 [0/85136 (0%)]\tlog Loss: -14.501580\n",
      "\n",
      "Train set: Average log loss: -14.5845\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2518 [0/85136 (0%)]\tlog Loss: -14.501543\n",
      "\n",
      "Train set: Average log loss: -14.5845\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2519 [0/85136 (0%)]\tlog Loss: -14.501432\n",
      "\n",
      "Train set: Average log loss: -14.5846\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2520 [0/85136 (0%)]\tlog Loss: -14.501661\n",
      "\n",
      "Train set: Average log loss: -14.5847\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2521 [0/85136 (0%)]\tlog Loss: -14.500973\n",
      "\n",
      "Train set: Average log loss: -14.5847\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2950\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2522 [0/85136 (0%)]\tlog Loss: -14.502288\n",
      "\n",
      "Train set: Average log loss: -14.5848\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2523 [0/85136 (0%)]\tlog Loss: -14.501080\n",
      "\n",
      "Train set: Average log loss: -14.5849\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2524 [0/85136 (0%)]\tlog Loss: -14.503063\n",
      "\n",
      "Train set: Average log loss: -14.5850\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2525 [0/85136 (0%)]\tlog Loss: -14.501167\n",
      "\n",
      "Train set: Average log loss: -14.5850\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2526 [0/85136 (0%)]\tlog Loss: -14.502354\n",
      "\n",
      "Train set: Average log loss: -14.5851\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2527 [0/85136 (0%)]\tlog Loss: -14.501649\n",
      "\n",
      "Train set: Average log loss: -14.5851\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2528 [0/85136 (0%)]\tlog Loss: -14.501588\n",
      "\n",
      "Train set: Average log loss: -14.5852\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2529 [0/85136 (0%)]\tlog Loss: -14.503272\n",
      "\n",
      "Train set: Average log loss: -14.5853\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2530 [0/85136 (0%)]\tlog Loss: -14.501899\n",
      "\n",
      "Train set: Average log loss: -14.5853\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2531 [0/85136 (0%)]\tlog Loss: -14.501506\n",
      "\n",
      "Train set: Average log loss: -14.5854\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2532 [0/85136 (0%)]\tlog Loss: -14.501636\n",
      "\n",
      "Train set: Average log loss: -14.5855\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2533 [0/85136 (0%)]\tlog Loss: -14.502767\n",
      "\n",
      "Train set: Average log loss: -14.5856\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2534 [0/85136 (0%)]\tlog Loss: -14.502502\n",
      "\n",
      "Train set: Average log loss: -14.5858\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2535 [0/85136 (0%)]\tlog Loss: -14.502423\n",
      "\n",
      "Train set: Average log loss: -14.5857\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2954\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2536 [0/85136 (0%)]\tlog Loss: -14.502956\n",
      "\n",
      "Train set: Average log loss: -14.5859\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2537 [0/85136 (0%)]\tlog Loss: -14.502050\n",
      "\n",
      "Train set: Average log loss: -14.5859\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2538 [0/85136 (0%)]\tlog Loss: -14.502705\n",
      "\n",
      "Train set: Average log loss: -14.5860\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2539 [0/85136 (0%)]\tlog Loss: -14.502265\n",
      "\n",
      "Train set: Average log loss: -14.5861\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2540 [0/85136 (0%)]\tlog Loss: -14.503123\n",
      "\n",
      "Train set: Average log loss: -14.5861\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2954\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2541 [0/85136 (0%)]\tlog Loss: -14.503023\n",
      "\n",
      "Train set: Average log loss: -14.5862\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2542 [0/85136 (0%)]\tlog Loss: -14.502298\n",
      "\n",
      "Train set: Average log loss: -14.5863\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2543 [0/85136 (0%)]\tlog Loss: -14.503504\n",
      "\n",
      "Train set: Average log loss: -14.5863\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2958\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2544 [0/85136 (0%)]\tlog Loss: -14.503367\n",
      "\n",
      "Train set: Average log loss: -14.5864\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2545 [0/85136 (0%)]\tlog Loss: -14.503519\n",
      "\n",
      "Train set: Average log loss: -14.5865\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2546 [0/85136 (0%)]\tlog Loss: -14.502790\n",
      "\n",
      "Train set: Average log loss: -14.5866\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2956\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2547 [0/85136 (0%)]\tlog Loss: -14.503795\n",
      "\n",
      "Train set: Average log loss: -14.5866\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2548 [0/85136 (0%)]\tlog Loss: -14.503562\n",
      "\n",
      "Train set: Average log loss: -14.5868\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2958\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2549 [0/85136 (0%)]\tlog Loss: -14.503558\n",
      "\n",
      "Train set: Average log loss: -14.5868\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2956\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2550 [0/85136 (0%)]\tlog Loss: -14.503456\n",
      "\n",
      "Train set: Average log loss: -14.5868\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2551 [0/85136 (0%)]\tlog Loss: -14.503071\n",
      "\n",
      "Train set: Average log loss: -14.5870\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2552 [0/85136 (0%)]\tlog Loss: -14.504102\n",
      "\n",
      "Train set: Average log loss: -14.5871\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2553 [0/85136 (0%)]\tlog Loss: -14.504290\n",
      "\n",
      "Train set: Average log loss: -14.5871\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2554 [0/85136 (0%)]\tlog Loss: -14.503991\n",
      "\n",
      "Train set: Average log loss: -14.5871\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2555 [0/85136 (0%)]\tlog Loss: -14.504154\n",
      "\n",
      "Train set: Average log loss: -14.5874\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2556 [0/85136 (0%)]\tlog Loss: -14.504282\n",
      "\n",
      "Train set: Average log loss: -14.5874\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2557 [0/85136 (0%)]\tlog Loss: -14.503545\n",
      "\n",
      "Train set: Average log loss: -14.5873\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2558 [0/85136 (0%)]\tlog Loss: -14.504430\n",
      "\n",
      "Train set: Average log loss: -14.5874\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2559 [0/85136 (0%)]\tlog Loss: -14.504622\n",
      "\n",
      "Train set: Average log loss: -14.5877\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2560 [0/85136 (0%)]\tlog Loss: -14.504122\n",
      "\n",
      "Train set: Average log loss: -14.5876\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2964\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2561 [0/85136 (0%)]\tlog Loss: -14.504906\n",
      "\n",
      "Train set: Average log loss: -14.5877\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2562 [0/85136 (0%)]\tlog Loss: -14.505231\n",
      "\n",
      "Train set: Average log loss: -14.5878\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2563 [0/85136 (0%)]\tlog Loss: -14.504835\n",
      "\n",
      "Train set: Average log loss: -14.5879\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2564 [0/85136 (0%)]\tlog Loss: -14.504701\n",
      "\n",
      "Train set: Average log loss: -14.5879\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2565 [0/85136 (0%)]\tlog Loss: -14.505422\n",
      "\n",
      "Train set: Average log loss: -14.5880\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2566 [0/85136 (0%)]\tlog Loss: -14.505539\n",
      "\n",
      "Train set: Average log loss: -14.5881\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2567 [0/85136 (0%)]\tlog Loss: -14.504643\n",
      "\n",
      "Train set: Average log loss: -14.5881\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2568 [0/85136 (0%)]\tlog Loss: -14.505051\n",
      "\n",
      "Train set: Average log loss: -14.5883\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2569 [0/85136 (0%)]\tlog Loss: -14.505562\n",
      "\n",
      "Train set: Average log loss: -14.5883\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2570 [0/85136 (0%)]\tlog Loss: -14.505813\n",
      "\n",
      "Train set: Average log loss: -14.5885\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2571 [0/85136 (0%)]\tlog Loss: -14.505199\n",
      "\n",
      "Train set: Average log loss: -14.5884\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2572 [0/85136 (0%)]\tlog Loss: -14.505967\n",
      "\n",
      "Train set: Average log loss: -14.5885\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2573 [0/85136 (0%)]\tlog Loss: -14.505514\n",
      "\n",
      "Train set: Average log loss: -14.5885\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2574 [0/85136 (0%)]\tlog Loss: -14.505749\n",
      "\n",
      "Train set: Average log loss: -14.5887\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2575 [0/85136 (0%)]\tlog Loss: -14.505387\n",
      "\n",
      "Train set: Average log loss: -14.5888\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2576 [0/85136 (0%)]\tlog Loss: -14.506317\n",
      "\n",
      "Train set: Average log loss: -14.5888\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2577 [0/85136 (0%)]\tlog Loss: -14.506239\n",
      "\n",
      "Train set: Average log loss: -14.5889\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2578 [0/85136 (0%)]\tlog Loss: -14.505658\n",
      "\n",
      "Train set: Average log loss: -14.5889\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2579 [0/85136 (0%)]\tlog Loss: -14.506110\n",
      "\n",
      "Train set: Average log loss: -14.5890\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2580 [0/85136 (0%)]\tlog Loss: -14.506873\n",
      "\n",
      "Train set: Average log loss: -14.5892\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2581 [0/85136 (0%)]\tlog Loss: -14.506424\n",
      "\n",
      "Train set: Average log loss: -14.5893\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2582 [0/85136 (0%)]\tlog Loss: -14.506175\n",
      "\n",
      "Train set: Average log loss: -14.5892\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2583 [0/85136 (0%)]\tlog Loss: -14.506513\n",
      "\n",
      "Train set: Average log loss: -14.5893\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2584 [0/85136 (0%)]\tlog Loss: -14.506978\n",
      "\n",
      "Train set: Average log loss: -14.5894\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2585 [0/85136 (0%)]\tlog Loss: -14.506571\n",
      "\n",
      "Train set: Average log loss: -14.5895\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2586 [0/85136 (0%)]\tlog Loss: -14.506071\n",
      "\n",
      "Train set: Average log loss: -14.5896\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2587 [0/85136 (0%)]\tlog Loss: -14.506875\n",
      "\n",
      "Train set: Average log loss: -14.5896\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2588 [0/85136 (0%)]\tlog Loss: -14.506869\n",
      "\n",
      "Train set: Average log loss: -14.5898\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2589 [0/85136 (0%)]\tlog Loss: -14.506890\n",
      "\n",
      "Train set: Average log loss: -14.5897\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2590 [0/85136 (0%)]\tlog Loss: -14.507302\n",
      "\n",
      "Train set: Average log loss: -14.5898\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2591 [0/85136 (0%)]\tlog Loss: -14.507062\n",
      "\n",
      "Train set: Average log loss: -14.5900\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2592 [0/85136 (0%)]\tlog Loss: -14.507091\n",
      "\n",
      "Train set: Average log loss: -14.5900\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2593 [0/85136 (0%)]\tlog Loss: -14.507417\n",
      "\n",
      "Train set: Average log loss: -14.5902\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2594 [0/85136 (0%)]\tlog Loss: -14.507400\n",
      "\n",
      "Train set: Average log loss: -14.5903\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2595 [0/85136 (0%)]\tlog Loss: -14.507331\n",
      "\n",
      "Train set: Average log loss: -14.5903\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2596 [0/85136 (0%)]\tlog Loss: -14.508512\n",
      "\n",
      "Train set: Average log loss: -14.5904\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2597 [0/85136 (0%)]\tlog Loss: -14.508544\n",
      "\n",
      "Train set: Average log loss: -14.5904\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2983\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2598 [0/85136 (0%)]\tlog Loss: -14.508049\n",
      "\n",
      "Train set: Average log loss: -14.5906\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2599 [0/85136 (0%)]\tlog Loss: -14.508625\n",
      "\n",
      "Train set: Average log loss: -14.5905\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2600 [0/85136 (0%)]\tlog Loss: -14.507485\n",
      "\n",
      "Train set: Average log loss: -14.5907\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2601 [0/85136 (0%)]\tlog Loss: -14.509090\n",
      "\n",
      "Train set: Average log loss: -14.5907\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2602 [0/85136 (0%)]\tlog Loss: -14.508138\n",
      "\n",
      "Train set: Average log loss: -14.5908\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2603 [0/85136 (0%)]\tlog Loss: -14.508858\n",
      "\n",
      "Train set: Average log loss: -14.5908\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2604 [0/85136 (0%)]\tlog Loss: -14.508949\n",
      "\n",
      "Train set: Average log loss: -14.5909\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2605 [0/85136 (0%)]\tlog Loss: -14.508823\n",
      "\n",
      "Train set: Average log loss: -14.5909\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2606 [0/85136 (0%)]\tlog Loss: -14.508944\n",
      "\n",
      "Train set: Average log loss: -14.5911\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2607 [0/85136 (0%)]\tlog Loss: -14.509371\n",
      "\n",
      "Train set: Average log loss: -14.5912\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2608 [0/85136 (0%)]\tlog Loss: -14.508643\n",
      "\n",
      "Train set: Average log loss: -14.5912\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2609 [0/85136 (0%)]\tlog Loss: -14.509865\n",
      "\n",
      "Train set: Average log loss: -14.5914\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2610 [0/85136 (0%)]\tlog Loss: -14.508958\n",
      "\n",
      "Train set: Average log loss: -14.5913\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2611 [0/85136 (0%)]\tlog Loss: -14.509276\n",
      "\n",
      "Train set: Average log loss: -14.5915\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2612 [0/85136 (0%)]\tlog Loss: -14.509305\n",
      "\n",
      "Train set: Average log loss: -14.5915\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2613 [0/85136 (0%)]\tlog Loss: -14.508974\n",
      "\n",
      "Train set: Average log loss: -14.5916\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2614 [0/85136 (0%)]\tlog Loss: -14.510859\n",
      "\n",
      "Train set: Average log loss: -14.5918\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2615 [0/85136 (0%)]\tlog Loss: -14.509941\n",
      "\n",
      "Train set: Average log loss: -14.5918\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2616 [0/85136 (0%)]\tlog Loss: -14.509322\n",
      "\n",
      "Train set: Average log loss: -14.5918\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2617 [0/85136 (0%)]\tlog Loss: -14.510468\n",
      "\n",
      "Train set: Average log loss: -14.5919\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2618 [0/85136 (0%)]\tlog Loss: -14.509688\n",
      "\n",
      "Train set: Average log loss: -14.5920\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2619 [0/85136 (0%)]\tlog Loss: -14.510246\n",
      "\n",
      "Train set: Average log loss: -14.5921\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2620 [0/85136 (0%)]\tlog Loss: -14.509841\n",
      "\n",
      "Train set: Average log loss: -14.5921\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2621 [0/85136 (0%)]\tlog Loss: -14.510377\n",
      "\n",
      "Train set: Average log loss: -14.5921\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2622 [0/85136 (0%)]\tlog Loss: -14.510714\n",
      "\n",
      "Train set: Average log loss: -14.5923\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2623 [0/85136 (0%)]\tlog Loss: -14.509940\n",
      "\n",
      "Train set: Average log loss: -14.5923\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2624 [0/85136 (0%)]\tlog Loss: -14.510640\n",
      "\n",
      "Train set: Average log loss: -14.5924\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2625 [0/85136 (0%)]\tlog Loss: -14.510959\n",
      "\n",
      "Train set: Average log loss: -14.5926\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2626 [0/85136 (0%)]\tlog Loss: -14.510611\n",
      "\n",
      "Train set: Average log loss: -14.5926\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2627 [0/85136 (0%)]\tlog Loss: -14.511270\n",
      "\n",
      "Train set: Average log loss: -14.5926\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.2999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2628 [0/85136 (0%)]\tlog Loss: -14.510395\n",
      "\n",
      "Train set: Average log loss: -14.5926\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2629 [0/85136 (0%)]\tlog Loss: -14.511114\n",
      "\n",
      "Train set: Average log loss: -14.5928\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2630 [0/85136 (0%)]\tlog Loss: -14.510998\n",
      "\n",
      "Train set: Average log loss: -14.5928\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2631 [0/85136 (0%)]\tlog Loss: -14.510956\n",
      "\n",
      "Train set: Average log loss: -14.5929\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2632 [0/85136 (0%)]\tlog Loss: -14.510717\n",
      "\n",
      "Train set: Average log loss: -14.5930\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2633 [0/85136 (0%)]\tlog Loss: -14.511674\n",
      "\n",
      "Train set: Average log loss: -14.5931\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2634 [0/85136 (0%)]\tlog Loss: -14.511866\n",
      "\n",
      "Train set: Average log loss: -14.5933\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2635 [0/85136 (0%)]\tlog Loss: -14.511150\n",
      "\n",
      "Train set: Average log loss: -14.5933\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3011\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2636 [0/85136 (0%)]\tlog Loss: -14.512447\n",
      "\n",
      "Train set: Average log loss: -14.5933\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2637 [0/85136 (0%)]\tlog Loss: -14.511565\n",
      "\n",
      "Train set: Average log loss: -14.5933\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2638 [0/85136 (0%)]\tlog Loss: -14.512643\n",
      "\n",
      "Train set: Average log loss: -14.5936\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2639 [0/85136 (0%)]\tlog Loss: -14.512284\n",
      "\n",
      "Train set: Average log loss: -14.5936\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2640 [0/85136 (0%)]\tlog Loss: -14.511904\n",
      "\n",
      "Train set: Average log loss: -14.5936\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2641 [0/85136 (0%)]\tlog Loss: -14.512756\n",
      "\n",
      "Train set: Average log loss: -14.5938\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2642 [0/85136 (0%)]\tlog Loss: -14.512445\n",
      "\n",
      "Train set: Average log loss: -14.5938\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2643 [0/85136 (0%)]\tlog Loss: -14.512553\n",
      "\n",
      "Train set: Average log loss: -14.5939\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2644 [0/85136 (0%)]\tlog Loss: -14.512406\n",
      "\n",
      "Train set: Average log loss: -14.5939\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3016\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2645 [0/85136 (0%)]\tlog Loss: -14.512976\n",
      "\n",
      "Train set: Average log loss: -14.5939\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2646 [0/85136 (0%)]\tlog Loss: -14.513202\n",
      "\n",
      "Train set: Average log loss: -14.5942\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2647 [0/85136 (0%)]\tlog Loss: -14.512971\n",
      "\n",
      "Train set: Average log loss: -14.5942\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2648 [0/85136 (0%)]\tlog Loss: -14.513142\n",
      "\n",
      "Train set: Average log loss: -14.5942\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3020\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2649 [0/85136 (0%)]\tlog Loss: -14.513384\n",
      "\n",
      "Train set: Average log loss: -14.5943\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2650 [0/85136 (0%)]\tlog Loss: -14.514233\n",
      "\n",
      "Train set: Average log loss: -14.5944\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3021\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2651 [0/85136 (0%)]\tlog Loss: -14.513554\n",
      "\n",
      "Train set: Average log loss: -14.5945\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2652 [0/85136 (0%)]\tlog Loss: -14.513463\n",
      "\n",
      "Train set: Average log loss: -14.5946\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2653 [0/85136 (0%)]\tlog Loss: -14.513670\n",
      "\n",
      "Train set: Average log loss: -14.5946\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2654 [0/85136 (0%)]\tlog Loss: -14.513825\n",
      "\n",
      "Train set: Average log loss: -14.5947\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2655 [0/85136 (0%)]\tlog Loss: -14.514178\n",
      "\n",
      "Train set: Average log loss: -14.5948\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2656 [0/85136 (0%)]\tlog Loss: -14.513678\n",
      "\n",
      "Train set: Average log loss: -14.5948\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2657 [0/85136 (0%)]\tlog Loss: -14.514186\n",
      "\n",
      "Train set: Average log loss: -14.5949\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2658 [0/85136 (0%)]\tlog Loss: -14.514224\n",
      "\n",
      "Train set: Average log loss: -14.5949\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2659 [0/85136 (0%)]\tlog Loss: -14.514174\n",
      "\n",
      "Train set: Average log loss: -14.5950\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2660 [0/85136 (0%)]\tlog Loss: -14.514730\n",
      "\n",
      "Train set: Average log loss: -14.5951\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2661 [0/85136 (0%)]\tlog Loss: -14.515186\n",
      "\n",
      "Train set: Average log loss: -14.5953\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2662 [0/85136 (0%)]\tlog Loss: -14.514145\n",
      "\n",
      "Train set: Average log loss: -14.5952\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2663 [0/85136 (0%)]\tlog Loss: -14.514823\n",
      "\n",
      "Train set: Average log loss: -14.5953\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2664 [0/85136 (0%)]\tlog Loss: -14.514417\n",
      "\n",
      "Train set: Average log loss: -14.5953\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2665 [0/85136 (0%)]\tlog Loss: -14.515269\n",
      "\n",
      "Train set: Average log loss: -14.5954\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2666 [0/85136 (0%)]\tlog Loss: -14.515176\n",
      "\n",
      "Train set: Average log loss: -14.5955\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2667 [0/85136 (0%)]\tlog Loss: -14.514902\n",
      "\n",
      "Train set: Average log loss: -14.5955\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2668 [0/85136 (0%)]\tlog Loss: -14.515160\n",
      "\n",
      "Train set: Average log loss: -14.5957\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2669 [0/85136 (0%)]\tlog Loss: -14.514930\n",
      "\n",
      "Train set: Average log loss: -14.5958\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2670 [0/85136 (0%)]\tlog Loss: -14.514670\n",
      "\n",
      "Train set: Average log loss: -14.5957\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2671 [0/85136 (0%)]\tlog Loss: -14.514998\n",
      "\n",
      "Train set: Average log loss: -14.5959\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2672 [0/85136 (0%)]\tlog Loss: -14.515554\n",
      "\n",
      "Train set: Average log loss: -14.5960\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2673 [0/85136 (0%)]\tlog Loss: -14.515156\n",
      "\n",
      "Train set: Average log loss: -14.5961\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2674 [0/85136 (0%)]\tlog Loss: -14.515946\n",
      "\n",
      "Train set: Average log loss: -14.5961\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2675 [0/85136 (0%)]\tlog Loss: -14.515835\n",
      "\n",
      "Train set: Average log loss: -14.5963\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2676 [0/85136 (0%)]\tlog Loss: -14.515696\n",
      "\n",
      "Train set: Average log loss: -14.5963\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2677 [0/85136 (0%)]\tlog Loss: -14.515567\n",
      "\n",
      "Train set: Average log loss: -14.5963\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2678 [0/85136 (0%)]\tlog Loss: -14.516941\n",
      "\n",
      "Train set: Average log loss: -14.5964\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2679 [0/85136 (0%)]\tlog Loss: -14.515709\n",
      "\n",
      "Train set: Average log loss: -14.5964\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2680 [0/85136 (0%)]\tlog Loss: -14.516603\n",
      "\n",
      "Train set: Average log loss: -14.5966\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2681 [0/85136 (0%)]\tlog Loss: -14.516114\n",
      "\n",
      "Train set: Average log loss: -14.5966\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2682 [0/85136 (0%)]\tlog Loss: -14.516390\n",
      "\n",
      "Train set: Average log loss: -14.5967\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2683 [0/85136 (0%)]\tlog Loss: -14.516840\n",
      "\n",
      "Train set: Average log loss: -14.5968\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2684 [0/85136 (0%)]\tlog Loss: -14.517184\n",
      "\n",
      "Train set: Average log loss: -14.5969\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2685 [0/85136 (0%)]\tlog Loss: -14.516267\n",
      "\n",
      "Train set: Average log loss: -14.5969\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2686 [0/85136 (0%)]\tlog Loss: -14.517135\n",
      "\n",
      "Train set: Average log loss: -14.5971\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2687 [0/85136 (0%)]\tlog Loss: -14.517212\n",
      "\n",
      "Train set: Average log loss: -14.5970\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2688 [0/85136 (0%)]\tlog Loss: -14.516680\n",
      "\n",
      "Train set: Average log loss: -14.5972\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2689 [0/85136 (0%)]\tlog Loss: -14.517363\n",
      "\n",
      "Train set: Average log loss: -14.5973\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2690 [0/85136 (0%)]\tlog Loss: -14.517277\n",
      "\n",
      "Train set: Average log loss: -14.5973\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2691 [0/85136 (0%)]\tlog Loss: -14.517710\n",
      "\n",
      "Train set: Average log loss: -14.5974\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2692 [0/85136 (0%)]\tlog Loss: -14.518194\n",
      "\n",
      "Train set: Average log loss: -14.5975\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2693 [0/85136 (0%)]\tlog Loss: -14.517772\n",
      "\n",
      "Train set: Average log loss: -14.5975\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2694 [0/85136 (0%)]\tlog Loss: -14.517826\n",
      "\n",
      "Train set: Average log loss: -14.5976\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3053\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2695 [0/85136 (0%)]\tlog Loss: -14.518954\n",
      "\n",
      "Train set: Average log loss: -14.5977\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2696 [0/85136 (0%)]\tlog Loss: -14.517865\n",
      "\n",
      "Train set: Average log loss: -14.5977\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2697 [0/85136 (0%)]\tlog Loss: -14.518757\n",
      "\n",
      "Train set: Average log loss: -14.5978\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3051\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2698 [0/85136 (0%)]\tlog Loss: -14.518624\n",
      "\n",
      "Train set: Average log loss: -14.5979\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3050\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2699 [0/85136 (0%)]\tlog Loss: -14.518211\n",
      "\n",
      "Train set: Average log loss: -14.5980\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2700 [0/85136 (0%)]\tlog Loss: -14.518661\n",
      "\n",
      "Train set: Average log loss: -14.5980\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3054\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2701 [0/85136 (0%)]\tlog Loss: -14.519272\n",
      "\n",
      "Train set: Average log loss: -14.5981\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2702 [0/85136 (0%)]\tlog Loss: -14.518447\n",
      "\n",
      "Train set: Average log loss: -14.5982\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3054\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2703 [0/85136 (0%)]\tlog Loss: -14.518781\n",
      "\n",
      "Train set: Average log loss: -14.5983\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3055\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2704 [0/85136 (0%)]\tlog Loss: -14.519408\n",
      "\n",
      "Train set: Average log loss: -14.5985\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3057\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2705 [0/85136 (0%)]\tlog Loss: -14.519349\n",
      "\n",
      "Train set: Average log loss: -14.5984\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2706 [0/85136 (0%)]\tlog Loss: -14.519238\n",
      "\n",
      "Train set: Average log loss: -14.5986\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3059\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2707 [0/85136 (0%)]\tlog Loss: -14.519673\n",
      "\n",
      "Train set: Average log loss: -14.5987\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3063\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2708 [0/85136 (0%)]\tlog Loss: -14.520151\n",
      "\n",
      "Train set: Average log loss: -14.5987\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3059\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2709 [0/85136 (0%)]\tlog Loss: -14.519692\n",
      "\n",
      "Train set: Average log loss: -14.5988\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3060\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2710 [0/85136 (0%)]\tlog Loss: -14.519605\n",
      "\n",
      "Train set: Average log loss: -14.5988\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3065\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2711 [0/85136 (0%)]\tlog Loss: -14.520587\n",
      "\n",
      "Train set: Average log loss: -14.5989\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3064\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2712 [0/85136 (0%)]\tlog Loss: -14.520172\n",
      "\n",
      "Train set: Average log loss: -14.5990\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3063\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2713 [0/85136 (0%)]\tlog Loss: -14.520136\n",
      "\n",
      "Train set: Average log loss: -14.5993\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3071\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2714 [0/85136 (0%)]\tlog Loss: -14.521526\n",
      "\n",
      "Train set: Average log loss: -14.5993\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3067\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2715 [0/85136 (0%)]\tlog Loss: -14.520784\n",
      "\n",
      "Train set: Average log loss: -14.5992\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3068\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2716 [0/85136 (0%)]\tlog Loss: -14.520870\n",
      "\n",
      "Train set: Average log loss: -14.5994\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3074\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2717 [0/85136 (0%)]\tlog Loss: -14.521580\n",
      "\n",
      "Train set: Average log loss: -14.5995\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3070\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2718 [0/85136 (0%)]\tlog Loss: -14.521007\n",
      "\n",
      "Train set: Average log loss: -14.5996\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3072\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2719 [0/85136 (0%)]\tlog Loss: -14.521161\n",
      "\n",
      "Train set: Average log loss: -14.5996\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3076\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2720 [0/85136 (0%)]\tlog Loss: -14.522020\n",
      "\n",
      "Train set: Average log loss: -14.5998\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2721 [0/85136 (0%)]\tlog Loss: -14.522278\n",
      "\n",
      "Train set: Average log loss: -14.5998\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3074\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2722 [0/85136 (0%)]\tlog Loss: -14.521453\n",
      "\n",
      "Train set: Average log loss: -14.5999\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2723 [0/85136 (0%)]\tlog Loss: -14.521996\n",
      "\n",
      "Train set: Average log loss: -14.6001\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3083\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2724 [0/85136 (0%)]\tlog Loss: -14.523052\n",
      "\n",
      "Train set: Average log loss: -14.6001\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3078\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2725 [0/85136 (0%)]\tlog Loss: -14.522067\n",
      "\n",
      "Train set: Average log loss: -14.6000\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2726 [0/85136 (0%)]\tlog Loss: -14.522760\n",
      "\n",
      "Train set: Average log loss: -14.6002\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2727 [0/85136 (0%)]\tlog Loss: -14.522824\n",
      "\n",
      "Train set: Average log loss: -14.6004\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2728 [0/85136 (0%)]\tlog Loss: -14.523551\n",
      "\n",
      "Train set: Average log loss: -14.6006\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2729 [0/85136 (0%)]\tlog Loss: -14.523671\n",
      "\n",
      "Train set: Average log loss: -14.6006\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3088\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2730 [0/85136 (0%)]\tlog Loss: -14.523319\n",
      "\n",
      "Train set: Average log loss: -14.6005\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3095\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2731 [0/85136 (0%)]\tlog Loss: -14.524375\n",
      "\n",
      "Train set: Average log loss: -14.6007\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3093\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2732 [0/85136 (0%)]\tlog Loss: -14.523904\n",
      "\n",
      "Train set: Average log loss: -14.6008\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3094\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2733 [0/85136 (0%)]\tlog Loss: -14.524167\n",
      "\n",
      "Train set: Average log loss: -14.6009\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3100\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2734 [0/85136 (0%)]\tlog Loss: -14.525456\n",
      "\n",
      "Train set: Average log loss: -14.6011\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3101\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2735 [0/85136 (0%)]\tlog Loss: -14.524936\n",
      "\n",
      "Train set: Average log loss: -14.6010\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3102\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2736 [0/85136 (0%)]\tlog Loss: -14.525221\n",
      "\n",
      "Train set: Average log loss: -14.6012\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3104\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2737 [0/85136 (0%)]\tlog Loss: -14.525189\n",
      "\n",
      "Train set: Average log loss: -14.6013\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3104\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2738 [0/85136 (0%)]\tlog Loss: -14.525337\n",
      "\n",
      "Train set: Average log loss: -14.6014\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3109\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2739 [0/85136 (0%)]\tlog Loss: -14.526136\n",
      "\n",
      "Train set: Average log loss: -14.6015\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3113\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2740 [0/85136 (0%)]\tlog Loss: -14.526790\n",
      "\n",
      "Train set: Average log loss: -14.6016\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3113\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2741 [0/85136 (0%)]\tlog Loss: -14.526675\n",
      "\n",
      "Train set: Average log loss: -14.6017\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3115\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2742 [0/85136 (0%)]\tlog Loss: -14.526700\n",
      "\n",
      "Train set: Average log loss: -14.6018\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3118\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2743 [0/85136 (0%)]\tlog Loss: -14.527369\n",
      "\n",
      "Train set: Average log loss: -14.6020\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3120\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2744 [0/85136 (0%)]\tlog Loss: -14.527489\n",
      "\n",
      "Train set: Average log loss: -14.6021\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3122\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2745 [0/85136 (0%)]\tlog Loss: -14.528008\n",
      "\n",
      "Train set: Average log loss: -14.6022\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3126\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2746 [0/85136 (0%)]\tlog Loss: -14.528519\n",
      "\n",
      "Train set: Average log loss: -14.6023\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3130\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2747 [0/85136 (0%)]\tlog Loss: -14.529097\n",
      "\n",
      "Train set: Average log loss: -14.6024\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3133\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2748 [0/85136 (0%)]\tlog Loss: -14.529583\n",
      "\n",
      "Train set: Average log loss: -14.6025\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3138\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2749 [0/85136 (0%)]\tlog Loss: -14.530142\n",
      "\n",
      "Train set: Average log loss: -14.6026\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3136\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2750 [0/85136 (0%)]\tlog Loss: -14.529686\n",
      "\n",
      "Train set: Average log loss: -14.6027\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3139\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2751 [0/85136 (0%)]\tlog Loss: -14.530074\n",
      "\n",
      "Train set: Average log loss: -14.6028\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3147\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2752 [0/85136 (0%)]\tlog Loss: -14.531491\n",
      "\n",
      "Train set: Average log loss: -14.6031\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3149\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2753 [0/85136 (0%)]\tlog Loss: -14.531162\n",
      "\n",
      "Train set: Average log loss: -14.6031\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3150\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2754 [0/85136 (0%)]\tlog Loss: -14.531557\n",
      "\n",
      "Train set: Average log loss: -14.6032\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3154\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2755 [0/85136 (0%)]\tlog Loss: -14.532259\n",
      "\n",
      "Train set: Average log loss: -14.6033\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3160\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2756 [0/85136 (0%)]\tlog Loss: -14.532975\n",
      "\n",
      "Train set: Average log loss: -14.6035\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3159\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2757 [0/85136 (0%)]\tlog Loss: -14.532688\n",
      "\n",
      "Train set: Average log loss: -14.6036\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3164\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2758 [0/85136 (0%)]\tlog Loss: -14.533799\n",
      "\n",
      "Train set: Average log loss: -14.6038\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3172\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2759 [0/85136 (0%)]\tlog Loss: -14.534964\n",
      "\n",
      "Train set: Average log loss: -14.6039\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3172\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2760 [0/85136 (0%)]\tlog Loss: -14.534465\n",
      "\n",
      "Train set: Average log loss: -14.6039\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3177\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2761 [0/85136 (0%)]\tlog Loss: -14.535143\n",
      "\n",
      "Train set: Average log loss: -14.6042\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3182\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2762 [0/85136 (0%)]\tlog Loss: -14.535873\n",
      "\n",
      "Train set: Average log loss: -14.6043\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3187\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2763 [0/85136 (0%)]\tlog Loss: -14.536403\n",
      "\n",
      "Train set: Average log loss: -14.6044\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3188\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2764 [0/85136 (0%)]\tlog Loss: -14.536988\n",
      "\n",
      "Train set: Average log loss: -14.6045\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3191\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2765 [0/85136 (0%)]\tlog Loss: -14.536979\n",
      "\n",
      "Train set: Average log loss: -14.6047\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3198\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2766 [0/85136 (0%)]\tlog Loss: -14.538205\n",
      "\n",
      "Train set: Average log loss: -14.6049\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3201\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2767 [0/85136 (0%)]\tlog Loss: -14.538420\n",
      "\n",
      "Train set: Average log loss: -14.6050\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3203\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2768 [0/85136 (0%)]\tlog Loss: -14.538613\n",
      "\n",
      "Train set: Average log loss: -14.6051\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3212\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2769 [0/85136 (0%)]\tlog Loss: -14.539956\n",
      "\n",
      "Train set: Average log loss: -14.6053\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3214\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2770 [0/85136 (0%)]\tlog Loss: -14.539971\n",
      "\n",
      "Train set: Average log loss: -14.6054\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3218\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2771 [0/85136 (0%)]\tlog Loss: -14.540412\n",
      "\n",
      "Train set: Average log loss: -14.6056\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3224\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2772 [0/85136 (0%)]\tlog Loss: -14.540971\n",
      "\n",
      "Train set: Average log loss: -14.6056\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3226\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2773 [0/85136 (0%)]\tlog Loss: -14.541559\n",
      "\n",
      "Train set: Average log loss: -14.6058\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3231\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2774 [0/85136 (0%)]\tlog Loss: -14.542267\n",
      "\n",
      "Train set: Average log loss: -14.6060\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3233\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2775 [0/85136 (0%)]\tlog Loss: -14.542515\n",
      "\n",
      "Train set: Average log loss: -14.6061\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3237\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2776 [0/85136 (0%)]\tlog Loss: -14.542967\n",
      "\n",
      "Train set: Average log loss: -14.6062\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3244\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2777 [0/85136 (0%)]\tlog Loss: -14.543742\n",
      "\n",
      "Train set: Average log loss: -14.6063\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3246\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2778 [0/85136 (0%)]\tlog Loss: -14.544003\n",
      "\n",
      "Train set: Average log loss: -14.6064\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3249\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2779 [0/85136 (0%)]\tlog Loss: -14.544478\n",
      "\n",
      "Train set: Average log loss: -14.6066\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3254\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2780 [0/85136 (0%)]\tlog Loss: -14.545346\n",
      "\n",
      "Train set: Average log loss: -14.6066\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3255\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2781 [0/85136 (0%)]\tlog Loss: -14.545378\n",
      "\n",
      "Train set: Average log loss: -14.6069\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3259\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2782 [0/85136 (0%)]\tlog Loss: -14.545605\n",
      "\n",
      "Train set: Average log loss: -14.6069\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3262\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2783 [0/85136 (0%)]\tlog Loss: -14.546345\n",
      "\n",
      "Train set: Average log loss: -14.6070\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3267\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2784 [0/85136 (0%)]\tlog Loss: -14.547061\n",
      "\n",
      "Train set: Average log loss: -14.6072\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3269\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2785 [0/85136 (0%)]\tlog Loss: -14.547133\n",
      "\n",
      "Train set: Average log loss: -14.6072\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3272\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2786 [0/85136 (0%)]\tlog Loss: -14.547418\n",
      "\n",
      "Train set: Average log loss: -14.6074\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3276\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2787 [0/85136 (0%)]\tlog Loss: -14.548327\n",
      "\n",
      "Train set: Average log loss: -14.6075\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3276\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2788 [0/85136 (0%)]\tlog Loss: -14.548158\n",
      "\n",
      "Train set: Average log loss: -14.6076\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3279\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2789 [0/85136 (0%)]\tlog Loss: -14.548336\n",
      "\n",
      "Train set: Average log loss: -14.6076\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3283\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2790 [0/85136 (0%)]\tlog Loss: -14.549097\n",
      "\n",
      "Train set: Average log loss: -14.6077\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3283\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2791 [0/85136 (0%)]\tlog Loss: -14.549139\n",
      "\n",
      "Train set: Average log loss: -14.6078\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3285\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2792 [0/85136 (0%)]\tlog Loss: -14.549409\n",
      "\n",
      "Train set: Average log loss: -14.6079\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3287\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2793 [0/85136 (0%)]\tlog Loss: -14.549843\n",
      "\n",
      "Train set: Average log loss: -14.6080\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3290\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2794 [0/85136 (0%)]\tlog Loss: -14.550080\n",
      "\n",
      "Train set: Average log loss: -14.6080\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3291\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2795 [0/85136 (0%)]\tlog Loss: -14.550452\n",
      "\n",
      "Train set: Average log loss: -14.6082\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3293\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2796 [0/85136 (0%)]\tlog Loss: -14.550673\n",
      "\n",
      "Train set: Average log loss: -14.6082\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3293\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2797 [0/85136 (0%)]\tlog Loss: -14.550676\n",
      "\n",
      "Train set: Average log loss: -14.6083\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3299\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2798 [0/85136 (0%)]\tlog Loss: -14.551613\n",
      "\n",
      "Train set: Average log loss: -14.6083\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3296\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2799 [0/85136 (0%)]\tlog Loss: -14.551101\n",
      "\n",
      "Train set: Average log loss: -14.6084\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3298\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2800 [0/85136 (0%)]\tlog Loss: -14.551435\n",
      "\n",
      "Train set: Average log loss: -14.6086\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3301\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2801 [0/85136 (0%)]\tlog Loss: -14.552045\n",
      "\n",
      "Train set: Average log loss: -14.6087\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3304\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2802 [0/85136 (0%)]\tlog Loss: -14.552188\n",
      "\n",
      "Train set: Average log loss: -14.6086\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3303\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2803 [0/85136 (0%)]\tlog Loss: -14.552348\n",
      "\n",
      "Train set: Average log loss: -14.6087\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3304\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2804 [0/85136 (0%)]\tlog Loss: -14.552288\n",
      "\n",
      "Train set: Average log loss: -14.6088\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3305\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2805 [0/85136 (0%)]\tlog Loss: -14.552216\n",
      "\n",
      "Train set: Average log loss: -14.6089\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3308\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2806 [0/85136 (0%)]\tlog Loss: -14.552914\n",
      "\n",
      "Train set: Average log loss: -14.6090\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3309\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2807 [0/85136 (0%)]\tlog Loss: -14.553269\n",
      "\n",
      "Train set: Average log loss: -14.6090\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3308\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2808 [0/85136 (0%)]\tlog Loss: -14.552880\n",
      "\n",
      "Train set: Average log loss: -14.6091\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3311\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2809 [0/85136 (0%)]\tlog Loss: -14.553438\n",
      "\n",
      "Train set: Average log loss: -14.6092\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3311\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2810 [0/85136 (0%)]\tlog Loss: -14.553243\n",
      "\n",
      "Train set: Average log loss: -14.6092\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3312\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2811 [0/85136 (0%)]\tlog Loss: -14.553531\n",
      "\n",
      "Train set: Average log loss: -14.6093\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3312\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2812 [0/85136 (0%)]\tlog Loss: -14.553917\n",
      "\n",
      "Train set: Average log loss: -14.6095\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3316\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2813 [0/85136 (0%)]\tlog Loss: -14.554248\n",
      "\n",
      "Train set: Average log loss: -14.6094\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3315\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2814 [0/85136 (0%)]\tlog Loss: -14.554026\n",
      "\n",
      "Train set: Average log loss: -14.6095\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3315\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2815 [0/85136 (0%)]\tlog Loss: -14.554088\n",
      "\n",
      "Train set: Average log loss: -14.6096\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3318\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2816 [0/85136 (0%)]\tlog Loss: -14.554457\n",
      "\n",
      "Train set: Average log loss: -14.6097\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3318\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2817 [0/85136 (0%)]\tlog Loss: -14.554682\n",
      "\n",
      "Train set: Average log loss: -14.6097\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3319\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2818 [0/85136 (0%)]\tlog Loss: -14.554714\n",
      "\n",
      "Train set: Average log loss: -14.6098\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3319\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2819 [0/85136 (0%)]\tlog Loss: -14.554763\n",
      "\n",
      "Train set: Average log loss: -14.6099\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3321\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2820 [0/85136 (0%)]\tlog Loss: -14.555513\n",
      "\n",
      "Train set: Average log loss: -14.6100\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3322\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2821 [0/85136 (0%)]\tlog Loss: -14.555043\n",
      "\n",
      "Train set: Average log loss: -14.6099\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3322\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2822 [0/85136 (0%)]\tlog Loss: -14.555075\n",
      "\n",
      "Train set: Average log loss: -14.6100\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3323\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2823 [0/85136 (0%)]\tlog Loss: -14.555084\n",
      "\n",
      "Train set: Average log loss: -14.6101\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3325\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2824 [0/85136 (0%)]\tlog Loss: -14.555549\n",
      "\n",
      "Train set: Average log loss: -14.6101\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3324\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2825 [0/85136 (0%)]\tlog Loss: -14.555376\n",
      "\n",
      "Train set: Average log loss: -14.6102\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3325\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2826 [0/85136 (0%)]\tlog Loss: -14.555711\n",
      "\n",
      "Train set: Average log loss: -14.6103\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3326\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2827 [0/85136 (0%)]\tlog Loss: -14.555673\n",
      "\n",
      "Train set: Average log loss: -14.6104\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3325\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2828 [0/85136 (0%)]\tlog Loss: -14.555675\n",
      "\n",
      "Train set: Average log loss: -14.6105\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3327\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2829 [0/85136 (0%)]\tlog Loss: -14.556026\n",
      "\n",
      "Train set: Average log loss: -14.6105\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3329\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2830 [0/85136 (0%)]\tlog Loss: -14.556076\n",
      "\n",
      "Train set: Average log loss: -14.6106\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3329\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2831 [0/85136 (0%)]\tlog Loss: -14.556575\n",
      "\n",
      "Train set: Average log loss: -14.6107\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3329\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2832 [0/85136 (0%)]\tlog Loss: -14.556131\n",
      "\n",
      "Train set: Average log loss: -14.6107\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3331\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2833 [0/85136 (0%)]\tlog Loss: -14.556345\n",
      "\n",
      "Train set: Average log loss: -14.6108\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3330\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2834 [0/85136 (0%)]\tlog Loss: -14.556379\n",
      "\n",
      "Train set: Average log loss: -14.6108\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3331\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2835 [0/85136 (0%)]\tlog Loss: -14.556410\n",
      "\n",
      "Train set: Average log loss: -14.6109\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3334\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2836 [0/85136 (0%)]\tlog Loss: -14.556919\n",
      "\n",
      "Train set: Average log loss: -14.6110\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3333\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2837 [0/85136 (0%)]\tlog Loss: -14.556774\n",
      "\n",
      "Train set: Average log loss: -14.6111\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3333\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2838 [0/85136 (0%)]\tlog Loss: -14.556595\n",
      "\n",
      "Train set: Average log loss: -14.6111\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3334\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2839 [0/85136 (0%)]\tlog Loss: -14.556995\n",
      "\n",
      "Train set: Average log loss: -14.6113\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3333\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2840 [0/85136 (0%)]\tlog Loss: -14.556945\n",
      "\n",
      "Train set: Average log loss: -14.6113\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3336\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2841 [0/85136 (0%)]\tlog Loss: -14.557159\n",
      "\n",
      "Train set: Average log loss: -14.6113\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3337\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2842 [0/85136 (0%)]\tlog Loss: -14.557501\n",
      "\n",
      "Train set: Average log loss: -14.6114\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3339\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2843 [0/85136 (0%)]\tlog Loss: -14.557767\n",
      "\n",
      "Train set: Average log loss: -14.6115\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3337\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2844 [0/85136 (0%)]\tlog Loss: -14.557442\n",
      "\n",
      "Train set: Average log loss: -14.6116\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3339\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2845 [0/85136 (0%)]\tlog Loss: -14.557389\n",
      "\n",
      "Train set: Average log loss: -14.6116\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3339\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2846 [0/85136 (0%)]\tlog Loss: -14.557642\n",
      "\n",
      "Train set: Average log loss: -14.6117\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3339\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2847 [0/85136 (0%)]\tlog Loss: -14.557802\n",
      "\n",
      "Train set: Average log loss: -14.6118\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3341\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2848 [0/85136 (0%)]\tlog Loss: -14.558016\n",
      "\n",
      "Train set: Average log loss: -14.6118\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3342\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2849 [0/85136 (0%)]\tlog Loss: -14.557993\n",
      "\n",
      "Train set: Average log loss: -14.6120\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3341\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2850 [0/85136 (0%)]\tlog Loss: -14.557971\n",
      "\n",
      "Train set: Average log loss: -14.6120\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3343\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2851 [0/85136 (0%)]\tlog Loss: -14.558519\n",
      "\n",
      "Train set: Average log loss: -14.6121\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3343\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2852 [0/85136 (0%)]\tlog Loss: -14.558211\n",
      "\n",
      "Train set: Average log loss: -14.6120\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3342\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2853 [0/85136 (0%)]\tlog Loss: -14.558252\n",
      "\n",
      "Train set: Average log loss: -14.6122\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3343\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2854 [0/85136 (0%)]\tlog Loss: -14.558332\n",
      "\n",
      "Train set: Average log loss: -14.6123\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2855 [0/85136 (0%)]\tlog Loss: -14.558660\n",
      "\n",
      "Train set: Average log loss: -14.6123\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2856 [0/85136 (0%)]\tlog Loss: -14.558621\n",
      "\n",
      "Train set: Average log loss: -14.6124\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3345\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2857 [0/85136 (0%)]\tlog Loss: -14.558553\n",
      "\n",
      "Train set: Average log loss: -14.6124\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2858 [0/85136 (0%)]\tlog Loss: -14.558716\n",
      "\n",
      "Train set: Average log loss: -14.6125\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3346\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2859 [0/85136 (0%)]\tlog Loss: -14.558644\n",
      "\n",
      "Train set: Average log loss: -14.6126\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3347\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2860 [0/85136 (0%)]\tlog Loss: -14.559150\n",
      "\n",
      "Train set: Average log loss: -14.6126\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3349\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2861 [0/85136 (0%)]\tlog Loss: -14.559122\n",
      "\n",
      "Train set: Average log loss: -14.6127\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3349\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2862 [0/85136 (0%)]\tlog Loss: -14.558997\n",
      "\n",
      "Train set: Average log loss: -14.6128\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3349\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2863 [0/85136 (0%)]\tlog Loss: -14.559346\n",
      "\n",
      "Train set: Average log loss: -14.6129\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3349\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2864 [0/85136 (0%)]\tlog Loss: -14.559311\n",
      "\n",
      "Train set: Average log loss: -14.6130\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3351\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2865 [0/85136 (0%)]\tlog Loss: -14.559392\n",
      "\n",
      "Train set: Average log loss: -14.6130\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3350\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2866 [0/85136 (0%)]\tlog Loss: -14.559202\n",
      "\n",
      "Train set: Average log loss: -14.6130\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3351\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2867 [0/85136 (0%)]\tlog Loss: -14.559645\n",
      "\n",
      "Train set: Average log loss: -14.6132\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3353\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2868 [0/85136 (0%)]\tlog Loss: -14.559679\n",
      "\n",
      "Train set: Average log loss: -14.6132\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3353\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2869 [0/85136 (0%)]\tlog Loss: -14.559765\n",
      "\n",
      "Train set: Average log loss: -14.6133\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3351\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2870 [0/85136 (0%)]\tlog Loss: -14.559676\n",
      "\n",
      "Train set: Average log loss: -14.6134\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3354\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2871 [0/85136 (0%)]\tlog Loss: -14.559851\n",
      "\n",
      "Train set: Average log loss: -14.6134\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3355\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2872 [0/85136 (0%)]\tlog Loss: -14.560340\n",
      "\n",
      "Train set: Average log loss: -14.6135\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3355\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2873 [0/85136 (0%)]\tlog Loss: -14.560190\n",
      "\n",
      "Train set: Average log loss: -14.6135\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3354\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2874 [0/85136 (0%)]\tlog Loss: -14.560287\n",
      "\n",
      "Train set: Average log loss: -14.6136\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3356\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2875 [0/85136 (0%)]\tlog Loss: -14.560621\n",
      "\n",
      "Train set: Average log loss: -14.6137\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3358\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2876 [0/85136 (0%)]\tlog Loss: -14.560837\n",
      "\n",
      "Train set: Average log loss: -14.6138\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3357\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2877 [0/85136 (0%)]\tlog Loss: -14.560488\n",
      "\n",
      "Train set: Average log loss: -14.6138\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3356\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2878 [0/85136 (0%)]\tlog Loss: -14.560318\n",
      "\n",
      "Train set: Average log loss: -14.6139\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3360\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2879 [0/85136 (0%)]\tlog Loss: -14.560808\n",
      "\n",
      "Train set: Average log loss: -14.6139\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3359\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2880 [0/85136 (0%)]\tlog Loss: -14.560650\n",
      "\n",
      "Train set: Average log loss: -14.6140\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3360\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2881 [0/85136 (0%)]\tlog Loss: -14.561160\n",
      "\n",
      "Train set: Average log loss: -14.6142\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3359\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2882 [0/85136 (0%)]\tlog Loss: -14.561063\n",
      "\n",
      "Train set: Average log loss: -14.6141\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3360\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2883 [0/85136 (0%)]\tlog Loss: -14.560857\n",
      "\n",
      "Train set: Average log loss: -14.6143\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3360\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2884 [0/85136 (0%)]\tlog Loss: -14.561051\n",
      "\n",
      "Train set: Average log loss: -14.6143\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2885 [0/85136 (0%)]\tlog Loss: -14.561864\n",
      "\n",
      "Train set: Average log loss: -14.6144\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3362\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2886 [0/85136 (0%)]\tlog Loss: -14.561010\n",
      "\n",
      "Train set: Average log loss: -14.6144\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3362\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2887 [0/85136 (0%)]\tlog Loss: -14.561837\n",
      "\n",
      "Train set: Average log loss: -14.6146\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3365\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2888 [0/85136 (0%)]\tlog Loss: -14.561859\n",
      "\n",
      "Train set: Average log loss: -14.6146\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2889 [0/85136 (0%)]\tlog Loss: -14.561548\n",
      "\n",
      "Train set: Average log loss: -14.6146\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2890 [0/85136 (0%)]\tlog Loss: -14.561574\n",
      "\n",
      "Train set: Average log loss: -14.6147\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3366\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2891 [0/85136 (0%)]\tlog Loss: -14.561958\n",
      "\n",
      "Train set: Average log loss: -14.6148\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3365\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2892 [0/85136 (0%)]\tlog Loss: -14.561759\n",
      "\n",
      "Train set: Average log loss: -14.6148\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2893 [0/85136 (0%)]\tlog Loss: -14.561460\n",
      "\n",
      "Train set: Average log loss: -14.6149\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3364\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2894 [0/85136 (0%)]\tlog Loss: -14.561999\n",
      "\n",
      "Train set: Average log loss: -14.6150\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3367\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2895 [0/85136 (0%)]\tlog Loss: -14.562260\n",
      "\n",
      "Train set: Average log loss: -14.6151\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3370\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2896 [0/85136 (0%)]\tlog Loss: -14.562531\n",
      "\n",
      "Train set: Average log loss: -14.6152\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3366\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2897 [0/85136 (0%)]\tlog Loss: -14.561986\n",
      "\n",
      "Train set: Average log loss: -14.6152\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3367\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2898 [0/85136 (0%)]\tlog Loss: -14.562184\n",
      "\n",
      "Train set: Average log loss: -14.6154\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3370\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2899 [0/85136 (0%)]\tlog Loss: -14.562753\n",
      "\n",
      "Train set: Average log loss: -14.6154\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3371\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2900 [0/85136 (0%)]\tlog Loss: -14.563169\n",
      "\n",
      "Train set: Average log loss: -14.6155\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3370\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2901 [0/85136 (0%)]\tlog Loss: -14.562281\n",
      "\n",
      "Train set: Average log loss: -14.6155\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3371\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2902 [0/85136 (0%)]\tlog Loss: -14.562454\n",
      "\n",
      "Train set: Average log loss: -14.6156\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3373\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2903 [0/85136 (0%)]\tlog Loss: -14.562718\n",
      "\n",
      "Train set: Average log loss: -14.6157\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3372\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2904 [0/85136 (0%)]\tlog Loss: -14.563105\n",
      "\n",
      "Train set: Average log loss: -14.6158\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3372\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2905 [0/85136 (0%)]\tlog Loss: -14.562795\n",
      "\n",
      "Train set: Average log loss: -14.6158\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3374\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2906 [0/85136 (0%)]\tlog Loss: -14.563115\n",
      "\n",
      "Train set: Average log loss: -14.6159\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3375\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2907 [0/85136 (0%)]\tlog Loss: -14.563202\n",
      "\n",
      "Train set: Average log loss: -14.6160\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3376\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2908 [0/85136 (0%)]\tlog Loss: -14.563266\n",
      "\n",
      "Train set: Average log loss: -14.6161\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3376\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2909 [0/85136 (0%)]\tlog Loss: -14.563189\n",
      "\n",
      "Train set: Average log loss: -14.6160\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3376\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2910 [0/85136 (0%)]\tlog Loss: -14.563097\n",
      "\n",
      "Train set: Average log loss: -14.6162\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3378\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2911 [0/85136 (0%)]\tlog Loss: -14.564049\n",
      "\n",
      "Train set: Average log loss: -14.6162\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3378\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2912 [0/85136 (0%)]\tlog Loss: -14.563362\n",
      "\n",
      "Train set: Average log loss: -14.6163\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3376\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2913 [0/85136 (0%)]\tlog Loss: -14.563277\n",
      "\n",
      "Train set: Average log loss: -14.6163\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3378\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2914 [0/85136 (0%)]\tlog Loss: -14.563251\n",
      "\n",
      "Train set: Average log loss: -14.6165\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3380\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2915 [0/85136 (0%)]\tlog Loss: -14.564139\n",
      "\n",
      "Train set: Average log loss: -14.6165\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3380\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2916 [0/85136 (0%)]\tlog Loss: -14.564025\n",
      "\n",
      "Train set: Average log loss: -14.6166\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3380\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2917 [0/85136 (0%)]\tlog Loss: -14.563985\n",
      "\n",
      "Train set: Average log loss: -14.6167\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3381\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2918 [0/85136 (0%)]\tlog Loss: -14.564034\n",
      "\n",
      "Train set: Average log loss: -14.6168\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3382\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2919 [0/85136 (0%)]\tlog Loss: -14.564444\n",
      "\n",
      "Train set: Average log loss: -14.6168\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3384\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2920 [0/85136 (0%)]\tlog Loss: -14.564520\n",
      "\n",
      "Train set: Average log loss: -14.6168\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3383\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2921 [0/85136 (0%)]\tlog Loss: -14.564300\n",
      "\n",
      "Train set: Average log loss: -14.6169\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3383\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2922 [0/85136 (0%)]\tlog Loss: -14.564536\n",
      "\n",
      "Train set: Average log loss: -14.6171\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3386\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2923 [0/85136 (0%)]\tlog Loss: -14.564855\n",
      "\n",
      "Train set: Average log loss: -14.6170\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3385\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2924 [0/85136 (0%)]\tlog Loss: -14.564643\n",
      "\n",
      "Train set: Average log loss: -14.6171\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3386\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2925 [0/85136 (0%)]\tlog Loss: -14.564684\n",
      "\n",
      "Train set: Average log loss: -14.6173\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3385\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2926 [0/85136 (0%)]\tlog Loss: -14.564523\n",
      "\n",
      "Train set: Average log loss: -14.6173\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3388\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2927 [0/85136 (0%)]\tlog Loss: -14.565111\n",
      "\n",
      "Train set: Average log loss: -14.6173\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3386\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2928 [0/85136 (0%)]\tlog Loss: -14.565128\n",
      "\n",
      "Train set: Average log loss: -14.6175\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3388\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2929 [0/85136 (0%)]\tlog Loss: -14.565105\n",
      "\n",
      "Train set: Average log loss: -14.6175\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3390\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2930 [0/85136 (0%)]\tlog Loss: -14.565233\n",
      "\n",
      "Train set: Average log loss: -14.6176\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3388\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2931 [0/85136 (0%)]\tlog Loss: -14.565289\n",
      "\n",
      "Train set: Average log loss: -14.6177\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3389\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2932 [0/85136 (0%)]\tlog Loss: -14.565024\n",
      "\n",
      "Train set: Average log loss: -14.6177\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3389\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2933 [0/85136 (0%)]\tlog Loss: -14.565267\n",
      "\n",
      "Train set: Average log loss: -14.6178\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3392\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2934 [0/85136 (0%)]\tlog Loss: -14.565485\n",
      "\n",
      "Train set: Average log loss: -14.6179\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3392\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2935 [0/85136 (0%)]\tlog Loss: -14.565758\n",
      "\n",
      "Train set: Average log loss: -14.6179\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3391\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2936 [0/85136 (0%)]\tlog Loss: -14.565404\n",
      "\n",
      "Train set: Average log loss: -14.6180\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3392\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2937 [0/85136 (0%)]\tlog Loss: -14.565529\n",
      "\n",
      "Train set: Average log loss: -14.6181\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3395\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2938 [0/85136 (0%)]\tlog Loss: -14.566189\n",
      "\n",
      "Train set: Average log loss: -14.6181\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3393\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2939 [0/85136 (0%)]\tlog Loss: -14.565656\n",
      "\n",
      "Train set: Average log loss: -14.6182\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3394\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2940 [0/85136 (0%)]\tlog Loss: -14.566208\n",
      "\n",
      "Train set: Average log loss: -14.6184\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3396\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2941 [0/85136 (0%)]\tlog Loss: -14.565948\n",
      "\n",
      "Train set: Average log loss: -14.6183\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3395\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2942 [0/85136 (0%)]\tlog Loss: -14.566136\n",
      "\n",
      "Train set: Average log loss: -14.6185\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3396\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2943 [0/85136 (0%)]\tlog Loss: -14.566218\n",
      "\n",
      "Train set: Average log loss: -14.6185\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3396\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2944 [0/85136 (0%)]\tlog Loss: -14.566498\n",
      "\n",
      "Train set: Average log loss: -14.6186\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3399\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2945 [0/85136 (0%)]\tlog Loss: -14.566588\n",
      "\n",
      "Train set: Average log loss: -14.6186\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3397\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2946 [0/85136 (0%)]\tlog Loss: -14.566329\n",
      "\n",
      "Train set: Average log loss: -14.6187\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3397\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2947 [0/85136 (0%)]\tlog Loss: -14.566449\n",
      "\n",
      "Train set: Average log loss: -14.6188\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3400\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2948 [0/85136 (0%)]\tlog Loss: -14.566578\n",
      "\n",
      "Train set: Average log loss: -14.6188\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3401\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2949 [0/85136 (0%)]\tlog Loss: -14.567204\n",
      "\n",
      "Train set: Average log loss: -14.6190\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3400\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2950 [0/85136 (0%)]\tlog Loss: -14.566929\n",
      "\n",
      "Train set: Average log loss: -14.6190\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3399\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2951 [0/85136 (0%)]\tlog Loss: -14.566637\n",
      "\n",
      "Train set: Average log loss: -14.6191\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3402\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2952 [0/85136 (0%)]\tlog Loss: -14.567006\n",
      "\n",
      "Train set: Average log loss: -14.6191\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3402\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2953 [0/85136 (0%)]\tlog Loss: -14.567380\n",
      "\n",
      "Train set: Average log loss: -14.6193\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3402\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2954 [0/85136 (0%)]\tlog Loss: -14.567284\n",
      "\n",
      "Train set: Average log loss: -14.6194\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3402\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2955 [0/85136 (0%)]\tlog Loss: -14.567153\n",
      "\n",
      "Train set: Average log loss: -14.6193\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3404\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2956 [0/85136 (0%)]\tlog Loss: -14.567098\n",
      "\n",
      "Train set: Average log loss: -14.6194\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3402\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2957 [0/85136 (0%)]\tlog Loss: -14.566989\n",
      "\n",
      "Train set: Average log loss: -14.6195\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3407\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2958 [0/85136 (0%)]\tlog Loss: -14.567848\n",
      "\n",
      "Train set: Average log loss: -14.6196\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3404\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2959 [0/85136 (0%)]\tlog Loss: -14.567255\n",
      "\n",
      "Train set: Average log loss: -14.6196\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3405\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2960 [0/85136 (0%)]\tlog Loss: -14.567596\n",
      "\n",
      "Train set: Average log loss: -14.6197\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3406\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2961 [0/85136 (0%)]\tlog Loss: -14.567672\n",
      "\n",
      "Train set: Average log loss: -14.6198\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3407\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2962 [0/85136 (0%)]\tlog Loss: -14.567932\n",
      "\n",
      "Train set: Average log loss: -14.6198\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3409\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2963 [0/85136 (0%)]\tlog Loss: -14.568039\n",
      "\n",
      "Train set: Average log loss: -14.6199\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3406\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2964 [0/85136 (0%)]\tlog Loss: -14.567547\n",
      "\n",
      "Train set: Average log loss: -14.6200\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3410\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2965 [0/85136 (0%)]\tlog Loss: -14.568117\n",
      "\n",
      "Train set: Average log loss: -14.6200\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3410\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2966 [0/85136 (0%)]\tlog Loss: -14.568377\n",
      "\n",
      "Train set: Average log loss: -14.6202\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3410\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2967 [0/85136 (0%)]\tlog Loss: -14.568318\n",
      "\n",
      "Train set: Average log loss: -14.6202\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3409\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2968 [0/85136 (0%)]\tlog Loss: -14.568089\n",
      "\n",
      "Train set: Average log loss: -14.6202\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3410\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2969 [0/85136 (0%)]\tlog Loss: -14.568211\n",
      "\n",
      "Train set: Average log loss: -14.6204\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3411\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2970 [0/85136 (0%)]\tlog Loss: -14.568614\n",
      "\n",
      "Train set: Average log loss: -14.6204\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3413\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2971 [0/85136 (0%)]\tlog Loss: -14.569190\n",
      "\n",
      "Train set: Average log loss: -14.6206\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3412\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2972 [0/85136 (0%)]\tlog Loss: -14.568253\n",
      "\n",
      "Train set: Average log loss: -14.6205\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3413\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2973 [0/85136 (0%)]\tlog Loss: -14.568745\n",
      "\n",
      "Train set: Average log loss: -14.6206\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3413\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2974 [0/85136 (0%)]\tlog Loss: -14.568557\n",
      "\n",
      "Train set: Average log loss: -14.6207\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3416\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2975 [0/85136 (0%)]\tlog Loss: -14.568977\n",
      "\n",
      "Train set: Average log loss: -14.6207\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3414\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2976 [0/85136 (0%)]\tlog Loss: -14.568849\n",
      "\n",
      "Train set: Average log loss: -14.6208\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3414\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2977 [0/85136 (0%)]\tlog Loss: -14.569006\n",
      "\n",
      "Train set: Average log loss: -14.6209\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3417\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2978 [0/85136 (0%)]\tlog Loss: -14.569022\n",
      "\n",
      "Train set: Average log loss: -14.6210\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3417\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2979 [0/85136 (0%)]\tlog Loss: -14.569014\n",
      "\n",
      "Train set: Average log loss: -14.6210\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3417\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2980 [0/85136 (0%)]\tlog Loss: -14.568890\n",
      "\n",
      "Train set: Average log loss: -14.6211\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3417\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2981 [0/85136 (0%)]\tlog Loss: -14.568911\n",
      "\n",
      "Train set: Average log loss: -14.6211\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3417\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2982 [0/85136 (0%)]\tlog Loss: -14.569344\n",
      "\n",
      "Train set: Average log loss: -14.6213\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3421\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2983 [0/85136 (0%)]\tlog Loss: -14.569772\n",
      "\n",
      "Train set: Average log loss: -14.6213\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3420\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2984 [0/85136 (0%)]\tlog Loss: -14.569317\n",
      "\n",
      "Train set: Average log loss: -14.6214\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3420\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2985 [0/85136 (0%)]\tlog Loss: -14.569537\n",
      "\n",
      "Train set: Average log loss: -14.6215\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3421\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2986 [0/85136 (0%)]\tlog Loss: -14.569941\n",
      "\n",
      "Train set: Average log loss: -14.6215\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3422\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2987 [0/85136 (0%)]\tlog Loss: -14.569491\n",
      "\n",
      "Train set: Average log loss: -14.6216\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3421\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2988 [0/85136 (0%)]\tlog Loss: -14.569645\n",
      "\n",
      "Train set: Average log loss: -14.6217\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3424\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2989 [0/85136 (0%)]\tlog Loss: -14.569956\n",
      "\n",
      "Train set: Average log loss: -14.6217\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3423\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2990 [0/85136 (0%)]\tlog Loss: -14.569986\n",
      "\n",
      "Train set: Average log loss: -14.6219\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3423\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2991 [0/85136 (0%)]\tlog Loss: -14.569897\n",
      "\n",
      "Train set: Average log loss: -14.6218\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3425\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2992 [0/85136 (0%)]\tlog Loss: -14.570161\n",
      "\n",
      "Train set: Average log loss: -14.6220\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3425\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2993 [0/85136 (0%)]\tlog Loss: -14.569781\n",
      "\n",
      "Train set: Average log loss: -14.6220\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3425\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2994 [0/85136 (0%)]\tlog Loss: -14.570029\n",
      "\n",
      "Train set: Average log loss: -14.6221\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3427\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2995 [0/85136 (0%)]\tlog Loss: -14.570200\n",
      "\n",
      "Train set: Average log loss: -14.6222\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3428\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2996 [0/85136 (0%)]\tlog Loss: -14.570248\n",
      "\n",
      "Train set: Average log loss: -14.6222\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3426\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2997 [0/85136 (0%)]\tlog Loss: -14.570262\n",
      "\n",
      "Train set: Average log loss: -14.6223\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3426\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2998 [0/85136 (0%)]\tlog Loss: -14.570111\n",
      "\n",
      "Train set: Average log loss: -14.6224\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3429\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2999 [0/85136 (0%)]\tlog Loss: -14.570858\n",
      "\n",
      "Train set: Average log loss: -14.6224\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3429\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 3000 [0/85136 (0%)]\tlog Loss: -14.570424\n",
      "\n",
      "Train set: Average log loss: -14.6225\n",
      "\n",
      "\n",
      "Test set: Average loss: -14.3428\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "args =  {\"batch_size\": 1024,\n",
    "         \"test_batch_size\": 4048,\n",
    "         \"epochs\" : 3*10**3,\n",
    "         \"lr\": 1e-4,\n",
    "         \"gamma\": .1,\n",
    "         \"no_cuda\" : False,\n",
    "         \"run_dry\": False,\n",
    "         \"seed\": 0,\n",
    "         \"log_interval\" : 100,\n",
    "         \"dry_run\" : False,\n",
    "         \"save_model\": True}\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "# Loading Train / Test Data\n",
    "bs_dataset = IV_LHS_data_generator(n = 10**5, log_scale = True)\n",
    "train_size, test_size = int(bs_dataset.shape[0]*0.9), int(bs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(bs_dataset, [train_size, bs_dataset.shape[0]-train_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])\n",
    "\n",
    "\n",
    "# Model training\n",
    "print(device)\n",
    "model = IV_ANN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"]) # Adam is found to be the best optimizer in the article\n",
    "scheduler = StepLR(optimizer, step_size=10**3, gamma=args[\"gamma\"]) # Deacreses the Learning rate by .1 every 1000 epoch\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "  train_loss = model.train_model(args, device, train_loader, optimizer, epoch)\n",
    "  test_loss = model.test_model(device, test_loader)\n",
    "  print(\"Current Learning rate {}\".format(scheduler.get_last_lr()[0]))\n",
    "  scheduler.step()\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "if args[\"save_model\"] :\n",
    "  torch.save(model.state_dict(), \"IV_SCALED_ANN.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izC1lm-COfWE",
    "outputId": "80157908-7122-46ad-e186-b0f1bd292cc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IV_ANN(\n",
       "  (fc1): Linear(in_features=4, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc3): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc4): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc5): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model from saved weights\n",
    "device = torch.device(\"cuda\")\n",
    "loaded_IV_ANN = IV_ANN().to(device)\n",
    "PATH = \"/content/IV_SCALED_ANN.pt\"\n",
    "loaded_IV_ANN.load_state_dict(torch.load(PATH))\n",
    "loaded_IV_ANN.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "NfoTYD3N1DLS",
    "outputId": "dc124cfa-ad30-4ab1-c771-95d8d0350cfa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX  0.011710465\n",
      "MIN  -0.014313459\n"
     ]
    }
   ],
   "source": [
    "# Plotting erros histogram\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_IV_ANN(data)\n",
    "    errors.append((target-output).data.cpu().numpy())\n",
    "  errors = np.concatenate(errors)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(errors,100,fill=False)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Difference\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim([-0.005, 0.005])\n",
    "plt.show()\n",
    "print(\"MAX \",errors.max())\n",
    "print(\"MIN \",errors.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaTuTmjOMwJm",
    "outputId": "0214c9c1-df35-4799-f906-def621c3caa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Train scores----------------\n",
      "MSE 6.154741e-07\n",
      "MAE 5.647203e-04\n",
      "MAPE 1.531313e-03\n",
      "R2 9.999911e-01\n",
      "-----------Test scores----------------\n",
      "MSE 6.253106e-07\n",
      "MAE 5.697074e-04\n",
      "MAPE 1.569888e-03\n",
      "R2 9.999909e-01\n"
     ]
    }
   ],
   "source": [
    "# Printing train and test MSE, MAE, MAPE and R2\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_IV_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Train scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )\n",
    "\n",
    "\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_IV_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Test scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78q0DF6HAeN5"
   },
   "source": [
    "### Training IV-ANN without log transofmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O26QLAObAhb6",
    "outputId": "8ac13683-813d-454d-ede4-e9d849278fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mLe flux de sortie a t tronqu et ne contient que les 5000dernires lignes.\u001b[0m\n",
      "Train Epoch: 2376 [0/90000 (0%)]\tlog Loss: -7.477649\n",
      "\n",
      "Train set: Average log loss: -7.3990\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2377 [0/90000 (0%)]\tlog Loss: -7.477773\n",
      "\n",
      "Train set: Average log loss: -7.3990\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2378 [0/90000 (0%)]\tlog Loss: -7.477827\n",
      "\n",
      "Train set: Average log loss: -7.3991\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2379 [0/90000 (0%)]\tlog Loss: -7.477878\n",
      "\n",
      "Train set: Average log loss: -7.3991\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2380 [0/90000 (0%)]\tlog Loss: -7.477941\n",
      "\n",
      "Train set: Average log loss: -7.3992\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2381 [0/90000 (0%)]\tlog Loss: -7.477950\n",
      "\n",
      "Train set: Average log loss: -7.3992\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2382 [0/90000 (0%)]\tlog Loss: -7.477885\n",
      "\n",
      "Train set: Average log loss: -7.3992\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2383 [0/90000 (0%)]\tlog Loss: -7.478020\n",
      "\n",
      "Train set: Average log loss: -7.3992\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2384 [0/90000 (0%)]\tlog Loss: -7.477952\n",
      "\n",
      "Train set: Average log loss: -7.3993\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2385 [0/90000 (0%)]\tlog Loss: -7.478053\n",
      "\n",
      "Train set: Average log loss: -7.3994\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2386 [0/90000 (0%)]\tlog Loss: -7.478087\n",
      "\n",
      "Train set: Average log loss: -7.3994\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2387 [0/90000 (0%)]\tlog Loss: -7.478164\n",
      "\n",
      "Train set: Average log loss: -7.3994\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2388 [0/90000 (0%)]\tlog Loss: -7.478206\n",
      "\n",
      "Train set: Average log loss: -7.3994\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2389 [0/90000 (0%)]\tlog Loss: -7.478196\n",
      "\n",
      "Train set: Average log loss: -7.3995\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2390 [0/90000 (0%)]\tlog Loss: -7.478255\n",
      "\n",
      "Train set: Average log loss: -7.3995\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2391 [0/90000 (0%)]\tlog Loss: -7.478143\n",
      "\n",
      "Train set: Average log loss: -7.3995\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2392 [0/90000 (0%)]\tlog Loss: -7.478256\n",
      "\n",
      "Train set: Average log loss: -7.3996\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2393 [0/90000 (0%)]\tlog Loss: -7.478232\n",
      "\n",
      "Train set: Average log loss: -7.3996\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3940\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2394 [0/90000 (0%)]\tlog Loss: -7.478269\n",
      "\n",
      "Train set: Average log loss: -7.3996\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2395 [0/90000 (0%)]\tlog Loss: -7.478312\n",
      "\n",
      "Train set: Average log loss: -7.3997\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2396 [0/90000 (0%)]\tlog Loss: -7.478304\n",
      "\n",
      "Train set: Average log loss: -7.3997\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2397 [0/90000 (0%)]\tlog Loss: -7.478361\n",
      "\n",
      "Train set: Average log loss: -7.3997\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2398 [0/90000 (0%)]\tlog Loss: -7.478402\n",
      "\n",
      "Train set: Average log loss: -7.3998\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2399 [0/90000 (0%)]\tlog Loss: -7.478432\n",
      "\n",
      "Train set: Average log loss: -7.3998\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2400 [0/90000 (0%)]\tlog Loss: -7.478501\n",
      "\n",
      "Train set: Average log loss: -7.3999\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2401 [0/90000 (0%)]\tlog Loss: -7.478493\n",
      "\n",
      "Train set: Average log loss: -7.3999\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2402 [0/90000 (0%)]\tlog Loss: -7.478519\n",
      "\n",
      "Train set: Average log loss: -7.3999\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2403 [0/90000 (0%)]\tlog Loss: -7.478586\n",
      "\n",
      "Train set: Average log loss: -7.3999\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2404 [0/90000 (0%)]\tlog Loss: -7.478574\n",
      "\n",
      "Train set: Average log loss: -7.4000\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2405 [0/90000 (0%)]\tlog Loss: -7.478623\n",
      "\n",
      "Train set: Average log loss: -7.4001\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2406 [0/90000 (0%)]\tlog Loss: -7.478711\n",
      "\n",
      "Train set: Average log loss: -7.4000\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2407 [0/90000 (0%)]\tlog Loss: -7.478771\n",
      "\n",
      "Train set: Average log loss: -7.4001\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2408 [0/90000 (0%)]\tlog Loss: -7.478764\n",
      "\n",
      "Train set: Average log loss: -7.4002\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2409 [0/90000 (0%)]\tlog Loss: -7.478826\n",
      "\n",
      "Train set: Average log loss: -7.4002\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2410 [0/90000 (0%)]\tlog Loss: -7.478781\n",
      "\n",
      "Train set: Average log loss: -7.4002\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2411 [0/90000 (0%)]\tlog Loss: -7.478917\n",
      "\n",
      "Train set: Average log loss: -7.4002\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2412 [0/90000 (0%)]\tlog Loss: -7.478926\n",
      "\n",
      "Train set: Average log loss: -7.4003\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2413 [0/90000 (0%)]\tlog Loss: -7.479029\n",
      "\n",
      "Train set: Average log loss: -7.4003\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2414 [0/90000 (0%)]\tlog Loss: -7.478883\n",
      "\n",
      "Train set: Average log loss: -7.4003\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2415 [0/90000 (0%)]\tlog Loss: -7.479017\n",
      "\n",
      "Train set: Average log loss: -7.4004\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2416 [0/90000 (0%)]\tlog Loss: -7.479105\n",
      "\n",
      "Train set: Average log loss: -7.4004\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2417 [0/90000 (0%)]\tlog Loss: -7.478999\n",
      "\n",
      "Train set: Average log loss: -7.4004\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2418 [0/90000 (0%)]\tlog Loss: -7.479104\n",
      "\n",
      "Train set: Average log loss: -7.4005\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2419 [0/90000 (0%)]\tlog Loss: -7.479192\n",
      "\n",
      "Train set: Average log loss: -7.4005\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2420 [0/90000 (0%)]\tlog Loss: -7.479154\n",
      "\n",
      "Train set: Average log loss: -7.4005\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2421 [0/90000 (0%)]\tlog Loss: -7.479220\n",
      "\n",
      "Train set: Average log loss: -7.4006\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2422 [0/90000 (0%)]\tlog Loss: -7.479246\n",
      "\n",
      "Train set: Average log loss: -7.4007\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2423 [0/90000 (0%)]\tlog Loss: -7.479328\n",
      "\n",
      "Train set: Average log loss: -7.4006\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2424 [0/90000 (0%)]\tlog Loss: -7.479350\n",
      "\n",
      "Train set: Average log loss: -7.4007\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2425 [0/90000 (0%)]\tlog Loss: -7.479414\n",
      "\n",
      "Train set: Average log loss: -7.4008\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2426 [0/90000 (0%)]\tlog Loss: -7.479393\n",
      "\n",
      "Train set: Average log loss: -7.4007\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3950\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2427 [0/90000 (0%)]\tlog Loss: -7.479411\n",
      "\n",
      "Train set: Average log loss: -7.4008\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2428 [0/90000 (0%)]\tlog Loss: -7.479454\n",
      "\n",
      "Train set: Average log loss: -7.4009\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2429 [0/90000 (0%)]\tlog Loss: -7.479564\n",
      "\n",
      "Train set: Average log loss: -7.4009\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2430 [0/90000 (0%)]\tlog Loss: -7.479599\n",
      "\n",
      "Train set: Average log loss: -7.4009\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2431 [0/90000 (0%)]\tlog Loss: -7.479618\n",
      "\n",
      "Train set: Average log loss: -7.4009\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2432 [0/90000 (0%)]\tlog Loss: -7.479618\n",
      "\n",
      "Train set: Average log loss: -7.4010\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2433 [0/90000 (0%)]\tlog Loss: -7.479664\n",
      "\n",
      "Train set: Average log loss: -7.4010\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2434 [0/90000 (0%)]\tlog Loss: -7.479748\n",
      "\n",
      "Train set: Average log loss: -7.4010\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2435 [0/90000 (0%)]\tlog Loss: -7.479688\n",
      "\n",
      "Train set: Average log loss: -7.4011\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3954\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2436 [0/90000 (0%)]\tlog Loss: -7.479781\n",
      "\n",
      "Train set: Average log loss: -7.4011\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2437 [0/90000 (0%)]\tlog Loss: -7.479786\n",
      "\n",
      "Train set: Average log loss: -7.4011\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3954\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2438 [0/90000 (0%)]\tlog Loss: -7.479786\n",
      "\n",
      "Train set: Average log loss: -7.4011\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2439 [0/90000 (0%)]\tlog Loss: -7.479878\n",
      "\n",
      "Train set: Average log loss: -7.4012\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2440 [0/90000 (0%)]\tlog Loss: -7.479946\n",
      "\n",
      "Train set: Average log loss: -7.4012\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2441 [0/90000 (0%)]\tlog Loss: -7.479875\n",
      "\n",
      "Train set: Average log loss: -7.4013\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2442 [0/90000 (0%)]\tlog Loss: -7.480028\n",
      "\n",
      "Train set: Average log loss: -7.4013\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2443 [0/90000 (0%)]\tlog Loss: -7.480070\n",
      "\n",
      "Train set: Average log loss: -7.4014\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3956\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2444 [0/90000 (0%)]\tlog Loss: -7.480129\n",
      "\n",
      "Train set: Average log loss: -7.4014\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2445 [0/90000 (0%)]\tlog Loss: -7.480080\n",
      "\n",
      "Train set: Average log loss: -7.4014\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3956\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2446 [0/90000 (0%)]\tlog Loss: -7.480144\n",
      "\n",
      "Train set: Average log loss: -7.4014\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3956\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2447 [0/90000 (0%)]\tlog Loss: -7.480206\n",
      "\n",
      "Train set: Average log loss: -7.4015\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2448 [0/90000 (0%)]\tlog Loss: -7.480211\n",
      "\n",
      "Train set: Average log loss: -7.4016\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2449 [0/90000 (0%)]\tlog Loss: -7.480242\n",
      "\n",
      "Train set: Average log loss: -7.4015\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2450 [0/90000 (0%)]\tlog Loss: -7.480298\n",
      "\n",
      "Train set: Average log loss: -7.4016\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2451 [0/90000 (0%)]\tlog Loss: -7.480351\n",
      "\n",
      "Train set: Average log loss: -7.4016\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3958\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2452 [0/90000 (0%)]\tlog Loss: -7.480378\n",
      "\n",
      "Train set: Average log loss: -7.4017\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3958\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2453 [0/90000 (0%)]\tlog Loss: -7.480428\n",
      "\n",
      "Train set: Average log loss: -7.4017\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2454 [0/90000 (0%)]\tlog Loss: -7.480419\n",
      "\n",
      "Train set: Average log loss: -7.4018\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2455 [0/90000 (0%)]\tlog Loss: -7.480482\n",
      "\n",
      "Train set: Average log loss: -7.4018\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2456 [0/90000 (0%)]\tlog Loss: -7.480505\n",
      "\n",
      "Train set: Average log loss: -7.4018\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2457 [0/90000 (0%)]\tlog Loss: -7.480425\n",
      "\n",
      "Train set: Average log loss: -7.4018\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2458 [0/90000 (0%)]\tlog Loss: -7.480543\n",
      "\n",
      "Train set: Average log loss: -7.4019\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2459 [0/90000 (0%)]\tlog Loss: -7.480579\n",
      "\n",
      "Train set: Average log loss: -7.4019\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2460 [0/90000 (0%)]\tlog Loss: -7.480579\n",
      "\n",
      "Train set: Average log loss: -7.4019\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2461 [0/90000 (0%)]\tlog Loss: -7.480647\n",
      "\n",
      "Train set: Average log loss: -7.4019\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2462 [0/90000 (0%)]\tlog Loss: -7.480718\n",
      "\n",
      "Train set: Average log loss: -7.4020\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2463 [0/90000 (0%)]\tlog Loss: -7.480643\n",
      "\n",
      "Train set: Average log loss: -7.4021\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2464 [0/90000 (0%)]\tlog Loss: -7.480674\n",
      "\n",
      "Train set: Average log loss: -7.4021\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2465 [0/90000 (0%)]\tlog Loss: -7.480765\n",
      "\n",
      "Train set: Average log loss: -7.4021\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2466 [0/90000 (0%)]\tlog Loss: -7.480863\n",
      "\n",
      "Train set: Average log loss: -7.4021\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2467 [0/90000 (0%)]\tlog Loss: -7.480900\n",
      "\n",
      "Train set: Average log loss: -7.4022\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2468 [0/90000 (0%)]\tlog Loss: -7.480932\n",
      "\n",
      "Train set: Average log loss: -7.4022\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2469 [0/90000 (0%)]\tlog Loss: -7.480964\n",
      "\n",
      "Train set: Average log loss: -7.4022\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2470 [0/90000 (0%)]\tlog Loss: -7.481012\n",
      "\n",
      "Train set: Average log loss: -7.4022\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2471 [0/90000 (0%)]\tlog Loss: -7.480980\n",
      "\n",
      "Train set: Average log loss: -7.4023\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3964\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2472 [0/90000 (0%)]\tlog Loss: -7.481028\n",
      "\n",
      "Train set: Average log loss: -7.4023\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3964\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2473 [0/90000 (0%)]\tlog Loss: -7.481052\n",
      "\n",
      "Train set: Average log loss: -7.4024\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3964\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2474 [0/90000 (0%)]\tlog Loss: -7.481078\n",
      "\n",
      "Train set: Average log loss: -7.4024\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2475 [0/90000 (0%)]\tlog Loss: -7.481148\n",
      "\n",
      "Train set: Average log loss: -7.4025\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2476 [0/90000 (0%)]\tlog Loss: -7.481107\n",
      "\n",
      "Train set: Average log loss: -7.4025\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2477 [0/90000 (0%)]\tlog Loss: -7.481142\n",
      "\n",
      "Train set: Average log loss: -7.4025\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2478 [0/90000 (0%)]\tlog Loss: -7.481251\n",
      "\n",
      "Train set: Average log loss: -7.4025\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2479 [0/90000 (0%)]\tlog Loss: -7.481261\n",
      "\n",
      "Train set: Average log loss: -7.4026\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2480 [0/90000 (0%)]\tlog Loss: -7.481331\n",
      "\n",
      "Train set: Average log loss: -7.4026\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2481 [0/90000 (0%)]\tlog Loss: -7.481394\n",
      "\n",
      "Train set: Average log loss: -7.4026\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2482 [0/90000 (0%)]\tlog Loss: -7.481358\n",
      "\n",
      "Train set: Average log loss: -7.4027\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2483 [0/90000 (0%)]\tlog Loss: -7.481433\n",
      "\n",
      "Train set: Average log loss: -7.4027\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2484 [0/90000 (0%)]\tlog Loss: -7.481427\n",
      "\n",
      "Train set: Average log loss: -7.4027\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2485 [0/90000 (0%)]\tlog Loss: -7.481457\n",
      "\n",
      "Train set: Average log loss: -7.4028\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2486 [0/90000 (0%)]\tlog Loss: -7.481474\n",
      "\n",
      "Train set: Average log loss: -7.4028\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2487 [0/90000 (0%)]\tlog Loss: -7.481510\n",
      "\n",
      "Train set: Average log loss: -7.4029\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2488 [0/90000 (0%)]\tlog Loss: -7.481624\n",
      "\n",
      "Train set: Average log loss: -7.4029\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2489 [0/90000 (0%)]\tlog Loss: -7.481559\n",
      "\n",
      "Train set: Average log loss: -7.4029\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2490 [0/90000 (0%)]\tlog Loss: -7.481628\n",
      "\n",
      "Train set: Average log loss: -7.4030\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2491 [0/90000 (0%)]\tlog Loss: -7.481643\n",
      "\n",
      "Train set: Average log loss: -7.4030\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2492 [0/90000 (0%)]\tlog Loss: -7.481672\n",
      "\n",
      "Train set: Average log loss: -7.4031\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2493 [0/90000 (0%)]\tlog Loss: -7.481689\n",
      "\n",
      "Train set: Average log loss: -7.4031\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2494 [0/90000 (0%)]\tlog Loss: -7.481691\n",
      "\n",
      "Train set: Average log loss: -7.4031\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2495 [0/90000 (0%)]\tlog Loss: -7.481765\n",
      "\n",
      "Train set: Average log loss: -7.4031\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2496 [0/90000 (0%)]\tlog Loss: -7.481765\n",
      "\n",
      "Train set: Average log loss: -7.4032\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2497 [0/90000 (0%)]\tlog Loss: -7.481807\n",
      "\n",
      "Train set: Average log loss: -7.4032\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2498 [0/90000 (0%)]\tlog Loss: -7.481751\n",
      "\n",
      "Train set: Average log loss: -7.4032\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2499 [0/90000 (0%)]\tlog Loss: -7.481845\n",
      "\n",
      "Train set: Average log loss: -7.4033\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2500 [0/90000 (0%)]\tlog Loss: -7.481811\n",
      "\n",
      "Train set: Average log loss: -7.4033\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2501 [0/90000 (0%)]\tlog Loss: -7.481864\n",
      "\n",
      "Train set: Average log loss: -7.4033\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2502 [0/90000 (0%)]\tlog Loss: -7.481878\n",
      "\n",
      "Train set: Average log loss: -7.4034\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2503 [0/90000 (0%)]\tlog Loss: -7.481912\n",
      "\n",
      "Train set: Average log loss: -7.4034\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2504 [0/90000 (0%)]\tlog Loss: -7.481949\n",
      "\n",
      "Train set: Average log loss: -7.4034\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2505 [0/90000 (0%)]\tlog Loss: -7.482072\n",
      "\n",
      "Train set: Average log loss: -7.4035\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2506 [0/90000 (0%)]\tlog Loss: -7.482140\n",
      "\n",
      "Train set: Average log loss: -7.4035\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2507 [0/90000 (0%)]\tlog Loss: -7.482035\n",
      "\n",
      "Train set: Average log loss: -7.4035\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2508 [0/90000 (0%)]\tlog Loss: -7.482172\n",
      "\n",
      "Train set: Average log loss: -7.4036\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2509 [0/90000 (0%)]\tlog Loss: -7.482249\n",
      "\n",
      "Train set: Average log loss: -7.4036\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2510 [0/90000 (0%)]\tlog Loss: -7.482214\n",
      "\n",
      "Train set: Average log loss: -7.4036\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2511 [0/90000 (0%)]\tlog Loss: -7.482251\n",
      "\n",
      "Train set: Average log loss: -7.4037\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2512 [0/90000 (0%)]\tlog Loss: -7.482261\n",
      "\n",
      "Train set: Average log loss: -7.4037\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2513 [0/90000 (0%)]\tlog Loss: -7.482282\n",
      "\n",
      "Train set: Average log loss: -7.4037\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2514 [0/90000 (0%)]\tlog Loss: -7.482338\n",
      "\n",
      "Train set: Average log loss: -7.4037\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2515 [0/90000 (0%)]\tlog Loss: -7.482328\n",
      "\n",
      "Train set: Average log loss: -7.4038\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2516 [0/90000 (0%)]\tlog Loss: -7.482437\n",
      "\n",
      "Train set: Average log loss: -7.4038\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2517 [0/90000 (0%)]\tlog Loss: -7.482499\n",
      "\n",
      "Train set: Average log loss: -7.4039\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2518 [0/90000 (0%)]\tlog Loss: -7.482494\n",
      "\n",
      "Train set: Average log loss: -7.4039\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2519 [0/90000 (0%)]\tlog Loss: -7.482474\n",
      "\n",
      "Train set: Average log loss: -7.4039\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2520 [0/90000 (0%)]\tlog Loss: -7.482534\n",
      "\n",
      "Train set: Average log loss: -7.4040\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2521 [0/90000 (0%)]\tlog Loss: -7.482588\n",
      "\n",
      "Train set: Average log loss: -7.4040\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2522 [0/90000 (0%)]\tlog Loss: -7.482692\n",
      "\n",
      "Train set: Average log loss: -7.4040\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2523 [0/90000 (0%)]\tlog Loss: -7.482665\n",
      "\n",
      "Train set: Average log loss: -7.4040\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2524 [0/90000 (0%)]\tlog Loss: -7.482747\n",
      "\n",
      "Train set: Average log loss: -7.4041\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2525 [0/90000 (0%)]\tlog Loss: -7.482742\n",
      "\n",
      "Train set: Average log loss: -7.4041\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2526 [0/90000 (0%)]\tlog Loss: -7.482669\n",
      "\n",
      "Train set: Average log loss: -7.4041\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2527 [0/90000 (0%)]\tlog Loss: -7.482882\n",
      "\n",
      "Train set: Average log loss: -7.4042\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2528 [0/90000 (0%)]\tlog Loss: -7.482826\n",
      "\n",
      "Train set: Average log loss: -7.4042\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2529 [0/90000 (0%)]\tlog Loss: -7.482914\n",
      "\n",
      "Train set: Average log loss: -7.4043\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2530 [0/90000 (0%)]\tlog Loss: -7.482961\n",
      "\n",
      "Train set: Average log loss: -7.4043\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2531 [0/90000 (0%)]\tlog Loss: -7.483013\n",
      "\n",
      "Train set: Average log loss: -7.4043\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2532 [0/90000 (0%)]\tlog Loss: -7.482984\n",
      "\n",
      "Train set: Average log loss: -7.4044\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2533 [0/90000 (0%)]\tlog Loss: -7.483052\n",
      "\n",
      "Train set: Average log loss: -7.4044\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2534 [0/90000 (0%)]\tlog Loss: -7.483177\n",
      "\n",
      "Train set: Average log loss: -7.4045\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2535 [0/90000 (0%)]\tlog Loss: -7.483196\n",
      "\n",
      "Train set: Average log loss: -7.4045\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2536 [0/90000 (0%)]\tlog Loss: -7.483190\n",
      "\n",
      "Train set: Average log loss: -7.4045\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2537 [0/90000 (0%)]\tlog Loss: -7.483253\n",
      "\n",
      "Train set: Average log loss: -7.4045\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2538 [0/90000 (0%)]\tlog Loss: -7.483305\n",
      "\n",
      "Train set: Average log loss: -7.4046\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2539 [0/90000 (0%)]\tlog Loss: -7.483307\n",
      "\n",
      "Train set: Average log loss: -7.4046\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2540 [0/90000 (0%)]\tlog Loss: -7.483471\n",
      "\n",
      "Train set: Average log loss: -7.4046\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2541 [0/90000 (0%)]\tlog Loss: -7.483418\n",
      "\n",
      "Train set: Average log loss: -7.4047\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2542 [0/90000 (0%)]\tlog Loss: -7.483472\n",
      "\n",
      "Train set: Average log loss: -7.4047\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2543 [0/90000 (0%)]\tlog Loss: -7.483531\n",
      "\n",
      "Train set: Average log loss: -7.4048\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2544 [0/90000 (0%)]\tlog Loss: -7.483545\n",
      "\n",
      "Train set: Average log loss: -7.4048\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2545 [0/90000 (0%)]\tlog Loss: -7.483504\n",
      "\n",
      "Train set: Average log loss: -7.4048\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2546 [0/90000 (0%)]\tlog Loss: -7.483560\n",
      "\n",
      "Train set: Average log loss: -7.4049\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2547 [0/90000 (0%)]\tlog Loss: -7.483713\n",
      "\n",
      "Train set: Average log loss: -7.4049\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2548 [0/90000 (0%)]\tlog Loss: -7.483606\n",
      "\n",
      "Train set: Average log loss: -7.4049\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2549 [0/90000 (0%)]\tlog Loss: -7.483720\n",
      "\n",
      "Train set: Average log loss: -7.4050\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3981\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2550 [0/90000 (0%)]\tlog Loss: -7.483765\n",
      "\n",
      "Train set: Average log loss: -7.4050\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3981\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2551 [0/90000 (0%)]\tlog Loss: -7.483762\n",
      "\n",
      "Train set: Average log loss: -7.4050\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3981\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2552 [0/90000 (0%)]\tlog Loss: -7.483735\n",
      "\n",
      "Train set: Average log loss: -7.4051\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2553 [0/90000 (0%)]\tlog Loss: -7.483928\n",
      "\n",
      "Train set: Average log loss: -7.4051\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3981\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2554 [0/90000 (0%)]\tlog Loss: -7.483913\n",
      "\n",
      "Train set: Average log loss: -7.4051\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2555 [0/90000 (0%)]\tlog Loss: -7.483839\n",
      "\n",
      "Train set: Average log loss: -7.4052\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2556 [0/90000 (0%)]\tlog Loss: -7.483985\n",
      "\n",
      "Train set: Average log loss: -7.4052\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2557 [0/90000 (0%)]\tlog Loss: -7.484017\n",
      "\n",
      "Train set: Average log loss: -7.4052\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2558 [0/90000 (0%)]\tlog Loss: -7.484051\n",
      "\n",
      "Train set: Average log loss: -7.4053\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3983\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2559 [0/90000 (0%)]\tlog Loss: -7.484098\n",
      "\n",
      "Train set: Average log loss: -7.4053\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3983\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2560 [0/90000 (0%)]\tlog Loss: -7.484131\n",
      "\n",
      "Train set: Average log loss: -7.4053\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3983\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2561 [0/90000 (0%)]\tlog Loss: -7.484135\n",
      "\n",
      "Train set: Average log loss: -7.4054\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2562 [0/90000 (0%)]\tlog Loss: -7.484175\n",
      "\n",
      "Train set: Average log loss: -7.4054\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2563 [0/90000 (0%)]\tlog Loss: -7.484267\n",
      "\n",
      "Train set: Average log loss: -7.4054\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2564 [0/90000 (0%)]\tlog Loss: -7.484250\n",
      "\n",
      "Train set: Average log loss: -7.4055\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2565 [0/90000 (0%)]\tlog Loss: -7.484292\n",
      "\n",
      "Train set: Average log loss: -7.4055\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2566 [0/90000 (0%)]\tlog Loss: -7.484369\n",
      "\n",
      "Train set: Average log loss: -7.4055\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2567 [0/90000 (0%)]\tlog Loss: -7.484440\n",
      "\n",
      "Train set: Average log loss: -7.4056\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2568 [0/90000 (0%)]\tlog Loss: -7.484431\n",
      "\n",
      "Train set: Average log loss: -7.4056\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2569 [0/90000 (0%)]\tlog Loss: -7.484512\n",
      "\n",
      "Train set: Average log loss: -7.4056\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2570 [0/90000 (0%)]\tlog Loss: -7.484471\n",
      "\n",
      "Train set: Average log loss: -7.4057\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2571 [0/90000 (0%)]\tlog Loss: -7.484608\n",
      "\n",
      "Train set: Average log loss: -7.4057\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2572 [0/90000 (0%)]\tlog Loss: -7.484693\n",
      "\n",
      "Train set: Average log loss: -7.4058\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2573 [0/90000 (0%)]\tlog Loss: -7.484667\n",
      "\n",
      "Train set: Average log loss: -7.4058\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2574 [0/90000 (0%)]\tlog Loss: -7.484654\n",
      "\n",
      "Train set: Average log loss: -7.4058\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2575 [0/90000 (0%)]\tlog Loss: -7.484783\n",
      "\n",
      "Train set: Average log loss: -7.4058\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2576 [0/90000 (0%)]\tlog Loss: -7.484771\n",
      "\n",
      "Train set: Average log loss: -7.4059\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2577 [0/90000 (0%)]\tlog Loss: -7.484832\n",
      "\n",
      "Train set: Average log loss: -7.4059\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2578 [0/90000 (0%)]\tlog Loss: -7.484890\n",
      "\n",
      "Train set: Average log loss: -7.4060\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2579 [0/90000 (0%)]\tlog Loss: -7.484870\n",
      "\n",
      "Train set: Average log loss: -7.4060\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2580 [0/90000 (0%)]\tlog Loss: -7.484993\n",
      "\n",
      "Train set: Average log loss: -7.4060\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2581 [0/90000 (0%)]\tlog Loss: -7.485014\n",
      "\n",
      "Train set: Average log loss: -7.4061\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2582 [0/90000 (0%)]\tlog Loss: -7.485041\n",
      "\n",
      "Train set: Average log loss: -7.4061\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2583 [0/90000 (0%)]\tlog Loss: -7.485083\n",
      "\n",
      "Train set: Average log loss: -7.4061\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2584 [0/90000 (0%)]\tlog Loss: -7.485083\n",
      "\n",
      "Train set: Average log loss: -7.4062\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2585 [0/90000 (0%)]\tlog Loss: -7.485217\n",
      "\n",
      "Train set: Average log loss: -7.4062\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2586 [0/90000 (0%)]\tlog Loss: -7.485205\n",
      "\n",
      "Train set: Average log loss: -7.4062\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2587 [0/90000 (0%)]\tlog Loss: -7.485161\n",
      "\n",
      "Train set: Average log loss: -7.4062\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2588 [0/90000 (0%)]\tlog Loss: -7.485262\n",
      "\n",
      "Train set: Average log loss: -7.4063\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2589 [0/90000 (0%)]\tlog Loss: -7.485328\n",
      "\n",
      "Train set: Average log loss: -7.4064\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2590 [0/90000 (0%)]\tlog Loss: -7.485361\n",
      "\n",
      "Train set: Average log loss: -7.4064\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2591 [0/90000 (0%)]\tlog Loss: -7.485443\n",
      "\n",
      "Train set: Average log loss: -7.4064\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2592 [0/90000 (0%)]\tlog Loss: -7.485451\n",
      "\n",
      "Train set: Average log loss: -7.4064\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2593 [0/90000 (0%)]\tlog Loss: -7.485453\n",
      "\n",
      "Train set: Average log loss: -7.4065\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2594 [0/90000 (0%)]\tlog Loss: -7.485497\n",
      "\n",
      "Train set: Average log loss: -7.4065\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2595 [0/90000 (0%)]\tlog Loss: -7.485627\n",
      "\n",
      "Train set: Average log loss: -7.4065\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2596 [0/90000 (0%)]\tlog Loss: -7.485569\n",
      "\n",
      "Train set: Average log loss: -7.4066\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2597 [0/90000 (0%)]\tlog Loss: -7.485673\n",
      "\n",
      "Train set: Average log loss: -7.4066\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2598 [0/90000 (0%)]\tlog Loss: -7.485702\n",
      "\n",
      "Train set: Average log loss: -7.4066\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2599 [0/90000 (0%)]\tlog Loss: -7.485609\n",
      "\n",
      "Train set: Average log loss: -7.4067\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3993\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2600 [0/90000 (0%)]\tlog Loss: -7.485765\n",
      "\n",
      "Train set: Average log loss: -7.4067\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3993\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2601 [0/90000 (0%)]\tlog Loss: -7.485846\n",
      "\n",
      "Train set: Average log loss: -7.4067\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3993\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2602 [0/90000 (0%)]\tlog Loss: -7.485835\n",
      "\n",
      "Train set: Average log loss: -7.4068\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3993\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2603 [0/90000 (0%)]\tlog Loss: -7.485840\n",
      "\n",
      "Train set: Average log loss: -7.4068\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2604 [0/90000 (0%)]\tlog Loss: -7.485875\n",
      "\n",
      "Train set: Average log loss: -7.4068\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2605 [0/90000 (0%)]\tlog Loss: -7.485879\n",
      "\n",
      "Train set: Average log loss: -7.4069\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2606 [0/90000 (0%)]\tlog Loss: -7.485974\n",
      "\n",
      "Train set: Average log loss: -7.4069\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2607 [0/90000 (0%)]\tlog Loss: -7.486072\n",
      "\n",
      "Train set: Average log loss: -7.4069\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2608 [0/90000 (0%)]\tlog Loss: -7.486015\n",
      "\n",
      "Train set: Average log loss: -7.4070\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2609 [0/90000 (0%)]\tlog Loss: -7.486074\n",
      "\n",
      "Train set: Average log loss: -7.4070\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2610 [0/90000 (0%)]\tlog Loss: -7.486043\n",
      "\n",
      "Train set: Average log loss: -7.4070\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2611 [0/90000 (0%)]\tlog Loss: -7.486099\n",
      "\n",
      "Train set: Average log loss: -7.4071\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2612 [0/90000 (0%)]\tlog Loss: -7.486120\n",
      "\n",
      "Train set: Average log loss: -7.4071\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2613 [0/90000 (0%)]\tlog Loss: -7.486186\n",
      "\n",
      "Train set: Average log loss: -7.4071\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2614 [0/90000 (0%)]\tlog Loss: -7.486250\n",
      "\n",
      "Train set: Average log loss: -7.4072\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2615 [0/90000 (0%)]\tlog Loss: -7.486312\n",
      "\n",
      "Train set: Average log loss: -7.4072\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2616 [0/90000 (0%)]\tlog Loss: -7.486239\n",
      "\n",
      "Train set: Average log loss: -7.4072\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2617 [0/90000 (0%)]\tlog Loss: -7.486308\n",
      "\n",
      "Train set: Average log loss: -7.4073\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2618 [0/90000 (0%)]\tlog Loss: -7.486344\n",
      "\n",
      "Train set: Average log loss: -7.4073\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2619 [0/90000 (0%)]\tlog Loss: -7.486442\n",
      "\n",
      "Train set: Average log loss: -7.4073\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3998\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2620 [0/90000 (0%)]\tlog Loss: -7.486409\n",
      "\n",
      "Train set: Average log loss: -7.4074\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2621 [0/90000 (0%)]\tlog Loss: -7.486514\n",
      "\n",
      "Train set: Average log loss: -7.4074\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3998\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2622 [0/90000 (0%)]\tlog Loss: -7.486521\n",
      "\n",
      "Train set: Average log loss: -7.4074\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2623 [0/90000 (0%)]\tlog Loss: -7.486576\n",
      "\n",
      "Train set: Average log loss: -7.4075\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2624 [0/90000 (0%)]\tlog Loss: -7.486568\n",
      "\n",
      "Train set: Average log loss: -7.4075\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2625 [0/90000 (0%)]\tlog Loss: -7.486586\n",
      "\n",
      "Train set: Average log loss: -7.4076\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3998\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2626 [0/90000 (0%)]\tlog Loss: -7.486645\n",
      "\n",
      "Train set: Average log loss: -7.4076\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3998\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2627 [0/90000 (0%)]\tlog Loss: -7.486733\n",
      "\n",
      "Train set: Average log loss: -7.4076\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3998\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2628 [0/90000 (0%)]\tlog Loss: -7.486724\n",
      "\n",
      "Train set: Average log loss: -7.4076\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2629 [0/90000 (0%)]\tlog Loss: -7.486780\n",
      "\n",
      "Train set: Average log loss: -7.4076\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2630 [0/90000 (0%)]\tlog Loss: -7.486764\n",
      "\n",
      "Train set: Average log loss: -7.4077\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2631 [0/90000 (0%)]\tlog Loss: -7.486835\n",
      "\n",
      "Train set: Average log loss: -7.4077\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2632 [0/90000 (0%)]\tlog Loss: -7.486912\n",
      "\n",
      "Train set: Average log loss: -7.4078\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.3999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2633 [0/90000 (0%)]\tlog Loss: -7.486876\n",
      "\n",
      "Train set: Average log loss: -7.4078\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2634 [0/90000 (0%)]\tlog Loss: -7.486961\n",
      "\n",
      "Train set: Average log loss: -7.4079\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2635 [0/90000 (0%)]\tlog Loss: -7.486985\n",
      "\n",
      "Train set: Average log loss: -7.4079\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2636 [0/90000 (0%)]\tlog Loss: -7.487059\n",
      "\n",
      "Train set: Average log loss: -7.4079\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2637 [0/90000 (0%)]\tlog Loss: -7.487022\n",
      "\n",
      "Train set: Average log loss: -7.4079\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2638 [0/90000 (0%)]\tlog Loss: -7.487040\n",
      "\n",
      "Train set: Average log loss: -7.4079\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2639 [0/90000 (0%)]\tlog Loss: -7.487132\n",
      "\n",
      "Train set: Average log loss: -7.4080\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2640 [0/90000 (0%)]\tlog Loss: -7.487121\n",
      "\n",
      "Train set: Average log loss: -7.4080\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2641 [0/90000 (0%)]\tlog Loss: -7.487157\n",
      "\n",
      "Train set: Average log loss: -7.4081\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2642 [0/90000 (0%)]\tlog Loss: -7.487286\n",
      "\n",
      "Train set: Average log loss: -7.4081\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2643 [0/90000 (0%)]\tlog Loss: -7.487231\n",
      "\n",
      "Train set: Average log loss: -7.4081\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2644 [0/90000 (0%)]\tlog Loss: -7.487268\n",
      "\n",
      "Train set: Average log loss: -7.4082\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2645 [0/90000 (0%)]\tlog Loss: -7.487333\n",
      "\n",
      "Train set: Average log loss: -7.4082\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2646 [0/90000 (0%)]\tlog Loss: -7.487301\n",
      "\n",
      "Train set: Average log loss: -7.4082\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2647 [0/90000 (0%)]\tlog Loss: -7.487414\n",
      "\n",
      "Train set: Average log loss: -7.4083\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2648 [0/90000 (0%)]\tlog Loss: -7.487490\n",
      "\n",
      "Train set: Average log loss: -7.4083\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2649 [0/90000 (0%)]\tlog Loss: -7.487449\n",
      "\n",
      "Train set: Average log loss: -7.4083\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2650 [0/90000 (0%)]\tlog Loss: -7.487493\n",
      "\n",
      "Train set: Average log loss: -7.4084\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2651 [0/90000 (0%)]\tlog Loss: -7.487568\n",
      "\n",
      "Train set: Average log loss: -7.4084\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2652 [0/90000 (0%)]\tlog Loss: -7.487568\n",
      "\n",
      "Train set: Average log loss: -7.4084\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2653 [0/90000 (0%)]\tlog Loss: -7.487646\n",
      "\n",
      "Train set: Average log loss: -7.4085\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2654 [0/90000 (0%)]\tlog Loss: -7.487594\n",
      "\n",
      "Train set: Average log loss: -7.4085\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2655 [0/90000 (0%)]\tlog Loss: -7.487716\n",
      "\n",
      "Train set: Average log loss: -7.4085\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2656 [0/90000 (0%)]\tlog Loss: -7.487786\n",
      "\n",
      "Train set: Average log loss: -7.4085\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2657 [0/90000 (0%)]\tlog Loss: -7.487736\n",
      "\n",
      "Train set: Average log loss: -7.4085\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2658 [0/90000 (0%)]\tlog Loss: -7.487765\n",
      "\n",
      "Train set: Average log loss: -7.4086\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2659 [0/90000 (0%)]\tlog Loss: -7.487823\n",
      "\n",
      "Train set: Average log loss: -7.4086\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2660 [0/90000 (0%)]\tlog Loss: -7.487864\n",
      "\n",
      "Train set: Average log loss: -7.4087\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2661 [0/90000 (0%)]\tlog Loss: -7.487853\n",
      "\n",
      "Train set: Average log loss: -7.4087\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4007\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2662 [0/90000 (0%)]\tlog Loss: -7.487987\n",
      "\n",
      "Train set: Average log loss: -7.4087\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4007\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2663 [0/90000 (0%)]\tlog Loss: -7.487839\n",
      "\n",
      "Train set: Average log loss: -7.4088\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4007\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2664 [0/90000 (0%)]\tlog Loss: -7.487952\n",
      "\n",
      "Train set: Average log loss: -7.4088\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2665 [0/90000 (0%)]\tlog Loss: -7.487980\n",
      "\n",
      "Train set: Average log loss: -7.4088\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2666 [0/90000 (0%)]\tlog Loss: -7.488049\n",
      "\n",
      "Train set: Average log loss: -7.4089\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4009\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2667 [0/90000 (0%)]\tlog Loss: -7.488001\n",
      "\n",
      "Train set: Average log loss: -7.4089\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2668 [0/90000 (0%)]\tlog Loss: -7.488094\n",
      "\n",
      "Train set: Average log loss: -7.4089\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4009\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2669 [0/90000 (0%)]\tlog Loss: -7.488096\n",
      "\n",
      "Train set: Average log loss: -7.4090\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4009\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2670 [0/90000 (0%)]\tlog Loss: -7.488150\n",
      "\n",
      "Train set: Average log loss: -7.4090\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2671 [0/90000 (0%)]\tlog Loss: -7.488110\n",
      "\n",
      "Train set: Average log loss: -7.4090\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2672 [0/90000 (0%)]\tlog Loss: -7.488208\n",
      "\n",
      "Train set: Average log loss: -7.4091\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2673 [0/90000 (0%)]\tlog Loss: -7.488242\n",
      "\n",
      "Train set: Average log loss: -7.4091\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2674 [0/90000 (0%)]\tlog Loss: -7.488253\n",
      "\n",
      "Train set: Average log loss: -7.4091\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4011\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2675 [0/90000 (0%)]\tlog Loss: -7.488316\n",
      "\n",
      "Train set: Average log loss: -7.4092\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4011\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2676 [0/90000 (0%)]\tlog Loss: -7.488373\n",
      "\n",
      "Train set: Average log loss: -7.4092\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2677 [0/90000 (0%)]\tlog Loss: -7.488351\n",
      "\n",
      "Train set: Average log loss: -7.4092\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2678 [0/90000 (0%)]\tlog Loss: -7.488421\n",
      "\n",
      "Train set: Average log loss: -7.4093\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4011\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2679 [0/90000 (0%)]\tlog Loss: -7.488380\n",
      "\n",
      "Train set: Average log loss: -7.4093\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2680 [0/90000 (0%)]\tlog Loss: -7.488460\n",
      "\n",
      "Train set: Average log loss: -7.4094\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4013\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2681 [0/90000 (0%)]\tlog Loss: -7.488506\n",
      "\n",
      "Train set: Average log loss: -7.4093\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2682 [0/90000 (0%)]\tlog Loss: -7.488553\n",
      "\n",
      "Train set: Average log loss: -7.4094\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4013\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2683 [0/90000 (0%)]\tlog Loss: -7.488525\n",
      "\n",
      "Train set: Average log loss: -7.4094\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4013\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2684 [0/90000 (0%)]\tlog Loss: -7.488619\n",
      "\n",
      "Train set: Average log loss: -7.4095\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2685 [0/90000 (0%)]\tlog Loss: -7.488649\n",
      "\n",
      "Train set: Average log loss: -7.4095\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2686 [0/90000 (0%)]\tlog Loss: -7.488624\n",
      "\n",
      "Train set: Average log loss: -7.4095\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2687 [0/90000 (0%)]\tlog Loss: -7.488661\n",
      "\n",
      "Train set: Average log loss: -7.4095\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2688 [0/90000 (0%)]\tlog Loss: -7.488735\n",
      "\n",
      "Train set: Average log loss: -7.4096\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2689 [0/90000 (0%)]\tlog Loss: -7.488712\n",
      "\n",
      "Train set: Average log loss: -7.4096\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2690 [0/90000 (0%)]\tlog Loss: -7.488731\n",
      "\n",
      "Train set: Average log loss: -7.4097\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4016\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2691 [0/90000 (0%)]\tlog Loss: -7.488816\n",
      "\n",
      "Train set: Average log loss: -7.4097\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4016\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2692 [0/90000 (0%)]\tlog Loss: -7.488822\n",
      "\n",
      "Train set: Average log loss: -7.4097\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2693 [0/90000 (0%)]\tlog Loss: -7.488909\n",
      "\n",
      "Train set: Average log loss: -7.4098\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2694 [0/90000 (0%)]\tlog Loss: -7.488834\n",
      "\n",
      "Train set: Average log loss: -7.4098\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2695 [0/90000 (0%)]\tlog Loss: -7.488880\n",
      "\n",
      "Train set: Average log loss: -7.4098\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2696 [0/90000 (0%)]\tlog Loss: -7.488930\n",
      "\n",
      "Train set: Average log loss: -7.4099\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2697 [0/90000 (0%)]\tlog Loss: -7.488958\n",
      "\n",
      "Train set: Average log loss: -7.4099\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2698 [0/90000 (0%)]\tlog Loss: -7.488915\n",
      "\n",
      "Train set: Average log loss: -7.4099\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2699 [0/90000 (0%)]\tlog Loss: -7.488980\n",
      "\n",
      "Train set: Average log loss: -7.4099\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2700 [0/90000 (0%)]\tlog Loss: -7.489092\n",
      "\n",
      "Train set: Average log loss: -7.4099\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2701 [0/90000 (0%)]\tlog Loss: -7.489002\n",
      "\n",
      "Train set: Average log loss: -7.4100\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4020\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2702 [0/90000 (0%)]\tlog Loss: -7.489109\n",
      "\n",
      "Train set: Average log loss: -7.4101\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2703 [0/90000 (0%)]\tlog Loss: -7.489101\n",
      "\n",
      "Train set: Average log loss: -7.4101\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4020\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2704 [0/90000 (0%)]\tlog Loss: -7.489266\n",
      "\n",
      "Train set: Average log loss: -7.4101\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4021\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2705 [0/90000 (0%)]\tlog Loss: -7.489186\n",
      "\n",
      "Train set: Average log loss: -7.4101\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4021\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2706 [0/90000 (0%)]\tlog Loss: -7.489222\n",
      "\n",
      "Train set: Average log loss: -7.4102\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2707 [0/90000 (0%)]\tlog Loss: -7.489207\n",
      "\n",
      "Train set: Average log loss: -7.4102\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2708 [0/90000 (0%)]\tlog Loss: -7.489365\n",
      "\n",
      "Train set: Average log loss: -7.4102\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2709 [0/90000 (0%)]\tlog Loss: -7.489337\n",
      "\n",
      "Train set: Average log loss: -7.4103\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2710 [0/90000 (0%)]\tlog Loss: -7.489310\n",
      "\n",
      "Train set: Average log loss: -7.4103\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4023\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2711 [0/90000 (0%)]\tlog Loss: -7.489337\n",
      "\n",
      "Train set: Average log loss: -7.4103\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2712 [0/90000 (0%)]\tlog Loss: -7.489365\n",
      "\n",
      "Train set: Average log loss: -7.4104\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2713 [0/90000 (0%)]\tlog Loss: -7.489506\n",
      "\n",
      "Train set: Average log loss: -7.4104\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2714 [0/90000 (0%)]\tlog Loss: -7.489455\n",
      "\n",
      "Train set: Average log loss: -7.4104\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2715 [0/90000 (0%)]\tlog Loss: -7.489523\n",
      "\n",
      "Train set: Average log loss: -7.4105\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2716 [0/90000 (0%)]\tlog Loss: -7.489624\n",
      "\n",
      "Train set: Average log loss: -7.4105\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2717 [0/90000 (0%)]\tlog Loss: -7.489558\n",
      "\n",
      "Train set: Average log loss: -7.4105\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2718 [0/90000 (0%)]\tlog Loss: -7.489661\n",
      "\n",
      "Train set: Average log loss: -7.4105\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2719 [0/90000 (0%)]\tlog Loss: -7.489637\n",
      "\n",
      "Train set: Average log loss: -7.4106\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2720 [0/90000 (0%)]\tlog Loss: -7.489723\n",
      "\n",
      "Train set: Average log loss: -7.4107\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2721 [0/90000 (0%)]\tlog Loss: -7.489740\n",
      "\n",
      "Train set: Average log loss: -7.4106\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2722 [0/90000 (0%)]\tlog Loss: -7.489753\n",
      "\n",
      "Train set: Average log loss: -7.4107\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2723 [0/90000 (0%)]\tlog Loss: -7.489845\n",
      "\n",
      "Train set: Average log loss: -7.4107\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2724 [0/90000 (0%)]\tlog Loss: -7.489868\n",
      "\n",
      "Train set: Average log loss: -7.4108\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2725 [0/90000 (0%)]\tlog Loss: -7.489990\n",
      "\n",
      "Train set: Average log loss: -7.4108\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2726 [0/90000 (0%)]\tlog Loss: -7.489852\n",
      "\n",
      "Train set: Average log loss: -7.4108\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2727 [0/90000 (0%)]\tlog Loss: -7.489900\n",
      "\n",
      "Train set: Average log loss: -7.4108\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2728 [0/90000 (0%)]\tlog Loss: -7.490044\n",
      "\n",
      "Train set: Average log loss: -7.4109\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2729 [0/90000 (0%)]\tlog Loss: -7.490097\n",
      "\n",
      "Train set: Average log loss: -7.4109\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2730 [0/90000 (0%)]\tlog Loss: -7.490032\n",
      "\n",
      "Train set: Average log loss: -7.4110\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2731 [0/90000 (0%)]\tlog Loss: -7.490122\n",
      "\n",
      "Train set: Average log loss: -7.4110\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2732 [0/90000 (0%)]\tlog Loss: -7.490191\n",
      "\n",
      "Train set: Average log loss: -7.4110\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2733 [0/90000 (0%)]\tlog Loss: -7.490153\n",
      "\n",
      "Train set: Average log loss: -7.4110\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2734 [0/90000 (0%)]\tlog Loss: -7.490265\n",
      "\n",
      "Train set: Average log loss: -7.4111\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2735 [0/90000 (0%)]\tlog Loss: -7.490346\n",
      "\n",
      "Train set: Average log loss: -7.4111\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2736 [0/90000 (0%)]\tlog Loss: -7.490267\n",
      "\n",
      "Train set: Average log loss: -7.4112\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2737 [0/90000 (0%)]\tlog Loss: -7.490344\n",
      "\n",
      "Train set: Average log loss: -7.4112\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2738 [0/90000 (0%)]\tlog Loss: -7.490417\n",
      "\n",
      "Train set: Average log loss: -7.4112\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2739 [0/90000 (0%)]\tlog Loss: -7.490405\n",
      "\n",
      "Train set: Average log loss: -7.4112\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2740 [0/90000 (0%)]\tlog Loss: -7.490405\n",
      "\n",
      "Train set: Average log loss: -7.4113\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2741 [0/90000 (0%)]\tlog Loss: -7.490580\n",
      "\n",
      "Train set: Average log loss: -7.4113\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2742 [0/90000 (0%)]\tlog Loss: -7.490554\n",
      "\n",
      "Train set: Average log loss: -7.4114\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2743 [0/90000 (0%)]\tlog Loss: -7.490486\n",
      "\n",
      "Train set: Average log loss: -7.4114\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2744 [0/90000 (0%)]\tlog Loss: -7.490576\n",
      "\n",
      "Train set: Average log loss: -7.4114\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2745 [0/90000 (0%)]\tlog Loss: -7.490638\n",
      "\n",
      "Train set: Average log loss: -7.4115\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4035\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2746 [0/90000 (0%)]\tlog Loss: -7.490687\n",
      "\n",
      "Train set: Average log loss: -7.4115\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4035\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2747 [0/90000 (0%)]\tlog Loss: -7.490742\n",
      "\n",
      "Train set: Average log loss: -7.4115\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4035\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2748 [0/90000 (0%)]\tlog Loss: -7.490694\n",
      "\n",
      "Train set: Average log loss: -7.4116\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2749 [0/90000 (0%)]\tlog Loss: -7.490837\n",
      "\n",
      "Train set: Average log loss: -7.4116\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2750 [0/90000 (0%)]\tlog Loss: -7.490874\n",
      "\n",
      "Train set: Average log loss: -7.4116\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2751 [0/90000 (0%)]\tlog Loss: -7.490806\n",
      "\n",
      "Train set: Average log loss: -7.4116\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2752 [0/90000 (0%)]\tlog Loss: -7.490884\n",
      "\n",
      "Train set: Average log loss: -7.4117\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2753 [0/90000 (0%)]\tlog Loss: -7.490972\n",
      "\n",
      "Train set: Average log loss: -7.4117\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2754 [0/90000 (0%)]\tlog Loss: -7.490937\n",
      "\n",
      "Train set: Average log loss: -7.4117\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2755 [0/90000 (0%)]\tlog Loss: -7.491066\n",
      "\n",
      "Train set: Average log loss: -7.4118\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2756 [0/90000 (0%)]\tlog Loss: -7.490968\n",
      "\n",
      "Train set: Average log loss: -7.4118\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2757 [0/90000 (0%)]\tlog Loss: -7.491103\n",
      "\n",
      "Train set: Average log loss: -7.4118\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2758 [0/90000 (0%)]\tlog Loss: -7.491119\n",
      "\n",
      "Train set: Average log loss: -7.4118\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2759 [0/90000 (0%)]\tlog Loss: -7.491072\n",
      "\n",
      "Train set: Average log loss: -7.4119\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2760 [0/90000 (0%)]\tlog Loss: -7.491150\n",
      "\n",
      "Train set: Average log loss: -7.4119\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2761 [0/90000 (0%)]\tlog Loss: -7.491245\n",
      "\n",
      "Train set: Average log loss: -7.4119\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2762 [0/90000 (0%)]\tlog Loss: -7.491174\n",
      "\n",
      "Train set: Average log loss: -7.4120\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2763 [0/90000 (0%)]\tlog Loss: -7.491311\n",
      "\n",
      "Train set: Average log loss: -7.4120\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2764 [0/90000 (0%)]\tlog Loss: -7.491308\n",
      "\n",
      "Train set: Average log loss: -7.4120\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2765 [0/90000 (0%)]\tlog Loss: -7.491396\n",
      "\n",
      "Train set: Average log loss: -7.4121\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2766 [0/90000 (0%)]\tlog Loss: -7.491372\n",
      "\n",
      "Train set: Average log loss: -7.4121\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2767 [0/90000 (0%)]\tlog Loss: -7.491341\n",
      "\n",
      "Train set: Average log loss: -7.4121\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2768 [0/90000 (0%)]\tlog Loss: -7.491417\n",
      "\n",
      "Train set: Average log loss: -7.4122\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2769 [0/90000 (0%)]\tlog Loss: -7.491355\n",
      "\n",
      "Train set: Average log loss: -7.4122\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2770 [0/90000 (0%)]\tlog Loss: -7.491563\n",
      "\n",
      "Train set: Average log loss: -7.4122\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2771 [0/90000 (0%)]\tlog Loss: -7.491461\n",
      "\n",
      "Train set: Average log loss: -7.4123\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2772 [0/90000 (0%)]\tlog Loss: -7.491565\n",
      "\n",
      "Train set: Average log loss: -7.4123\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2773 [0/90000 (0%)]\tlog Loss: -7.491530\n",
      "\n",
      "Train set: Average log loss: -7.4124\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2774 [0/90000 (0%)]\tlog Loss: -7.491539\n",
      "\n",
      "Train set: Average log loss: -7.4123\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2775 [0/90000 (0%)]\tlog Loss: -7.491709\n",
      "\n",
      "Train set: Average log loss: -7.4124\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2776 [0/90000 (0%)]\tlog Loss: -7.491655\n",
      "\n",
      "Train set: Average log loss: -7.4125\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2777 [0/90000 (0%)]\tlog Loss: -7.491709\n",
      "\n",
      "Train set: Average log loss: -7.4125\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2778 [0/90000 (0%)]\tlog Loss: -7.491746\n",
      "\n",
      "Train set: Average log loss: -7.4125\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2779 [0/90000 (0%)]\tlog Loss: -7.491772\n",
      "\n",
      "Train set: Average log loss: -7.4125\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4044\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2780 [0/90000 (0%)]\tlog Loss: -7.491816\n",
      "\n",
      "Train set: Average log loss: -7.4125\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2781 [0/90000 (0%)]\tlog Loss: -7.491756\n",
      "\n",
      "Train set: Average log loss: -7.4126\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2782 [0/90000 (0%)]\tlog Loss: -7.491877\n",
      "\n",
      "Train set: Average log loss: -7.4126\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4044\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2783 [0/90000 (0%)]\tlog Loss: -7.491836\n",
      "\n",
      "Train set: Average log loss: -7.4127\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4044\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2784 [0/90000 (0%)]\tlog Loss: -7.491896\n",
      "\n",
      "Train set: Average log loss: -7.4127\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4044\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2785 [0/90000 (0%)]\tlog Loss: -7.491898\n",
      "\n",
      "Train set: Average log loss: -7.4127\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2786 [0/90000 (0%)]\tlog Loss: -7.491984\n",
      "\n",
      "Train set: Average log loss: -7.4127\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2787 [0/90000 (0%)]\tlog Loss: -7.492005\n",
      "\n",
      "Train set: Average log loss: -7.4128\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2788 [0/90000 (0%)]\tlog Loss: -7.492062\n",
      "\n",
      "Train set: Average log loss: -7.4128\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2789 [0/90000 (0%)]\tlog Loss: -7.492122\n",
      "\n",
      "Train set: Average log loss: -7.4128\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2790 [0/90000 (0%)]\tlog Loss: -7.492160\n",
      "\n",
      "Train set: Average log loss: -7.4129\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2791 [0/90000 (0%)]\tlog Loss: -7.492137\n",
      "\n",
      "Train set: Average log loss: -7.4129\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2792 [0/90000 (0%)]\tlog Loss: -7.492169\n",
      "\n",
      "Train set: Average log loss: -7.4129\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2793 [0/90000 (0%)]\tlog Loss: -7.492228\n",
      "\n",
      "Train set: Average log loss: -7.4130\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2794 [0/90000 (0%)]\tlog Loss: -7.492275\n",
      "\n",
      "Train set: Average log loss: -7.4130\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2795 [0/90000 (0%)]\tlog Loss: -7.492286\n",
      "\n",
      "Train set: Average log loss: -7.4130\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2796 [0/90000 (0%)]\tlog Loss: -7.492275\n",
      "\n",
      "Train set: Average log loss: -7.4131\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2797 [0/90000 (0%)]\tlog Loss: -7.492386\n",
      "\n",
      "Train set: Average log loss: -7.4131\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2798 [0/90000 (0%)]\tlog Loss: -7.492368\n",
      "\n",
      "Train set: Average log loss: -7.4131\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2799 [0/90000 (0%)]\tlog Loss: -7.492451\n",
      "\n",
      "Train set: Average log loss: -7.4131\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2800 [0/90000 (0%)]\tlog Loss: -7.492520\n",
      "\n",
      "Train set: Average log loss: -7.4132\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4049\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2801 [0/90000 (0%)]\tlog Loss: -7.492481\n",
      "\n",
      "Train set: Average log loss: -7.4132\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4049\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2802 [0/90000 (0%)]\tlog Loss: -7.492545\n",
      "\n",
      "Train set: Average log loss: -7.4133\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4050\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2803 [0/90000 (0%)]\tlog Loss: -7.492569\n",
      "\n",
      "Train set: Average log loss: -7.4133\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4049\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2804 [0/90000 (0%)]\tlog Loss: -7.492553\n",
      "\n",
      "Train set: Average log loss: -7.4133\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4050\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2805 [0/90000 (0%)]\tlog Loss: -7.492557\n",
      "\n",
      "Train set: Average log loss: -7.4134\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4051\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2806 [0/90000 (0%)]\tlog Loss: -7.492709\n",
      "\n",
      "Train set: Average log loss: -7.4134\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4050\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2807 [0/90000 (0%)]\tlog Loss: -7.492670\n",
      "\n",
      "Train set: Average log loss: -7.4134\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4050\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2808 [0/90000 (0%)]\tlog Loss: -7.492701\n",
      "\n",
      "Train set: Average log loss: -7.4135\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2809 [0/90000 (0%)]\tlog Loss: -7.492678\n",
      "\n",
      "Train set: Average log loss: -7.4135\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4051\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2810 [0/90000 (0%)]\tlog Loss: -7.492754\n",
      "\n",
      "Train set: Average log loss: -7.4135\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2811 [0/90000 (0%)]\tlog Loss: -7.492775\n",
      "\n",
      "Train set: Average log loss: -7.4136\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2812 [0/90000 (0%)]\tlog Loss: -7.492891\n",
      "\n",
      "Train set: Average log loss: -7.4136\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2813 [0/90000 (0%)]\tlog Loss: -7.492807\n",
      "\n",
      "Train set: Average log loss: -7.4136\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4052\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2814 [0/90000 (0%)]\tlog Loss: -7.492945\n",
      "\n",
      "Train set: Average log loss: -7.4136\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4053\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2815 [0/90000 (0%)]\tlog Loss: -7.492864\n",
      "\n",
      "Train set: Average log loss: -7.4137\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4053\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2816 [0/90000 (0%)]\tlog Loss: -7.492979\n",
      "\n",
      "Train set: Average log loss: -7.4137\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4053\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2817 [0/90000 (0%)]\tlog Loss: -7.492935\n",
      "\n",
      "Train set: Average log loss: -7.4137\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4054\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2818 [0/90000 (0%)]\tlog Loss: -7.493045\n",
      "\n",
      "Train set: Average log loss: -7.4137\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4054\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2819 [0/90000 (0%)]\tlog Loss: -7.493070\n",
      "\n",
      "Train set: Average log loss: -7.4138\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4054\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2820 [0/90000 (0%)]\tlog Loss: -7.493091\n",
      "\n",
      "Train set: Average log loss: -7.4138\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4055\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2821 [0/90000 (0%)]\tlog Loss: -7.493194\n",
      "\n",
      "Train set: Average log loss: -7.4139\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4055\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2822 [0/90000 (0%)]\tlog Loss: -7.493064\n",
      "\n",
      "Train set: Average log loss: -7.4139\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4055\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2823 [0/90000 (0%)]\tlog Loss: -7.493166\n",
      "\n",
      "Train set: Average log loss: -7.4139\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2824 [0/90000 (0%)]\tlog Loss: -7.493296\n",
      "\n",
      "Train set: Average log loss: -7.4140\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4055\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2825 [0/90000 (0%)]\tlog Loss: -7.493240\n",
      "\n",
      "Train set: Average log loss: -7.4140\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2826 [0/90000 (0%)]\tlog Loss: -7.493286\n",
      "\n",
      "Train set: Average log loss: -7.4140\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2827 [0/90000 (0%)]\tlog Loss: -7.493357\n",
      "\n",
      "Train set: Average log loss: -7.4140\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4057\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2828 [0/90000 (0%)]\tlog Loss: -7.493385\n",
      "\n",
      "Train set: Average log loss: -7.4141\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2829 [0/90000 (0%)]\tlog Loss: -7.493350\n",
      "\n",
      "Train set: Average log loss: -7.4141\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4058\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2830 [0/90000 (0%)]\tlog Loss: -7.493404\n",
      "\n",
      "Train set: Average log loss: -7.4141\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4056\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2831 [0/90000 (0%)]\tlog Loss: -7.493492\n",
      "\n",
      "Train set: Average log loss: -7.4142\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4058\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2832 [0/90000 (0%)]\tlog Loss: -7.493473\n",
      "\n",
      "Train set: Average log loss: -7.4142\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4058\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2833 [0/90000 (0%)]\tlog Loss: -7.493585\n",
      "\n",
      "Train set: Average log loss: -7.4142\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4058\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2834 [0/90000 (0%)]\tlog Loss: -7.493480\n",
      "\n",
      "Train set: Average log loss: -7.4143\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4059\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2835 [0/90000 (0%)]\tlog Loss: -7.493595\n",
      "\n",
      "Train set: Average log loss: -7.4143\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4059\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2836 [0/90000 (0%)]\tlog Loss: -7.493680\n",
      "\n",
      "Train set: Average log loss: -7.4143\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4059\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2837 [0/90000 (0%)]\tlog Loss: -7.493654\n",
      "\n",
      "Train set: Average log loss: -7.4144\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4059\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2838 [0/90000 (0%)]\tlog Loss: -7.493623\n",
      "\n",
      "Train set: Average log loss: -7.4144\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4060\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2839 [0/90000 (0%)]\tlog Loss: -7.493767\n",
      "\n",
      "Train set: Average log loss: -7.4144\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4060\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2840 [0/90000 (0%)]\tlog Loss: -7.493781\n",
      "\n",
      "Train set: Average log loss: -7.4144\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4060\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2841 [0/90000 (0%)]\tlog Loss: -7.493720\n",
      "\n",
      "Train set: Average log loss: -7.4145\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4061\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2842 [0/90000 (0%)]\tlog Loss: -7.493804\n",
      "\n",
      "Train set: Average log loss: -7.4145\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4060\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2843 [0/90000 (0%)]\tlog Loss: -7.493827\n",
      "\n",
      "Train set: Average log loss: -7.4145\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4061\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2844 [0/90000 (0%)]\tlog Loss: -7.493874\n",
      "\n",
      "Train set: Average log loss: -7.4146\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4061\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2845 [0/90000 (0%)]\tlog Loss: -7.493947\n",
      "\n",
      "Train set: Average log loss: -7.4146\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4061\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2846 [0/90000 (0%)]\tlog Loss: -7.493971\n",
      "\n",
      "Train set: Average log loss: -7.4147\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4061\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2847 [0/90000 (0%)]\tlog Loss: -7.493935\n",
      "\n",
      "Train set: Average log loss: -7.4147\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4062\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2848 [0/90000 (0%)]\tlog Loss: -7.494021\n",
      "\n",
      "Train set: Average log loss: -7.4147\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4061\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2849 [0/90000 (0%)]\tlog Loss: -7.493962\n",
      "\n",
      "Train set: Average log loss: -7.4148\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4062\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2850 [0/90000 (0%)]\tlog Loss: -7.494056\n",
      "\n",
      "Train set: Average log loss: -7.4148\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4062\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2851 [0/90000 (0%)]\tlog Loss: -7.494059\n",
      "\n",
      "Train set: Average log loss: -7.4148\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4063\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2852 [0/90000 (0%)]\tlog Loss: -7.494129\n",
      "\n",
      "Train set: Average log loss: -7.4149\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4063\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2853 [0/90000 (0%)]\tlog Loss: -7.494138\n",
      "\n",
      "Train set: Average log loss: -7.4149\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4064\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2854 [0/90000 (0%)]\tlog Loss: -7.494250\n",
      "\n",
      "Train set: Average log loss: -7.4149\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4063\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2855 [0/90000 (0%)]\tlog Loss: -7.494091\n",
      "\n",
      "Train set: Average log loss: -7.4149\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4063\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2856 [0/90000 (0%)]\tlog Loss: -7.494169\n",
      "\n",
      "Train set: Average log loss: -7.4150\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4064\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2857 [0/90000 (0%)]\tlog Loss: -7.494305\n",
      "\n",
      "Train set: Average log loss: -7.4150\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4065\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2858 [0/90000 (0%)]\tlog Loss: -7.494291\n",
      "\n",
      "Train set: Average log loss: -7.4150\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4064\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2859 [0/90000 (0%)]\tlog Loss: -7.494214\n",
      "\n",
      "Train set: Average log loss: -7.4150\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4065\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2860 [0/90000 (0%)]\tlog Loss: -7.494358\n",
      "\n",
      "Train set: Average log loss: -7.4151\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4065\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2861 [0/90000 (0%)]\tlog Loss: -7.494469\n",
      "\n",
      "Train set: Average log loss: -7.4151\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4065\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2862 [0/90000 (0%)]\tlog Loss: -7.494407\n",
      "\n",
      "Train set: Average log loss: -7.4152\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4066\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2863 [0/90000 (0%)]\tlog Loss: -7.494468\n",
      "\n",
      "Train set: Average log loss: -7.4152\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4066\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2864 [0/90000 (0%)]\tlog Loss: -7.494515\n",
      "\n",
      "Train set: Average log loss: -7.4153\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4066\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2865 [0/90000 (0%)]\tlog Loss: -7.494600\n",
      "\n",
      "Train set: Average log loss: -7.4152\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4066\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2866 [0/90000 (0%)]\tlog Loss: -7.494518\n",
      "\n",
      "Train set: Average log loss: -7.4153\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4066\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2867 [0/90000 (0%)]\tlog Loss: -7.494558\n",
      "\n",
      "Train set: Average log loss: -7.4153\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4067\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2868 [0/90000 (0%)]\tlog Loss: -7.494625\n",
      "\n",
      "Train set: Average log loss: -7.4154\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4067\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2869 [0/90000 (0%)]\tlog Loss: -7.494610\n",
      "\n",
      "Train set: Average log loss: -7.4154\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4067\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2870 [0/90000 (0%)]\tlog Loss: -7.494601\n",
      "\n",
      "Train set: Average log loss: -7.4154\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4067\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2871 [0/90000 (0%)]\tlog Loss: -7.494669\n",
      "\n",
      "Train set: Average log loss: -7.4154\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4067\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2872 [0/90000 (0%)]\tlog Loss: -7.494689\n",
      "\n",
      "Train set: Average log loss: -7.4155\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4068\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2873 [0/90000 (0%)]\tlog Loss: -7.494653\n",
      "\n",
      "Train set: Average log loss: -7.4155\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4068\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2874 [0/90000 (0%)]\tlog Loss: -7.494764\n",
      "\n",
      "Train set: Average log loss: -7.4155\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4068\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2875 [0/90000 (0%)]\tlog Loss: -7.494787\n",
      "\n",
      "Train set: Average log loss: -7.4156\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4068\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2876 [0/90000 (0%)]\tlog Loss: -7.494932\n",
      "\n",
      "Train set: Average log loss: -7.4156\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4068\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2877 [0/90000 (0%)]\tlog Loss: -7.494891\n",
      "\n",
      "Train set: Average log loss: -7.4156\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4069\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2878 [0/90000 (0%)]\tlog Loss: -7.494903\n",
      "\n",
      "Train set: Average log loss: -7.4157\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4070\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2879 [0/90000 (0%)]\tlog Loss: -7.494905\n",
      "\n",
      "Train set: Average log loss: -7.4157\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4069\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2880 [0/90000 (0%)]\tlog Loss: -7.494870\n",
      "\n",
      "Train set: Average log loss: -7.4157\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4070\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2881 [0/90000 (0%)]\tlog Loss: -7.495038\n",
      "\n",
      "Train set: Average log loss: -7.4157\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4070\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2882 [0/90000 (0%)]\tlog Loss: -7.494943\n",
      "\n",
      "Train set: Average log loss: -7.4158\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4071\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2883 [0/90000 (0%)]\tlog Loss: -7.495125\n",
      "\n",
      "Train set: Average log loss: -7.4158\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4070\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2884 [0/90000 (0%)]\tlog Loss: -7.495131\n",
      "\n",
      "Train set: Average log loss: -7.4158\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4071\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2885 [0/90000 (0%)]\tlog Loss: -7.495150\n",
      "\n",
      "Train set: Average log loss: -7.4159\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4071\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2886 [0/90000 (0%)]\tlog Loss: -7.495034\n",
      "\n",
      "Train set: Average log loss: -7.4159\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4072\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2887 [0/90000 (0%)]\tlog Loss: -7.495139\n",
      "\n",
      "Train set: Average log loss: -7.4159\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4072\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2888 [0/90000 (0%)]\tlog Loss: -7.495102\n",
      "\n",
      "Train set: Average log loss: -7.4160\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4072\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2889 [0/90000 (0%)]\tlog Loss: -7.495204\n",
      "\n",
      "Train set: Average log loss: -7.4160\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4073\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2890 [0/90000 (0%)]\tlog Loss: -7.495379\n",
      "\n",
      "Train set: Average log loss: -7.4160\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4073\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2891 [0/90000 (0%)]\tlog Loss: -7.495243\n",
      "\n",
      "Train set: Average log loss: -7.4161\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4073\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2892 [0/90000 (0%)]\tlog Loss: -7.495236\n",
      "\n",
      "Train set: Average log loss: -7.4161\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4074\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2893 [0/90000 (0%)]\tlog Loss: -7.495341\n",
      "\n",
      "Train set: Average log loss: -7.4161\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4073\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2894 [0/90000 (0%)]\tlog Loss: -7.495345\n",
      "\n",
      "Train set: Average log loss: -7.4161\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4074\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2895 [0/90000 (0%)]\tlog Loss: -7.495368\n",
      "\n",
      "Train set: Average log loss: -7.4162\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4074\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2896 [0/90000 (0%)]\tlog Loss: -7.495440\n",
      "\n",
      "Train set: Average log loss: -7.4162\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4074\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2897 [0/90000 (0%)]\tlog Loss: -7.495455\n",
      "\n",
      "Train set: Average log loss: -7.4163\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4075\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2898 [0/90000 (0%)]\tlog Loss: -7.495502\n",
      "\n",
      "Train set: Average log loss: -7.4163\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4075\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2899 [0/90000 (0%)]\tlog Loss: -7.495559\n",
      "\n",
      "Train set: Average log loss: -7.4163\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4075\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2900 [0/90000 (0%)]\tlog Loss: -7.495544\n",
      "\n",
      "Train set: Average log loss: -7.4163\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4076\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2901 [0/90000 (0%)]\tlog Loss: -7.495634\n",
      "\n",
      "Train set: Average log loss: -7.4164\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4076\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2902 [0/90000 (0%)]\tlog Loss: -7.495624\n",
      "\n",
      "Train set: Average log loss: -7.4164\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4076\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2903 [0/90000 (0%)]\tlog Loss: -7.495727\n",
      "\n",
      "Train set: Average log loss: -7.4164\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4077\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2904 [0/90000 (0%)]\tlog Loss: -7.495632\n",
      "\n",
      "Train set: Average log loss: -7.4165\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4076\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2905 [0/90000 (0%)]\tlog Loss: -7.495681\n",
      "\n",
      "Train set: Average log loss: -7.4165\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4076\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2906 [0/90000 (0%)]\tlog Loss: -7.495682\n",
      "\n",
      "Train set: Average log loss: -7.4165\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4077\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2907 [0/90000 (0%)]\tlog Loss: -7.495736\n",
      "\n",
      "Train set: Average log loss: -7.4166\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4077\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2908 [0/90000 (0%)]\tlog Loss: -7.495799\n",
      "\n",
      "Train set: Average log loss: -7.4166\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4078\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2909 [0/90000 (0%)]\tlog Loss: -7.495872\n",
      "\n",
      "Train set: Average log loss: -7.4166\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4077\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2910 [0/90000 (0%)]\tlog Loss: -7.495916\n",
      "\n",
      "Train set: Average log loss: -7.4166\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4078\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2911 [0/90000 (0%)]\tlog Loss: -7.495801\n",
      "\n",
      "Train set: Average log loss: -7.4167\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4078\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2912 [0/90000 (0%)]\tlog Loss: -7.495952\n",
      "\n",
      "Train set: Average log loss: -7.4167\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4078\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2913 [0/90000 (0%)]\tlog Loss: -7.495861\n",
      "\n",
      "Train set: Average log loss: -7.4167\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2914 [0/90000 (0%)]\tlog Loss: -7.495989\n",
      "\n",
      "Train set: Average log loss: -7.4167\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2915 [0/90000 (0%)]\tlog Loss: -7.495955\n",
      "\n",
      "Train set: Average log loss: -7.4168\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2916 [0/90000 (0%)]\tlog Loss: -7.496001\n",
      "\n",
      "Train set: Average log loss: -7.4168\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2917 [0/90000 (0%)]\tlog Loss: -7.495977\n",
      "\n",
      "Train set: Average log loss: -7.4169\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4079\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2918 [0/90000 (0%)]\tlog Loss: -7.496122\n",
      "\n",
      "Train set: Average log loss: -7.4169\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4080\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2919 [0/90000 (0%)]\tlog Loss: -7.496079\n",
      "\n",
      "Train set: Average log loss: -7.4169\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4080\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2920 [0/90000 (0%)]\tlog Loss: -7.496182\n",
      "\n",
      "Train set: Average log loss: -7.4170\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4080\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2921 [0/90000 (0%)]\tlog Loss: -7.496152\n",
      "\n",
      "Train set: Average log loss: -7.4170\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4080\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2922 [0/90000 (0%)]\tlog Loss: -7.496166\n",
      "\n",
      "Train set: Average log loss: -7.4170\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4081\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2923 [0/90000 (0%)]\tlog Loss: -7.496197\n",
      "\n",
      "Train set: Average log loss: -7.4170\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4081\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2924 [0/90000 (0%)]\tlog Loss: -7.496209\n",
      "\n",
      "Train set: Average log loss: -7.4171\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4081\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2925 [0/90000 (0%)]\tlog Loss: -7.496191\n",
      "\n",
      "Train set: Average log loss: -7.4171\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4081\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2926 [0/90000 (0%)]\tlog Loss: -7.496293\n",
      "\n",
      "Train set: Average log loss: -7.4171\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4082\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2927 [0/90000 (0%)]\tlog Loss: -7.496245\n",
      "\n",
      "Train set: Average log loss: -7.4172\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4081\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2928 [0/90000 (0%)]\tlog Loss: -7.496388\n",
      "\n",
      "Train set: Average log loss: -7.4172\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4082\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2929 [0/90000 (0%)]\tlog Loss: -7.496343\n",
      "\n",
      "Train set: Average log loss: -7.4172\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4082\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2930 [0/90000 (0%)]\tlog Loss: -7.496351\n",
      "\n",
      "Train set: Average log loss: -7.4173\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4083\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2931 [0/90000 (0%)]\tlog Loss: -7.496426\n",
      "\n",
      "Train set: Average log loss: -7.4173\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4083\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2932 [0/90000 (0%)]\tlog Loss: -7.496443\n",
      "\n",
      "Train set: Average log loss: -7.4174\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4083\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2933 [0/90000 (0%)]\tlog Loss: -7.496423\n",
      "\n",
      "Train set: Average log loss: -7.4173\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2934 [0/90000 (0%)]\tlog Loss: -7.496505\n",
      "\n",
      "Train set: Average log loss: -7.4174\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2935 [0/90000 (0%)]\tlog Loss: -7.496470\n",
      "\n",
      "Train set: Average log loss: -7.4174\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2936 [0/90000 (0%)]\tlog Loss: -7.496605\n",
      "\n",
      "Train set: Average log loss: -7.4174\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2937 [0/90000 (0%)]\tlog Loss: -7.496496\n",
      "\n",
      "Train set: Average log loss: -7.4175\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2938 [0/90000 (0%)]\tlog Loss: -7.496633\n",
      "\n",
      "Train set: Average log loss: -7.4175\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4085\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2939 [0/90000 (0%)]\tlog Loss: -7.496634\n",
      "\n",
      "Train set: Average log loss: -7.4176\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2940 [0/90000 (0%)]\tlog Loss: -7.496634\n",
      "\n",
      "Train set: Average log loss: -7.4175\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4084\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2941 [0/90000 (0%)]\tlog Loss: -7.496530\n",
      "\n",
      "Train set: Average log loss: -7.4176\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4086\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2942 [0/90000 (0%)]\tlog Loss: -7.496703\n",
      "\n",
      "Train set: Average log loss: -7.4176\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4085\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2943 [0/90000 (0%)]\tlog Loss: -7.496630\n",
      "\n",
      "Train set: Average log loss: -7.4177\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4085\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2944 [0/90000 (0%)]\tlog Loss: -7.496729\n",
      "\n",
      "Train set: Average log loss: -7.4177\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4086\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2945 [0/90000 (0%)]\tlog Loss: -7.496750\n",
      "\n",
      "Train set: Average log loss: -7.4177\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4086\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2946 [0/90000 (0%)]\tlog Loss: -7.496669\n",
      "\n",
      "Train set: Average log loss: -7.4178\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4086\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2947 [0/90000 (0%)]\tlog Loss: -7.496887\n",
      "\n",
      "Train set: Average log loss: -7.4178\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4086\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2948 [0/90000 (0%)]\tlog Loss: -7.496752\n",
      "\n",
      "Train set: Average log loss: -7.4178\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4087\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2949 [0/90000 (0%)]\tlog Loss: -7.496863\n",
      "\n",
      "Train set: Average log loss: -7.4179\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4087\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2950 [0/90000 (0%)]\tlog Loss: -7.496832\n",
      "\n",
      "Train set: Average log loss: -7.4179\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4087\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2951 [0/90000 (0%)]\tlog Loss: -7.496896\n",
      "\n",
      "Train set: Average log loss: -7.4179\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4088\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2952 [0/90000 (0%)]\tlog Loss: -7.496903\n",
      "\n",
      "Train set: Average log loss: -7.4179\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4088\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2953 [0/90000 (0%)]\tlog Loss: -7.496957\n",
      "\n",
      "Train set: Average log loss: -7.4179\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4088\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2954 [0/90000 (0%)]\tlog Loss: -7.496946\n",
      "\n",
      "Train set: Average log loss: -7.4180\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4088\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2955 [0/90000 (0%)]\tlog Loss: -7.496964\n",
      "\n",
      "Train set: Average log loss: -7.4180\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2956 [0/90000 (0%)]\tlog Loss: -7.496998\n",
      "\n",
      "Train set: Average log loss: -7.4181\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2957 [0/90000 (0%)]\tlog Loss: -7.497088\n",
      "\n",
      "Train set: Average log loss: -7.4181\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2958 [0/90000 (0%)]\tlog Loss: -7.497023\n",
      "\n",
      "Train set: Average log loss: -7.4182\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4090\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2959 [0/90000 (0%)]\tlog Loss: -7.497163\n",
      "\n",
      "Train set: Average log loss: -7.4181\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2960 [0/90000 (0%)]\tlog Loss: -7.497136\n",
      "\n",
      "Train set: Average log loss: -7.4182\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2961 [0/90000 (0%)]\tlog Loss: -7.497211\n",
      "\n",
      "Train set: Average log loss: -7.4183\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4091\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2962 [0/90000 (0%)]\tlog Loss: -7.497211\n",
      "\n",
      "Train set: Average log loss: -7.4182\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4089\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2963 [0/90000 (0%)]\tlog Loss: -7.497111\n",
      "\n",
      "Train set: Average log loss: -7.4183\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4090\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2964 [0/90000 (0%)]\tlog Loss: -7.497284\n",
      "\n",
      "Train set: Average log loss: -7.4183\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4091\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2965 [0/90000 (0%)]\tlog Loss: -7.497186\n",
      "\n",
      "Train set: Average log loss: -7.4183\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4091\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2966 [0/90000 (0%)]\tlog Loss: -7.497290\n",
      "\n",
      "Train set: Average log loss: -7.4184\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4091\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2967 [0/90000 (0%)]\tlog Loss: -7.497361\n",
      "\n",
      "Train set: Average log loss: -7.4184\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4091\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2968 [0/90000 (0%)]\tlog Loss: -7.497281\n",
      "\n",
      "Train set: Average log loss: -7.4184\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4092\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2969 [0/90000 (0%)]\tlog Loss: -7.497379\n",
      "\n",
      "Train set: Average log loss: -7.4185\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4092\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2970 [0/90000 (0%)]\tlog Loss: -7.497373\n",
      "\n",
      "Train set: Average log loss: -7.4185\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4092\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2971 [0/90000 (0%)]\tlog Loss: -7.497417\n",
      "\n",
      "Train set: Average log loss: -7.4185\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4092\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2972 [0/90000 (0%)]\tlog Loss: -7.497483\n",
      "\n",
      "Train set: Average log loss: -7.4186\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4092\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2973 [0/90000 (0%)]\tlog Loss: -7.497394\n",
      "\n",
      "Train set: Average log loss: -7.4186\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4093\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2974 [0/90000 (0%)]\tlog Loss: -7.497437\n",
      "\n",
      "Train set: Average log loss: -7.4186\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4093\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2975 [0/90000 (0%)]\tlog Loss: -7.497451\n",
      "\n",
      "Train set: Average log loss: -7.4187\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4093\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2976 [0/90000 (0%)]\tlog Loss: -7.497549\n",
      "\n",
      "Train set: Average log loss: -7.4187\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4093\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2977 [0/90000 (0%)]\tlog Loss: -7.497439\n",
      "\n",
      "Train set: Average log loss: -7.4187\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4094\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2978 [0/90000 (0%)]\tlog Loss: -7.497538\n",
      "\n",
      "Train set: Average log loss: -7.4188\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4094\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2979 [0/90000 (0%)]\tlog Loss: -7.497678\n",
      "\n",
      "Train set: Average log loss: -7.4188\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4094\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2980 [0/90000 (0%)]\tlog Loss: -7.497600\n",
      "\n",
      "Train set: Average log loss: -7.4188\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4095\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2981 [0/90000 (0%)]\tlog Loss: -7.497737\n",
      "\n",
      "Train set: Average log loss: -7.4188\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4095\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2982 [0/90000 (0%)]\tlog Loss: -7.497676\n",
      "\n",
      "Train set: Average log loss: -7.4189\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4094\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2983 [0/90000 (0%)]\tlog Loss: -7.497718\n",
      "\n",
      "Train set: Average log loss: -7.4189\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4095\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2984 [0/90000 (0%)]\tlog Loss: -7.497808\n",
      "\n",
      "Train set: Average log loss: -7.4189\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4095\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2985 [0/90000 (0%)]\tlog Loss: -7.497786\n",
      "\n",
      "Train set: Average log loss: -7.4189\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4095\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2986 [0/90000 (0%)]\tlog Loss: -7.497739\n",
      "\n",
      "Train set: Average log loss: -7.4190\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4096\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2987 [0/90000 (0%)]\tlog Loss: -7.497829\n",
      "\n",
      "Train set: Average log loss: -7.4190\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4096\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2988 [0/90000 (0%)]\tlog Loss: -7.497845\n",
      "\n",
      "Train set: Average log loss: -7.4190\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4097\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2989 [0/90000 (0%)]\tlog Loss: -7.497944\n",
      "\n",
      "Train set: Average log loss: -7.4191\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4097\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2990 [0/90000 (0%)]\tlog Loss: -7.497865\n",
      "\n",
      "Train set: Average log loss: -7.4191\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4097\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2991 [0/90000 (0%)]\tlog Loss: -7.497961\n",
      "\n",
      "Train set: Average log loss: -7.4192\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4098\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2992 [0/90000 (0%)]\tlog Loss: -7.497989\n",
      "\n",
      "Train set: Average log loss: -7.4192\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4096\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2993 [0/90000 (0%)]\tlog Loss: -7.498017\n",
      "\n",
      "Train set: Average log loss: -7.4192\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4098\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2994 [0/90000 (0%)]\tlog Loss: -7.498049\n",
      "\n",
      "Train set: Average log loss: -7.4192\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4098\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2995 [0/90000 (0%)]\tlog Loss: -7.498019\n",
      "\n",
      "Train set: Average log loss: -7.4192\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4098\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2996 [0/90000 (0%)]\tlog Loss: -7.498083\n",
      "\n",
      "Train set: Average log loss: -7.4193\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4099\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2997 [0/90000 (0%)]\tlog Loss: -7.498154\n",
      "\n",
      "Train set: Average log loss: -7.4193\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4098\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2998 [0/90000 (0%)]\tlog Loss: -7.498139\n",
      "\n",
      "Train set: Average log loss: -7.4194\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4099\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2999 [0/90000 (0%)]\tlog Loss: -7.498143\n",
      "\n",
      "Train set: Average log loss: -7.4194\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4099\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 3000 [0/90000 (0%)]\tlog Loss: -7.498239\n",
      "\n",
      "Train set: Average log loss: -7.4194\n",
      "\n",
      "\n",
      "Test set: Average loss: -7.4099\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "args =  {\"batch_size\": 1024,\n",
    "         \"test_batch_size\": 4048,\n",
    "         \"epochs\" : 3*10**3,\n",
    "         \"lr\": 1e-4,\n",
    "         \"gamma\": .1,\n",
    "         \"no_cuda\" : False,\n",
    "         \"run_dry\": False,\n",
    "         \"seed\": 0,\n",
    "         \"log_interval\" : 100,\n",
    "         \"dry_run\" : False,\n",
    "         \"save_model\": True}\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "\n",
    "# Train / Test Data\n",
    "bs_dataset = IV_LHS_data_generator(n = 10**5, log_scale = False)\n",
    "train_size, test_size = int(bs_dataset.shape[0]*0.9), int(bs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(bs_dataset, [train_size, bs_dataset.shape[0]-train_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])\n",
    "\n",
    "\n",
    "# Model training\n",
    "print(device)\n",
    "model = BS_ANN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"]) # Adam is found to be the best optimizer in the article\n",
    "scheduler = StepLR(optimizer, step_size=10**3, gamma=args[\"gamma\"]) # Deacreses the Learning rate by .1 every 1000 epoch\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "  train_loss = model.train_model(args, device, train_loader, optimizer, epoch)\n",
    "  test_loss = model.test_model(device, test_loader)\n",
    "  print(\"Current Learning rate {}\".format(scheduler.get_last_lr()[0]))\n",
    "  scheduler.step()\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "if args[\"save_model\"] :\n",
    "  torch.save(model.state_dict(), \"IV_UNSCALED_ANN.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZDnGUuxOMTE",
    "outputId": "f93a95bb-f8e5-4180-b0f6-6d2809a430dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IV_ANN(\n",
       "  (fc1): Linear(in_features=4, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc3): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc4): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc5): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model\n",
    "device = torch.device(\"cuda\")\n",
    "loaded_IV_ANN_UN = IV_ANN().to(device)\n",
    "PATH = \"/content/IV_UNSCALED_ANN.pt\"\n",
    "loaded_IV_ANN_UN.load_state_dict(torch.load(PATH))\n",
    "loaded_IV_ANN_UN.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "cOJNP-He2BOE",
    "outputId": "4244f22e-0c25-4cd0-dac9-8a0fe2091df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX  0.3485536\n",
      "MIN  -0.39093545\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting erros histogram\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_IV_ANN_UN(data)\n",
    "    errors.append((target-output).data.cpu().numpy())\n",
    "  errors = np.concatenate(errors)\n",
    "plt.figure(figsize=(10,7))\n",
    "h = plt.hist(errors,100)\n",
    "plt.grid()\n",
    "print(\"MAX \",errors.max())\n",
    "print(\"MIN \",errors.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH9jjl-OKeir",
    "outputId": "7eb48466-a7f2-4402-f818-f64f09b41c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Train scores----------------\n",
      "MSE 6.182165e-04\n",
      "MAE 8.109048e-03\n",
      "MAPE 3.735277e-02\n",
      "R2 9.915911e-01\n",
      "-----------Test scores----------------\n",
      "MSE 7.214432e-04\n",
      "MAE 8.542195e-03\n",
      "MAPE 3.846751e-02\n",
      "R2 9.903062e-01\n"
     ]
    }
   ],
   "source": [
    "# Printing train and test MSE, MAE, MAPE and R2\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_IV_ANN_UN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Train scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )\n",
    "\n",
    "\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_IV_ANN_UN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Test scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3QMaOLPPwux"
   },
   "source": [
    "# 3) COS Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBQ-dVz4DFUh"
   },
   "source": [
    "#### Vanilla call coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEFZmb6MBYVq"
   },
   "source": [
    "$\\begin{aligned} \\chi_{k}(c, d):=& \\frac{1}{1+\\left(\\frac{k \\pi}{b-a}\\right)^{2}}\\left[\\cos \\left(k \\pi \\frac{d-a}{b-a}\\right) e^{d}-\\cos \\left(k \\pi \\frac{c-a}{b-a}\\right) e^{c}\\right.\\\\ &\\left.+\\frac{k \\pi}{b-a} \\sin \\left(k \\pi \\frac{d-a}{b-a}\\right) e^{d}-\\frac{k \\pi}{b-a} \\sin \\left(k \\pi \\frac{c-a}{b-a}\\right) e^{c}\\right] \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UYjpY3wBabR"
   },
   "source": [
    "$\\psi_{k}(c, d):= \\begin{cases}{\\left[\\sin \\left(k \\pi \\frac{d-a}{b-a}\\right)-\\sin \\left(k \\pi \\frac{c-a}{b-a}\\right)\\right] \\frac{b-a}{k \\pi},} & k \\neq 0, \\\\ (d-c), & k=0 .\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aP1rhLNBrIm"
   },
   "source": [
    "$V_{k}^{\\text {call }}=\\frac{2}{b-a} K\\left(\\chi_{k}(0, b)-\\psi_{k}(0, b)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zjZDCl9_SeJ"
   },
   "outputs": [],
   "source": [
    "def call_cosine_coef(a, b, c, d, K, N):\n",
    "  \"\"\"\n",
    "  Coefficient V_k for plain vanilla options. \n",
    "  Section 3.1, equations (22) and (23).\n",
    "  a, b, c, d: truncation range parameters.\n",
    "  K: Strike.\n",
    "  N: summing lenght.\n",
    "  Returns V_k coefficient for call option.\n",
    "  \"\"\"\n",
    "  k = np.arange(N).reshape(1,-1)\n",
    "  bma    = b - a\n",
    "  uu     = k * np.pi / bma\n",
    "  chi_k = 1 / (1 + uu **2 ) * \\\n",
    "  ( np.cos(uu * ( d - a))* np.exp(d) - np.cos(uu * (c - a))* np.exp(c) + uu * np.sin(uu * (d - a)) * np.exp(d)- uu * np.sin(uu*(c-a))* np.exp(c) )\n",
    "  psi_k = uu\n",
    "  psi_k[:,1:] = 1 / uu[:,1:] * ( np.sin(uu[:,1:] * ( d - a )) - np.sin( uu[:,1:] * ( c - a) ) )\n",
    "  psi_k[:,0] = (d - c).flatten() #d - c \n",
    "  return 2/bma*K*(chi_k-psi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsi05vUVbzTK"
   },
   "outputs": [],
   "source": [
    "def call_cosine_coef_moy(a, b, c, d, N):\n",
    "  \"\"\"\n",
    "  Moyeness version.\n",
    "  Coefficient V_k for plain vanilla options. \n",
    "  Section 3.1, equations (22) and (23).\n",
    "  a, b, c, d: truncation range parameters.\n",
    "  N: summing lenght.\n",
    "  Returns V_k coefficient for call option devided by K.\n",
    "  \"\"\"\n",
    "  k = np.arange(N).reshape(1,-1)\n",
    "  bma    = b - a\n",
    "  uu     = k * np.pi / bma\n",
    "  chi_k = 1 / (1 + uu **2 ) * \\\n",
    "  ( np.cos(uu * ( d - a))* np.exp(d) - np.cos(uu * (c - a))* np.exp(c) + uu * np.sin(uu * (d - a)) * np.exp(d)- uu * np.sin(uu*(c-a))* np.exp(c) )\n",
    "  psi_k = uu\n",
    "  psi_k[:,1:] = 1 / uu[:,1:] * ( np.sin(uu[:,1:] * ( d - a )) - np.sin( uu[:,1:] * ( c - a) ) )\n",
    "  psi_k[:,0] = (d - c).flatten() #d - c \n",
    "  return 2/bma*(chi_k-psi_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py6auAo1DAq3"
   },
   "source": [
    "#### Charateristic function for Heston model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgOVOvZUB2jG"
   },
   "source": [
    "$\\begin{aligned} \\varphi_{h e s}\\left(\\omega ; u_{0}\\right)=& \\exp \\left(i \\omega \\mu \\Delta t+\\frac{u_{0}}{\\eta^{2}}\\left(\\frac{1-e^{-D \\Delta t}}{1-G e^{-D \\Delta t}}\\right)(\\lambda-i \\rho \\eta \\omega-D)\\right) \\\\ & \\cdot \\exp \\left(\\frac{\\lambda \\bar{u}}{\\eta^{2}}\\left(\\Delta t(\\lambda-i \\rho \\eta \\omega-D)-2 \\log \\left(\\frac{1-G e^{-D \\Delta t}}{1-G}\\right)\\right)\\right) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Wi_OJIB3o4"
   },
   "source": [
    "$D=\\sqrt{(\\lambda-i \\rho \\eta \\omega)^{2}+\\left(\\omega^{2}+i \\omega\\right) \\eta^{2}} \\quad$ and $\\quad G=\\frac{\\lambda-i \\rho \\eta \\omega-D}{\\lambda-i \\rho \\eta \\omega+D} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwbqZhhC_Se9"
   },
   "outputs": [],
   "source": [
    "def phi_Heston(a ,b ,N ,r, tau, kappa, gamma, vbar, v0, rho):\n",
    "  \"\"\"\n",
    "  Characteristic function of the log-asset price following the Heston model.\n",
    "  Section 3.3, equation (33).\n",
    "  a, b: truncation range parameters.\n",
    "  N: summing lenght.\n",
    "  r: risk free rate.\n",
    "  tau: time to maturity.\n",
    "  kappa: reversion speed .\n",
    "  gamma: Vol of vol.\n",
    "  vbar: Long average variance.\n",
    "  v0: Initial variance.\n",
    "  rho: Correlation.\n",
    "  Returns Charateristic function vector.\n",
    "  \"\"\"\n",
    "  k = np.arange(N).reshape(1,-1)\n",
    "  u = np.pi*k/(b-a)\n",
    "  i = complex(0.0,1.0)\n",
    "  D = np.sqrt((kappa-gamma*rho*i*u)**2+(u**2+i*u)*gamma**2)\n",
    "  G = (kappa-gamma*rho*i*u-D)/(kappa-gamma*rho*i*u+D)\n",
    "  C = v0* (1-np.exp(-D*tau)) / (gamma**2*(1-G*np.exp(-D*tau)))*(kappa-gamma*rho*i*u-D)\n",
    "  A = r*i*u*tau + kappa*vbar/(gamma**2)*(tau*(kappa-gamma*rho*i*u-D) -2*np.log( (1-G*np.exp(-D*tau))/(1-G) )   ) \n",
    "  phi = np.exp( A + C )\n",
    "  return phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF6h1RohC9KX"
   },
   "source": [
    "#### Heston cumulants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyNS4d1TCIBI"
   },
   "source": [
    "$c_{1}=\\mu T+\\left(1-e^{-\\lambda T}\\right) \\frac{\\bar{u}-u_{0}}{2 \\lambda}-\\frac{1}{2} \\bar{u} T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIcIQxmcCRDU"
   },
   "source": [
    "$\\begin{aligned} c_{2}=& \\frac{1}{8 \\lambda^{3}}\\left(\\eta T \\lambda e^{-\\lambda T}\\left(u_{0}-\\bar{u}\\right)(8 \\lambda \\rho-4 \\eta)\\right.\\\\ &+\\lambda \\rho \\eta\\left(1-e^{-\\lambda T}\\right)\\left(16 \\bar{u}-8 u_{0}\\right) \\\\ &+2 \\bar{u} \\lambda T\\left(-4 \\lambda \\rho \\eta+\\eta^{2}+4 \\lambda^{2}\\right) \\\\ &+\\eta^{2}\\left(\\left(\\bar{u}-2 u_{0}\\right) e^{-2 \\lambda T}+\\bar{u}\\left(6 e^{-\\lambda T}-7\\right)+2 u_{0}\\right) \\\\ &\\left.+8 \\lambda^{2}\\left(u_{0}-\\bar{u}\\right)\\left(1-e^{-\\lambda T}\\right)\\right) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTCdkZDZ_Sid"
   },
   "outputs": [],
   "source": [
    "def Heston_cumulants( r, tau, kappa, gamma, vbar, v0, rho ):\n",
    "  \"\"\"\n",
    "  Computes Heston cumulants as defined in the appendix A for the Heston model.\n",
    "  r: risk free rate.\n",
    "  tau: time to maturity.\n",
    "  kappa: reversion speed .\n",
    "  gamma: Vol of vol.\n",
    "  vbar: Long average variance.\n",
    "  v0: Initial variance.\n",
    "  rho: Correlation.\n",
    "  Returns cumulants c1 and c2.\n",
    "  \"\"\"\n",
    "  c1 = r * tau + (1 - np.exp(- kappa * tau)) * (vbar - v0) / (2 * kappa) - 0.5 * vbar * tau\n",
    "  \n",
    "  c2 = 0.125 / kappa ** 3 * (gamma * tau * np.exp(- kappa * tau) * (v0 - vbar) * (8 * kappa * rho - 4 * gamma) + \\\n",
    "     kappa * rho * gamma * (1 - np.exp(- kappa * tau)) * (16 * vbar - 8 * v0) + \\\n",
    "     2 * vbar * kappa * tau * (- 4 * kappa * rho * gamma + gamma ** 2 + 4 * kappa ** 2) + \\\n",
    "     gamma ** 2 * ((vbar - 2 * v0) * np.exp(-2 * kappa * tau) + vbar * (6 * np.exp(- kappa * tau) - 7) + 2 * v0) + \\\n",
    "     8 * kappa ** 2 * (v0 - vbar) * (1 - np.exp(-kappa * tau)))\n",
    "  return c1, c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fO7JQlDDNcW"
   },
   "source": [
    "#### Truncation range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5Uqcq59ChoT"
   },
   "source": [
    "$[a, b]:=\\left[c_{1}-L \\sqrt{c_{2}+\\sqrt{c_{4}}}, \\quad c_{1}+L \\sqrt{c_{2}+\\sqrt{c_{4}}}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjrTS6X1FqqF"
   },
   "outputs": [],
   "source": [
    "def Truncation_range(c1, c2, c4, L):\n",
    "  \"\"\"\n",
    "  Computes the truncation range given the cumulants c1, c2, c4 and the truncation lenght L.\n",
    "  c1, c2, c4: Model cumulants.\n",
    "  L: truncation lenght.\n",
    "  Returns tuncation interval sup b and inf a.\n",
    "  \"\"\"\n",
    "  a = c1 - L * np.sqrt( np.abs( c2 ) + np.sqrt( np.abs(c4) ) )\n",
    "  b = c1 + L * np.sqrt( np.abs( c2 ) + np.sqrt( np.abs(c4) ) ) \n",
    "  return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehFOGXCpDPyX"
   },
   "source": [
    "#### Exponential coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZul_Sr-Cof5"
   },
   "source": [
    "$e^{i k \\pi \\frac{\\mathbf{x}-a}{b-a}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2emRWahI3bJ"
   },
   "outputs": [],
   "source": [
    "def exponential_coef(a, b, N, S0, K):\n",
    "  \"\"\"\n",
    "  Computes the exponential coefficient used in the final pricing sum.\n",
    "  a, b: truncation range.\n",
    "  N: sum lenght.\n",
    "  S0: asset initial price.\n",
    "  K: strike.\n",
    "  Returns the exponential coefficient vector.\n",
    "  \"\"\"\n",
    "  i = complex(0.0,1.0)\n",
    "  k = np.arange(N).reshape(1,-1)\n",
    "  return np.exp(i*k*np.pi*(np.log(S0/K)-a)/(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLSrDystbZ6F"
   },
   "outputs": [],
   "source": [
    "def exponential_coef_moy(a, b, N, moyeness):\n",
    "  \"\"\"\n",
    "  Moyeness version.\n",
    "  Computes the exponential coefficient used in the final pricing sum.\n",
    "  a, b: truncation range.\n",
    "  N: sum lenght.\n",
    "  moyeness: S/K.\n",
    "  Returns the exponential coefficient vector devided by K.\n",
    "  \"\"\"\n",
    "  i = complex(0.0,1.0)\n",
    "  k = np.arange(N).reshape(1,-1)\n",
    "  return np.exp(i*k*np.pi*(np.log(moyeness)-a)/(b-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfm4WDluDS_2"
   },
   "source": [
    "#### Vanilla call price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEAn4EYbCttc"
   },
   "source": [
    "$v\\left(\\mathbf{x}, t_{0}, u_{0}\\right) \\approx \\mathbf{K} e^{-r \\Delta t} \\cdot \\operatorname{Re}\\left\\{\\sum_{k=0}^{N-1} \\varphi_{h e s}\\left(\\frac{k \\pi}{b-a} ; u_{0}\\right) U_{k} \\cdot e^{i k \\pi \\frac{\\mathbf{x}-a}{b-a}}\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgdAg_ts_SjW"
   },
   "outputs": [],
   "source": [
    "def call_option_Heston(N, L, r, tau, kappa, gamma, vbar, v0, rho, K, S0 ):\n",
    "  \"\"\"\n",
    "  Computes call option price using COS method.\n",
    "  N: sum lenght.\n",
    "  L: truncation lenght.\n",
    "  r: risk free rate.\n",
    "  tau: time to maturity.\n",
    "  kappa: reversion speed .\n",
    "  gamma: Vol of vol.\n",
    "  vbar: Long average variance.\n",
    "  v0: Initial variance.\n",
    "  rho: Correlation.\n",
    "  S0: asset initial price.\n",
    "  K: strike.\n",
    "  Returns option price.\n",
    "  \"\"\"\n",
    "  c1, c2 = Heston_cumulants( r, tau, kappa, gamma, vbar, v0, rho )\n",
    "  c4 = 0\n",
    "  a, b = Truncation_range(c1, c2, c4, L)\n",
    "  c= 0\n",
    "  d = b\n",
    "  V_k = call_cosine_coef(a, b, c, d, K, N)\n",
    "  phi_k = phi_Heston(a, b, N, r, tau, kappa, gamma, vbar, v0, rho)\n",
    "  product_k = V_k * phi_k * exponential_coef(a, b, N, S0, K)\n",
    "  product_k[:,0] = .5 * product_k[:,0]\n",
    "  return np.exp(-r*tau)*(np.real(  np.sum( product_k,1 )  )).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vnE4AhoTPY5",
    "outputId": "175e8b15-8109-49a6-faf1-a3c1a4217b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[29.79261082]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheking the function\n",
    "N = 500\n",
    "L= 12\n",
    "r= .1\n",
    "tau= 1.4\n",
    "rho = -0.95\n",
    "kappa= 1.4\n",
    "vbar = .1\n",
    "gamma = .1\n",
    "v0 = 0.5\n",
    "K = 100\n",
    "S0 = 100\n",
    "# Feller's condition\n",
    "print(2*kappa*vbar>gamma**2)\n",
    "call_option_Heston(N = 1500, L= 12, r= r, tau= tau, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, K = K, S0 = S0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4xZ6hb-bgph"
   },
   "outputs": [],
   "source": [
    "def call_option_Heston_moy(N, L, r, tau, kappa, gamma, vbar, v0, rho, moyeness ):\n",
    "  \"\"\"\n",
    "  Moyeness version.\n",
    "  Computes call option price using COS method.\n",
    "  N: sum lenght.\n",
    "  L: truncation lenght.\n",
    "  r: risk free rate.\n",
    "  tau: time to maturity.\n",
    "  kappa: reversion speed .\n",
    "  gamma: Vol of vol.\n",
    "  vbar: Long average variance.\n",
    "  v0: Initial variance.\n",
    "  rho: Correlation.\n",
    "  moyeness: S/K.\n",
    "  Returns option price.\n",
    "  \"\"\"\n",
    "  c1, c2 = Heston_cumulants( r, tau, kappa, gamma, vbar, v0, rho )\n",
    "  c4 = 0\n",
    "  a, b = Truncation_range(c1, c2, c4, L)\n",
    "  c= 0\n",
    "  d = b\n",
    "  V_k = call_cosine_coef_moy(a, b, c, d, N)\n",
    "  phi_k = phi_Heston(a, b, N, r, tau, kappa, gamma, vbar, v0, rho)\n",
    "  product_k = V_k * phi_k * exponential_coef_moy(a, b, N, moyeness)\n",
    "  product_k[:,0] = .5 * product_k[:,0]\n",
    "  return np.exp(-r*tau)*(np.real(  np.sum( product_k,1 )  )).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jm7u10L0cDYL",
    "outputId": "c896e876-d916-441a-c8a6-4348e918a528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.29792611]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheking the function\n",
    "N = 500\n",
    "L= 12\n",
    "r= .1\n",
    "tau= 1.4\n",
    "rho = -0.95\n",
    "kappa= 1.4\n",
    "vbar = .1\n",
    "gamma = .1\n",
    "v0 = 0.5\n",
    "K = 100\n",
    "S0 = 100\n",
    "# Feller's condition\n",
    "print(2*kappa*vbar>gamma**2)\n",
    "call_option_Heston_moy(N = 1500, L= 12, r= r, tau= tau, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, moyeness = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aCVlS1x_RIt"
   },
   "source": [
    "# 4) Heston pricing network: Heston-ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JZsZfYFUDjZ"
   },
   "source": [
    "### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZC85E7uUGor",
    "outputId": "17b0b197-5a7c-425f-acbe-bffc2f996333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ INPUT ------\n",
      "S/K.......... [0.61 1.39]\n",
      "Tau.......... [0.11 1.39]\n",
      "r............ [0.01 0.09]\n",
      "Corr ........ [-0.94 -0.01]\n",
      "Kappa........ [0.41 1.99]\n",
      "V_bar........ [0.01 0.49]\n",
      "Gamma........ [0.01 0.49]\n",
      "V_0 ......... [0.06 0.49]\n",
      "------ OUTPUT ------\n",
      "V/K.......... [-0.00 0.64]\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-2 # is used to generate open intervals\n",
    "def Heston_LHS_data_generator(n = 10**6, l_bounds = [.6+eps, .1+eps, .0+eps, -0.95+eps, .4+eps, .0+eps, .0+eps, .05+eps], u_bounds = [1.4-eps, 1.4-eps, .1-eps, 0.0-eps, 2.0-eps, .5-eps, .5-eps, .5-eps]):\n",
    "  \"\"\"\n",
    "  Generates samples of call prices using Latin hypercube sampling.\n",
    "  Returns a torch float tensor of dimension (n,9) containig the inputs samples and the value of the call.\n",
    "  The inputs are: Moyeness (S/K), time to maturity (tau), Risk free rate (r), Correlation (rho), reversion speed (kappa), Long average variance (vbar), Vol of vol (gamma), Initial variance (v0).\n",
    "  n: number of samples.\n",
    "  l_bounds: lower bound for the inputs.  \n",
    "  u_bounds: upper bound for the inputs.\n",
    "  \"\"\"\n",
    "  sampler = qmc.LatinHypercube(d=8) #, seed = 0\n",
    "  sample = sampler.random(n)\n",
    "  sample = qmc.scale(sample, l_bounds, u_bounds)\n",
    "  indecies = np.argwhere(2*sample[:,4]*sample[:,5]<= sample[:,6]**2)\n",
    "  sample = np.delete(sample, indecies,0)\n",
    "  # print(sample.shape)\n",
    "  # print(2*kappa*vbar>gamma**2)\n",
    "  hs_prices = call_option_Heston_moy(N = 160, L= 12, r= sample[:,2].reshape((-1,1)), tau= sample[:,1].reshape((-1,1)), kappa= sample[:,4].reshape((-1,1)),\\\n",
    "                                     gamma = sample[:,6].reshape((-1,1)), vbar = sample[:,5].reshape((-1,1)), v0 = sample[:,7].reshape((-1,1)),\\\n",
    "                                     rho = sample[:,3].reshape((-1,1)), moyeness = sample[:,0].reshape((-1,1))).reshape((-1,1))\n",
    "  # hs_prices = call_option_Heston_moy(N = 1500, L= 50, r= sample[:,2].reshape((-1,1)), tau= sample[:,1].reshape((-1,1)), kappa= sample[:,4].reshape((-1,1)),\\\n",
    "  #                                    gamma = sample[:,6].reshape((-1,1)), vbar = sample[:,5].reshape((-1,1)), v0 = sample[:,7].reshape((-1,1)),\\\n",
    "  #                                    rho = sample[:,3].reshape((-1,1)), moyeness = sample[:,0].reshape((-1,1))).reshape((-1,1))\n",
    "  \n",
    "  hs_dataset = np.concatenate((sample, hs_prices),axis=1)\n",
    "\n",
    "  print(\"------ INPUT ------\")\n",
    "  print(\"S/K.......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,0]),max(hs_dataset[:,0])))\n",
    "  print(\"Tau.......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,1]),max(hs_dataset[:,1])))\n",
    "  print(\"r............ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,2]),max(hs_dataset[:,2])))\n",
    "  print(\"Corr ........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,3]),max(hs_dataset[:,3])))\n",
    "  print(\"Kappa........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,4]),max(hs_dataset[:,4])))\n",
    "  print(\"V_bar........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,5]),max(hs_dataset[:,5])))\n",
    "  print(\"Gamma........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,6]),max(hs_dataset[:,6])))\n",
    "  print(\"V_0 ......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,7]),max(hs_dataset[:,7])))\n",
    "  print(\"------ OUTPUT ------\")\n",
    "  print(\"V/K.......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,-1]),max(hs_dataset[:,-1])))\n",
    "  # ind_max = np.argmax(hs_dataset[:,-1])\n",
    "  # print(hs_dataset[ind_max,:])\n",
    "\n",
    "  return torch.FloatTensor(hs_dataset)\n",
    "\n",
    "hs_dataset = Heston_LHS_data_generator(10**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOjnH0N6HMXt"
   },
   "source": [
    "### Defining class Heston-ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nQmv3BmUGpg"
   },
   "outputs": [],
   "source": [
    "class Heston_ANN(BS_ANN):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Initilize BS-Heston_ANN netwwork. Same architecture as the one used in tge article.\n",
    "    Weight are initialized using Glorot_uniform also known as Xavier_uniform.\n",
    "    Takes inputs of size 8.\n",
    "    \"\"\"\n",
    "    super(Heston_ANN, self).__init__()\n",
    "    self.fc1 = nn.Linear(8, 400)\n",
    "    self.fc2 = nn.Linear(400, 400)\n",
    "    self.fc3 = nn.Linear(400, 400)\n",
    "    self.fc4 = nn.Linear(400, 400)\n",
    "    self.fc5 = nn.Linear(400, 1)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc5.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3o5w0J_Hl1O"
   },
   "source": [
    "### Training Heston-ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hh1i0WibUGuD",
    "outputId": "19eb2734-d269-47d7-bd8e-476a390180d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mLe flux de sortie a t tronqu et ne contient que les 5000dernires lignes.\u001b[0m\n",
      "Train Epoch: 2376 [0/83817 (0%)]\tlog Loss: -16.307240\n",
      "\n",
      "Train set: Average log loss: -16.2969\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5811\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2377 [0/83817 (0%)]\tlog Loss: -16.307254\n",
      "\n",
      "Train set: Average log loss: -16.2970\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5811\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2378 [0/83817 (0%)]\tlog Loss: -16.307308\n",
      "\n",
      "Train set: Average log loss: -16.2970\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5812\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2379 [0/83817 (0%)]\tlog Loss: -16.307379\n",
      "\n",
      "Train set: Average log loss: -16.2971\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5812\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2380 [0/83817 (0%)]\tlog Loss: -16.307475\n",
      "\n",
      "Train set: Average log loss: -16.2972\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5812\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2381 [0/83817 (0%)]\tlog Loss: -16.307516\n",
      "\n",
      "Train set: Average log loss: -16.2972\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5813\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2382 [0/83817 (0%)]\tlog Loss: -16.307587\n",
      "\n",
      "Train set: Average log loss: -16.2973\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5813\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2383 [0/83817 (0%)]\tlog Loss: -16.307598\n",
      "\n",
      "Train set: Average log loss: -16.2974\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5814\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2384 [0/83817 (0%)]\tlog Loss: -16.307696\n",
      "\n",
      "Train set: Average log loss: -16.2974\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5814\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2385 [0/83817 (0%)]\tlog Loss: -16.307801\n",
      "\n",
      "Train set: Average log loss: -16.2975\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5815\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2386 [0/83817 (0%)]\tlog Loss: -16.307832\n",
      "\n",
      "Train set: Average log loss: -16.2976\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5815\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2387 [0/83817 (0%)]\tlog Loss: -16.307879\n",
      "\n",
      "Train set: Average log loss: -16.2977\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5815\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2388 [0/83817 (0%)]\tlog Loss: -16.307942\n",
      "\n",
      "Train set: Average log loss: -16.2977\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5816\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2389 [0/83817 (0%)]\tlog Loss: -16.308054\n",
      "\n",
      "Train set: Average log loss: -16.2978\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5816\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2390 [0/83817 (0%)]\tlog Loss: -16.308098\n",
      "\n",
      "Train set: Average log loss: -16.2979\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5817\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2391 [0/83817 (0%)]\tlog Loss: -16.308148\n",
      "\n",
      "Train set: Average log loss: -16.2979\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5817\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2392 [0/83817 (0%)]\tlog Loss: -16.308215\n",
      "\n",
      "Train set: Average log loss: -16.2980\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5818\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2393 [0/83817 (0%)]\tlog Loss: -16.308282\n",
      "\n",
      "Train set: Average log loss: -16.2981\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5818\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2394 [0/83817 (0%)]\tlog Loss: -16.308333\n",
      "\n",
      "Train set: Average log loss: -16.2982\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5819\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2395 [0/83817 (0%)]\tlog Loss: -16.308373\n",
      "\n",
      "Train set: Average log loss: -16.2982\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5819\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2396 [0/83817 (0%)]\tlog Loss: -16.308404\n",
      "\n",
      "Train set: Average log loss: -16.2983\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5819\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2397 [0/83817 (0%)]\tlog Loss: -16.308543\n",
      "\n",
      "Train set: Average log loss: -16.2983\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5820\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2398 [0/83817 (0%)]\tlog Loss: -16.308560\n",
      "\n",
      "Train set: Average log loss: -16.2984\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5820\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2399 [0/83817 (0%)]\tlog Loss: -16.308673\n",
      "\n",
      "Train set: Average log loss: -16.2985\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5820\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2400 [0/83817 (0%)]\tlog Loss: -16.308710\n",
      "\n",
      "Train set: Average log loss: -16.2986\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5821\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2401 [0/83817 (0%)]\tlog Loss: -16.308793\n",
      "\n",
      "Train set: Average log loss: -16.2986\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5821\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2402 [0/83817 (0%)]\tlog Loss: -16.308848\n",
      "\n",
      "Train set: Average log loss: -16.2987\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5822\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2403 [0/83817 (0%)]\tlog Loss: -16.308857\n",
      "\n",
      "Train set: Average log loss: -16.2988\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5822\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2404 [0/83817 (0%)]\tlog Loss: -16.308993\n",
      "\n",
      "Train set: Average log loss: -16.2988\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5822\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2405 [0/83817 (0%)]\tlog Loss: -16.309080\n",
      "\n",
      "Train set: Average log loss: -16.2989\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5823\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2406 [0/83817 (0%)]\tlog Loss: -16.309181\n",
      "\n",
      "Train set: Average log loss: -16.2990\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5823\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2407 [0/83817 (0%)]\tlog Loss: -16.309220\n",
      "\n",
      "Train set: Average log loss: -16.2990\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5824\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2408 [0/83817 (0%)]\tlog Loss: -16.309263\n",
      "\n",
      "Train set: Average log loss: -16.2991\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5824\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2409 [0/83817 (0%)]\tlog Loss: -16.309393\n",
      "\n",
      "Train set: Average log loss: -16.2992\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5824\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2410 [0/83817 (0%)]\tlog Loss: -16.309399\n",
      "\n",
      "Train set: Average log loss: -16.2993\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5825\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2411 [0/83817 (0%)]\tlog Loss: -16.309442\n",
      "\n",
      "Train set: Average log loss: -16.2993\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5825\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2412 [0/83817 (0%)]\tlog Loss: -16.309515\n",
      "\n",
      "Train set: Average log loss: -16.2994\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5826\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2413 [0/83817 (0%)]\tlog Loss: -16.309672\n",
      "\n",
      "Train set: Average log loss: -16.2994\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5826\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2414 [0/83817 (0%)]\tlog Loss: -16.309603\n",
      "\n",
      "Train set: Average log loss: -16.2995\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5826\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2415 [0/83817 (0%)]\tlog Loss: -16.309695\n",
      "\n",
      "Train set: Average log loss: -16.2996\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5827\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2416 [0/83817 (0%)]\tlog Loss: -16.309858\n",
      "\n",
      "Train set: Average log loss: -16.2996\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5827\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2417 [0/83817 (0%)]\tlog Loss: -16.309843\n",
      "\n",
      "Train set: Average log loss: -16.2997\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5828\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2418 [0/83817 (0%)]\tlog Loss: -16.309874\n",
      "\n",
      "Train set: Average log loss: -16.2998\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5828\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2419 [0/83817 (0%)]\tlog Loss: -16.310026\n",
      "\n",
      "Train set: Average log loss: -16.2998\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5829\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2420 [0/83817 (0%)]\tlog Loss: -16.310103\n",
      "\n",
      "Train set: Average log loss: -16.2999\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5829\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2421 [0/83817 (0%)]\tlog Loss: -16.310159\n",
      "\n",
      "Train set: Average log loss: -16.3000\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5830\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2422 [0/83817 (0%)]\tlog Loss: -16.310274\n",
      "\n",
      "Train set: Average log loss: -16.3001\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5830\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2423 [0/83817 (0%)]\tlog Loss: -16.310354\n",
      "\n",
      "Train set: Average log loss: -16.3001\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5830\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2424 [0/83817 (0%)]\tlog Loss: -16.310383\n",
      "\n",
      "Train set: Average log loss: -16.3002\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5831\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2425 [0/83817 (0%)]\tlog Loss: -16.310406\n",
      "\n",
      "Train set: Average log loss: -16.3002\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5831\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2426 [0/83817 (0%)]\tlog Loss: -16.310522\n",
      "\n",
      "Train set: Average log loss: -16.3003\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5832\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2427 [0/83817 (0%)]\tlog Loss: -16.310594\n",
      "\n",
      "Train set: Average log loss: -16.3004\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5832\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2428 [0/83817 (0%)]\tlog Loss: -16.310620\n",
      "\n",
      "Train set: Average log loss: -16.3004\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5832\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2429 [0/83817 (0%)]\tlog Loss: -16.310695\n",
      "\n",
      "Train set: Average log loss: -16.3005\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5833\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2430 [0/83817 (0%)]\tlog Loss: -16.310778\n",
      "\n",
      "Train set: Average log loss: -16.3006\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5833\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2431 [0/83817 (0%)]\tlog Loss: -16.310891\n",
      "\n",
      "Train set: Average log loss: -16.3006\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5834\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2432 [0/83817 (0%)]\tlog Loss: -16.310880\n",
      "\n",
      "Train set: Average log loss: -16.3007\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5834\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2433 [0/83817 (0%)]\tlog Loss: -16.310911\n",
      "\n",
      "Train set: Average log loss: -16.3008\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5835\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2434 [0/83817 (0%)]\tlog Loss: -16.311026\n",
      "\n",
      "Train set: Average log loss: -16.3009\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5835\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2435 [0/83817 (0%)]\tlog Loss: -16.311109\n",
      "\n",
      "Train set: Average log loss: -16.3009\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5836\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2436 [0/83817 (0%)]\tlog Loss: -16.311112\n",
      "\n",
      "Train set: Average log loss: -16.3010\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5836\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2437 [0/83817 (0%)]\tlog Loss: -16.311192\n",
      "\n",
      "Train set: Average log loss: -16.3010\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5837\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2438 [0/83817 (0%)]\tlog Loss: -16.311256\n",
      "\n",
      "Train set: Average log loss: -16.3011\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5837\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2439 [0/83817 (0%)]\tlog Loss: -16.311313\n",
      "\n",
      "Train set: Average log loss: -16.3012\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5838\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2440 [0/83817 (0%)]\tlog Loss: -16.311343\n",
      "\n",
      "Train set: Average log loss: -16.3012\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5838\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2441 [0/83817 (0%)]\tlog Loss: -16.311425\n",
      "\n",
      "Train set: Average log loss: -16.3013\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5838\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2442 [0/83817 (0%)]\tlog Loss: -16.311464\n",
      "\n",
      "Train set: Average log loss: -16.3014\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5839\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2443 [0/83817 (0%)]\tlog Loss: -16.311567\n",
      "\n",
      "Train set: Average log loss: -16.3015\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5839\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2444 [0/83817 (0%)]\tlog Loss: -16.311642\n",
      "\n",
      "Train set: Average log loss: -16.3015\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5839\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2445 [0/83817 (0%)]\tlog Loss: -16.311737\n",
      "\n",
      "Train set: Average log loss: -16.3016\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5840\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2446 [0/83817 (0%)]\tlog Loss: -16.311786\n",
      "\n",
      "Train set: Average log loss: -16.3016\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5841\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2447 [0/83817 (0%)]\tlog Loss: -16.311905\n",
      "\n",
      "Train set: Average log loss: -16.3017\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5841\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2448 [0/83817 (0%)]\tlog Loss: -16.311907\n",
      "\n",
      "Train set: Average log loss: -16.3018\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5841\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2449 [0/83817 (0%)]\tlog Loss: -16.312007\n",
      "\n",
      "Train set: Average log loss: -16.3018\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5842\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2450 [0/83817 (0%)]\tlog Loss: -16.312078\n",
      "\n",
      "Train set: Average log loss: -16.3019\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5842\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2451 [0/83817 (0%)]\tlog Loss: -16.312152\n",
      "\n",
      "Train set: Average log loss: -16.3020\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5843\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2452 [0/83817 (0%)]\tlog Loss: -16.312204\n",
      "\n",
      "Train set: Average log loss: -16.3021\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5843\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2453 [0/83817 (0%)]\tlog Loss: -16.312287\n",
      "\n",
      "Train set: Average log loss: -16.3021\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5843\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2454 [0/83817 (0%)]\tlog Loss: -16.312332\n",
      "\n",
      "Train set: Average log loss: -16.3022\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5844\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2455 [0/83817 (0%)]\tlog Loss: -16.312384\n",
      "\n",
      "Train set: Average log loss: -16.3022\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5844\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2456 [0/83817 (0%)]\tlog Loss: -16.312553\n",
      "\n",
      "Train set: Average log loss: -16.3023\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5845\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2457 [0/83817 (0%)]\tlog Loss: -16.312623\n",
      "\n",
      "Train set: Average log loss: -16.3024\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5845\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2458 [0/83817 (0%)]\tlog Loss: -16.312647\n",
      "\n",
      "Train set: Average log loss: -16.3024\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5846\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2459 [0/83817 (0%)]\tlog Loss: -16.312783\n",
      "\n",
      "Train set: Average log loss: -16.3025\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5846\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2460 [0/83817 (0%)]\tlog Loss: -16.312833\n",
      "\n",
      "Train set: Average log loss: -16.3026\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5846\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2461 [0/83817 (0%)]\tlog Loss: -16.312936\n",
      "\n",
      "Train set: Average log loss: -16.3027\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5847\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2462 [0/83817 (0%)]\tlog Loss: -16.312966\n",
      "\n",
      "Train set: Average log loss: -16.3027\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5847\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2463 [0/83817 (0%)]\tlog Loss: -16.313088\n",
      "\n",
      "Train set: Average log loss: -16.3028\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5848\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2464 [0/83817 (0%)]\tlog Loss: -16.313155\n",
      "\n",
      "Train set: Average log loss: -16.3029\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5848\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2465 [0/83817 (0%)]\tlog Loss: -16.313256\n",
      "\n",
      "Train set: Average log loss: -16.3029\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5849\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2466 [0/83817 (0%)]\tlog Loss: -16.313341\n",
      "\n",
      "Train set: Average log loss: -16.3030\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5849\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2467 [0/83817 (0%)]\tlog Loss: -16.313374\n",
      "\n",
      "Train set: Average log loss: -16.3030\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5850\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2468 [0/83817 (0%)]\tlog Loss: -16.313441\n",
      "\n",
      "Train set: Average log loss: -16.3031\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5850\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2469 [0/83817 (0%)]\tlog Loss: -16.313487\n",
      "\n",
      "Train set: Average log loss: -16.3032\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5851\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2470 [0/83817 (0%)]\tlog Loss: -16.313603\n",
      "\n",
      "Train set: Average log loss: -16.3033\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5851\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2471 [0/83817 (0%)]\tlog Loss: -16.313618\n",
      "\n",
      "Train set: Average log loss: -16.3033\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5851\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2472 [0/83817 (0%)]\tlog Loss: -16.313758\n",
      "\n",
      "Train set: Average log loss: -16.3034\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5852\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2473 [0/83817 (0%)]\tlog Loss: -16.313765\n",
      "\n",
      "Train set: Average log loss: -16.3034\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5852\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2474 [0/83817 (0%)]\tlog Loss: -16.313850\n",
      "\n",
      "Train set: Average log loss: -16.3035\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5853\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2475 [0/83817 (0%)]\tlog Loss: -16.313903\n",
      "\n",
      "Train set: Average log loss: -16.3036\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5853\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2476 [0/83817 (0%)]\tlog Loss: -16.313897\n",
      "\n",
      "Train set: Average log loss: -16.3036\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5853\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2477 [0/83817 (0%)]\tlog Loss: -16.313974\n",
      "\n",
      "Train set: Average log loss: -16.3037\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5854\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2478 [0/83817 (0%)]\tlog Loss: -16.314182\n",
      "\n",
      "Train set: Average log loss: -16.3038\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5854\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2479 [0/83817 (0%)]\tlog Loss: -16.314230\n",
      "\n",
      "Train set: Average log loss: -16.3038\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5855\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2480 [0/83817 (0%)]\tlog Loss: -16.314286\n",
      "\n",
      "Train set: Average log loss: -16.3039\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5855\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2481 [0/83817 (0%)]\tlog Loss: -16.314310\n",
      "\n",
      "Train set: Average log loss: -16.3040\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5856\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2482 [0/83817 (0%)]\tlog Loss: -16.314409\n",
      "\n",
      "Train set: Average log loss: -16.3040\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5856\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2483 [0/83817 (0%)]\tlog Loss: -16.314433\n",
      "\n",
      "Train set: Average log loss: -16.3041\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5856\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2484 [0/83817 (0%)]\tlog Loss: -16.314505\n",
      "\n",
      "Train set: Average log loss: -16.3042\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5857\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2485 [0/83817 (0%)]\tlog Loss: -16.314573\n",
      "\n",
      "Train set: Average log loss: -16.3042\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5857\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2486 [0/83817 (0%)]\tlog Loss: -16.314632\n",
      "\n",
      "Train set: Average log loss: -16.3043\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5858\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2487 [0/83817 (0%)]\tlog Loss: -16.314787\n",
      "\n",
      "Train set: Average log loss: -16.3044\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5858\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2488 [0/83817 (0%)]\tlog Loss: -16.314775\n",
      "\n",
      "Train set: Average log loss: -16.3044\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5858\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2489 [0/83817 (0%)]\tlog Loss: -16.314897\n",
      "\n",
      "Train set: Average log loss: -16.3045\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5859\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2490 [0/83817 (0%)]\tlog Loss: -16.314857\n",
      "\n",
      "Train set: Average log loss: -16.3046\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5859\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2491 [0/83817 (0%)]\tlog Loss: -16.314996\n",
      "\n",
      "Train set: Average log loss: -16.3046\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5859\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2492 [0/83817 (0%)]\tlog Loss: -16.315074\n",
      "\n",
      "Train set: Average log loss: -16.3047\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5860\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2493 [0/83817 (0%)]\tlog Loss: -16.315164\n",
      "\n",
      "Train set: Average log loss: -16.3048\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5860\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2494 [0/83817 (0%)]\tlog Loss: -16.315259\n",
      "\n",
      "Train set: Average log loss: -16.3048\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5861\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2495 [0/83817 (0%)]\tlog Loss: -16.315315\n",
      "\n",
      "Train set: Average log loss: -16.3049\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5861\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2496 [0/83817 (0%)]\tlog Loss: -16.315424\n",
      "\n",
      "Train set: Average log loss: -16.3050\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5861\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2497 [0/83817 (0%)]\tlog Loss: -16.315493\n",
      "\n",
      "Train set: Average log loss: -16.3050\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5862\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2498 [0/83817 (0%)]\tlog Loss: -16.315593\n",
      "\n",
      "Train set: Average log loss: -16.3051\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5862\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2499 [0/83817 (0%)]\tlog Loss: -16.315666\n",
      "\n",
      "Train set: Average log loss: -16.3052\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5863\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2500 [0/83817 (0%)]\tlog Loss: -16.315740\n",
      "\n",
      "Train set: Average log loss: -16.3052\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5863\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2501 [0/83817 (0%)]\tlog Loss: -16.315777\n",
      "\n",
      "Train set: Average log loss: -16.3053\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5863\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2502 [0/83817 (0%)]\tlog Loss: -16.315875\n",
      "\n",
      "Train set: Average log loss: -16.3054\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5864\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2503 [0/83817 (0%)]\tlog Loss: -16.315996\n",
      "\n",
      "Train set: Average log loss: -16.3054\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5864\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2504 [0/83817 (0%)]\tlog Loss: -16.315979\n",
      "\n",
      "Train set: Average log loss: -16.3055\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5865\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2505 [0/83817 (0%)]\tlog Loss: -16.316079\n",
      "\n",
      "Train set: Average log loss: -16.3055\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5865\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2506 [0/83817 (0%)]\tlog Loss: -16.316159\n",
      "\n",
      "Train set: Average log loss: -16.3056\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5865\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2507 [0/83817 (0%)]\tlog Loss: -16.316209\n",
      "\n",
      "Train set: Average log loss: -16.3057\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5866\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2508 [0/83817 (0%)]\tlog Loss: -16.316319\n",
      "\n",
      "Train set: Average log loss: -16.3057\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5866\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2509 [0/83817 (0%)]\tlog Loss: -16.316387\n",
      "\n",
      "Train set: Average log loss: -16.3058\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5866\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2510 [0/83817 (0%)]\tlog Loss: -16.316464\n",
      "\n",
      "Train set: Average log loss: -16.3059\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5867\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2511 [0/83817 (0%)]\tlog Loss: -16.316522\n",
      "\n",
      "Train set: Average log loss: -16.3059\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5867\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2512 [0/83817 (0%)]\tlog Loss: -16.316523\n",
      "\n",
      "Train set: Average log loss: -16.3060\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5868\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2513 [0/83817 (0%)]\tlog Loss: -16.316721\n",
      "\n",
      "Train set: Average log loss: -16.3061\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5868\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2514 [0/83817 (0%)]\tlog Loss: -16.316731\n",
      "\n",
      "Train set: Average log loss: -16.3061\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5869\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2515 [0/83817 (0%)]\tlog Loss: -16.316758\n",
      "\n",
      "Train set: Average log loss: -16.3062\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5869\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2516 [0/83817 (0%)]\tlog Loss: -16.316881\n",
      "\n",
      "Train set: Average log loss: -16.3063\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5870\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2517 [0/83817 (0%)]\tlog Loss: -16.316956\n",
      "\n",
      "Train set: Average log loss: -16.3063\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5870\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2518 [0/83817 (0%)]\tlog Loss: -16.317041\n",
      "\n",
      "Train set: Average log loss: -16.3064\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5870\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2519 [0/83817 (0%)]\tlog Loss: -16.317100\n",
      "\n",
      "Train set: Average log loss: -16.3065\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5871\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2520 [0/83817 (0%)]\tlog Loss: -16.317161\n",
      "\n",
      "Train set: Average log loss: -16.3065\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5871\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2521 [0/83817 (0%)]\tlog Loss: -16.317236\n",
      "\n",
      "Train set: Average log loss: -16.3066\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5871\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2522 [0/83817 (0%)]\tlog Loss: -16.317317\n",
      "\n",
      "Train set: Average log loss: -16.3067\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5872\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2523 [0/83817 (0%)]\tlog Loss: -16.317374\n",
      "\n",
      "Train set: Average log loss: -16.3067\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5872\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2524 [0/83817 (0%)]\tlog Loss: -16.317480\n",
      "\n",
      "Train set: Average log loss: -16.3068\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5872\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2525 [0/83817 (0%)]\tlog Loss: -16.317507\n",
      "\n",
      "Train set: Average log loss: -16.3069\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5873\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2526 [0/83817 (0%)]\tlog Loss: -16.317611\n",
      "\n",
      "Train set: Average log loss: -16.3069\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5873\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2527 [0/83817 (0%)]\tlog Loss: -16.317666\n",
      "\n",
      "Train set: Average log loss: -16.3070\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5874\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2528 [0/83817 (0%)]\tlog Loss: -16.317729\n",
      "\n",
      "Train set: Average log loss: -16.3071\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5874\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2529 [0/83817 (0%)]\tlog Loss: -16.317772\n",
      "\n",
      "Train set: Average log loss: -16.3071\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5875\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2530 [0/83817 (0%)]\tlog Loss: -16.317861\n",
      "\n",
      "Train set: Average log loss: -16.3072\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5875\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2531 [0/83817 (0%)]\tlog Loss: -16.317934\n",
      "\n",
      "Train set: Average log loss: -16.3073\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5875\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2532 [0/83817 (0%)]\tlog Loss: -16.318079\n",
      "\n",
      "Train set: Average log loss: -16.3073\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5875\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2533 [0/83817 (0%)]\tlog Loss: -16.318073\n",
      "\n",
      "Train set: Average log loss: -16.3074\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5876\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2534 [0/83817 (0%)]\tlog Loss: -16.318171\n",
      "\n",
      "Train set: Average log loss: -16.3075\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5877\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2535 [0/83817 (0%)]\tlog Loss: -16.318228\n",
      "\n",
      "Train set: Average log loss: -16.3075\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5877\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2536 [0/83817 (0%)]\tlog Loss: -16.318328\n",
      "\n",
      "Train set: Average log loss: -16.3076\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5877\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2537 [0/83817 (0%)]\tlog Loss: -16.318426\n",
      "\n",
      "Train set: Average log loss: -16.3076\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5878\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2538 [0/83817 (0%)]\tlog Loss: -16.318472\n",
      "\n",
      "Train set: Average log loss: -16.3077\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5878\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2539 [0/83817 (0%)]\tlog Loss: -16.318543\n",
      "\n",
      "Train set: Average log loss: -16.3078\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5879\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2540 [0/83817 (0%)]\tlog Loss: -16.318608\n",
      "\n",
      "Train set: Average log loss: -16.3079\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5879\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2541 [0/83817 (0%)]\tlog Loss: -16.318650\n",
      "\n",
      "Train set: Average log loss: -16.3079\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5879\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2542 [0/83817 (0%)]\tlog Loss: -16.318693\n",
      "\n",
      "Train set: Average log loss: -16.3080\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5880\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2543 [0/83817 (0%)]\tlog Loss: -16.318758\n",
      "\n",
      "Train set: Average log loss: -16.3080\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5880\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2544 [0/83817 (0%)]\tlog Loss: -16.318857\n",
      "\n",
      "Train set: Average log loss: -16.3081\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5881\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2545 [0/83817 (0%)]\tlog Loss: -16.318987\n",
      "\n",
      "Train set: Average log loss: -16.3082\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5881\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2546 [0/83817 (0%)]\tlog Loss: -16.318957\n",
      "\n",
      "Train set: Average log loss: -16.3082\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5882\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2547 [0/83817 (0%)]\tlog Loss: -16.319033\n",
      "\n",
      "Train set: Average log loss: -16.3083\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5882\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2548 [0/83817 (0%)]\tlog Loss: -16.319133\n",
      "\n",
      "Train set: Average log loss: -16.3083\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5882\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2549 [0/83817 (0%)]\tlog Loss: -16.319178\n",
      "\n",
      "Train set: Average log loss: -16.3084\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5883\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2550 [0/83817 (0%)]\tlog Loss: -16.319249\n",
      "\n",
      "Train set: Average log loss: -16.3085\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5883\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2551 [0/83817 (0%)]\tlog Loss: -16.319278\n",
      "\n",
      "Train set: Average log loss: -16.3086\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5884\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2552 [0/83817 (0%)]\tlog Loss: -16.319340\n",
      "\n",
      "Train set: Average log loss: -16.3086\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5884\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2553 [0/83817 (0%)]\tlog Loss: -16.319463\n",
      "\n",
      "Train set: Average log loss: -16.3087\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5884\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2554 [0/83817 (0%)]\tlog Loss: -16.319515\n",
      "\n",
      "Train set: Average log loss: -16.3088\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2555 [0/83817 (0%)]\tlog Loss: -16.319581\n",
      "\n",
      "Train set: Average log loss: -16.3088\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5885\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2556 [0/83817 (0%)]\tlog Loss: -16.319624\n",
      "\n",
      "Train set: Average log loss: -16.3089\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5886\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2557 [0/83817 (0%)]\tlog Loss: -16.319763\n",
      "\n",
      "Train set: Average log loss: -16.3090\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5886\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2558 [0/83817 (0%)]\tlog Loss: -16.319763\n",
      "\n",
      "Train set: Average log loss: -16.3090\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5886\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2559 [0/83817 (0%)]\tlog Loss: -16.319849\n",
      "\n",
      "Train set: Average log loss: -16.3091\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5887\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2560 [0/83817 (0%)]\tlog Loss: -16.319986\n",
      "\n",
      "Train set: Average log loss: -16.3091\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5887\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2561 [0/83817 (0%)]\tlog Loss: -16.319973\n",
      "\n",
      "Train set: Average log loss: -16.3092\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5887\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2562 [0/83817 (0%)]\tlog Loss: -16.320051\n",
      "\n",
      "Train set: Average log loss: -16.3093\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5888\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2563 [0/83817 (0%)]\tlog Loss: -16.320150\n",
      "\n",
      "Train set: Average log loss: -16.3093\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5888\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2564 [0/83817 (0%)]\tlog Loss: -16.320184\n",
      "\n",
      "Train set: Average log loss: -16.3094\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5889\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2565 [0/83817 (0%)]\tlog Loss: -16.320280\n",
      "\n",
      "Train set: Average log loss: -16.3095\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5889\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2566 [0/83817 (0%)]\tlog Loss: -16.320258\n",
      "\n",
      "Train set: Average log loss: -16.3095\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5890\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2567 [0/83817 (0%)]\tlog Loss: -16.320376\n",
      "\n",
      "Train set: Average log loss: -16.3096\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5890\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2568 [0/83817 (0%)]\tlog Loss: -16.320490\n",
      "\n",
      "Train set: Average log loss: -16.3097\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5890\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2569 [0/83817 (0%)]\tlog Loss: -16.320464\n",
      "\n",
      "Train set: Average log loss: -16.3098\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2570 [0/83817 (0%)]\tlog Loss: -16.320615\n",
      "\n",
      "Train set: Average log loss: -16.3098\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2571 [0/83817 (0%)]\tlog Loss: -16.320624\n",
      "\n",
      "Train set: Average log loss: -16.3099\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2572 [0/83817 (0%)]\tlog Loss: -16.320648\n",
      "\n",
      "Train set: Average log loss: -16.3099\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5891\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2573 [0/83817 (0%)]\tlog Loss: -16.320808\n",
      "\n",
      "Train set: Average log loss: -16.3100\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5892\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2574 [0/83817 (0%)]\tlog Loss: -16.320825\n",
      "\n",
      "Train set: Average log loss: -16.3101\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5892\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2575 [0/83817 (0%)]\tlog Loss: -16.320875\n",
      "\n",
      "Train set: Average log loss: -16.3101\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5893\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2576 [0/83817 (0%)]\tlog Loss: -16.320909\n",
      "\n",
      "Train set: Average log loss: -16.3102\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5893\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2577 [0/83817 (0%)]\tlog Loss: -16.321046\n",
      "\n",
      "Train set: Average log loss: -16.3103\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5893\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2578 [0/83817 (0%)]\tlog Loss: -16.321043\n",
      "\n",
      "Train set: Average log loss: -16.3103\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5894\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2579 [0/83817 (0%)]\tlog Loss: -16.321082\n",
      "\n",
      "Train set: Average log loss: -16.3104\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5894\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2580 [0/83817 (0%)]\tlog Loss: -16.321224\n",
      "\n",
      "Train set: Average log loss: -16.3105\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5895\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2581 [0/83817 (0%)]\tlog Loss: -16.321253\n",
      "\n",
      "Train set: Average log loss: -16.3105\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5895\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2582 [0/83817 (0%)]\tlog Loss: -16.321328\n",
      "\n",
      "Train set: Average log loss: -16.3106\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5895\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2583 [0/83817 (0%)]\tlog Loss: -16.321398\n",
      "\n",
      "Train set: Average log loss: -16.3106\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5896\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2584 [0/83817 (0%)]\tlog Loss: -16.321416\n",
      "\n",
      "Train set: Average log loss: -16.3107\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5896\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2585 [0/83817 (0%)]\tlog Loss: -16.321524\n",
      "\n",
      "Train set: Average log loss: -16.3108\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5897\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2586 [0/83817 (0%)]\tlog Loss: -16.321576\n",
      "\n",
      "Train set: Average log loss: -16.3108\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5897\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2587 [0/83817 (0%)]\tlog Loss: -16.321602\n",
      "\n",
      "Train set: Average log loss: -16.3109\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5897\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2588 [0/83817 (0%)]\tlog Loss: -16.321692\n",
      "\n",
      "Train set: Average log loss: -16.3110\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5898\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2589 [0/83817 (0%)]\tlog Loss: -16.321717\n",
      "\n",
      "Train set: Average log loss: -16.3110\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5898\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2590 [0/83817 (0%)]\tlog Loss: -16.321835\n",
      "\n",
      "Train set: Average log loss: -16.3111\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5898\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2591 [0/83817 (0%)]\tlog Loss: -16.321927\n",
      "\n",
      "Train set: Average log loss: -16.3111\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5899\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2592 [0/83817 (0%)]\tlog Loss: -16.321954\n",
      "\n",
      "Train set: Average log loss: -16.3112\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5899\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2593 [0/83817 (0%)]\tlog Loss: -16.321965\n",
      "\n",
      "Train set: Average log loss: -16.3113\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5900\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2594 [0/83817 (0%)]\tlog Loss: -16.322014\n",
      "\n",
      "Train set: Average log loss: -16.3113\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5900\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2595 [0/83817 (0%)]\tlog Loss: -16.322117\n",
      "\n",
      "Train set: Average log loss: -16.3114\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5900\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2596 [0/83817 (0%)]\tlog Loss: -16.322152\n",
      "\n",
      "Train set: Average log loss: -16.3115\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5901\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2597 [0/83817 (0%)]\tlog Loss: -16.322247\n",
      "\n",
      "Train set: Average log loss: -16.3115\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5901\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2598 [0/83817 (0%)]\tlog Loss: -16.322272\n",
      "\n",
      "Train set: Average log loss: -16.3116\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5902\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2599 [0/83817 (0%)]\tlog Loss: -16.322398\n",
      "\n",
      "Train set: Average log loss: -16.3117\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5902\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2600 [0/83817 (0%)]\tlog Loss: -16.322426\n",
      "\n",
      "Train set: Average log loss: -16.3117\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5902\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2601 [0/83817 (0%)]\tlog Loss: -16.322457\n",
      "\n",
      "Train set: Average log loss: -16.3118\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5903\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2602 [0/83817 (0%)]\tlog Loss: -16.322538\n",
      "\n",
      "Train set: Average log loss: -16.3118\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5903\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2603 [0/83817 (0%)]\tlog Loss: -16.322608\n",
      "\n",
      "Train set: Average log loss: -16.3119\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5903\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2604 [0/83817 (0%)]\tlog Loss: -16.322622\n",
      "\n",
      "Train set: Average log loss: -16.3120\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5904\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2605 [0/83817 (0%)]\tlog Loss: -16.322691\n",
      "\n",
      "Train set: Average log loss: -16.3121\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5904\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2606 [0/83817 (0%)]\tlog Loss: -16.322783\n",
      "\n",
      "Train set: Average log loss: -16.3121\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5905\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2607 [0/83817 (0%)]\tlog Loss: -16.322784\n",
      "\n",
      "Train set: Average log loss: -16.3122\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5905\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2608 [0/83817 (0%)]\tlog Loss: -16.322902\n",
      "\n",
      "Train set: Average log loss: -16.3122\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5905\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2609 [0/83817 (0%)]\tlog Loss: -16.323013\n",
      "\n",
      "Train set: Average log loss: -16.3123\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5906\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2610 [0/83817 (0%)]\tlog Loss: -16.323060\n",
      "\n",
      "Train set: Average log loss: -16.3124\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5906\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2611 [0/83817 (0%)]\tlog Loss: -16.323114\n",
      "\n",
      "Train set: Average log loss: -16.3124\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5906\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2612 [0/83817 (0%)]\tlog Loss: -16.323197\n",
      "\n",
      "Train set: Average log loss: -16.3125\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5907\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2613 [0/83817 (0%)]\tlog Loss: -16.323262\n",
      "\n",
      "Train set: Average log loss: -16.3126\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5907\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2614 [0/83817 (0%)]\tlog Loss: -16.323295\n",
      "\n",
      "Train set: Average log loss: -16.3126\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5908\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2615 [0/83817 (0%)]\tlog Loss: -16.323391\n",
      "\n",
      "Train set: Average log loss: -16.3127\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5908\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2616 [0/83817 (0%)]\tlog Loss: -16.323413\n",
      "\n",
      "Train set: Average log loss: -16.3128\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5908\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2617 [0/83817 (0%)]\tlog Loss: -16.323468\n",
      "\n",
      "Train set: Average log loss: -16.3128\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5909\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2618 [0/83817 (0%)]\tlog Loss: -16.323590\n",
      "\n",
      "Train set: Average log loss: -16.3129\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5909\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2619 [0/83817 (0%)]\tlog Loss: -16.323577\n",
      "\n",
      "Train set: Average log loss: -16.3129\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5910\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2620 [0/83817 (0%)]\tlog Loss: -16.323697\n",
      "\n",
      "Train set: Average log loss: -16.3130\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5910\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2621 [0/83817 (0%)]\tlog Loss: -16.323767\n",
      "\n",
      "Train set: Average log loss: -16.3131\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5910\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2622 [0/83817 (0%)]\tlog Loss: -16.323777\n",
      "\n",
      "Train set: Average log loss: -16.3132\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5911\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2623 [0/83817 (0%)]\tlog Loss: -16.323842\n",
      "\n",
      "Train set: Average log loss: -16.3132\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5911\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2624 [0/83817 (0%)]\tlog Loss: -16.323919\n",
      "\n",
      "Train set: Average log loss: -16.3133\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5911\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2625 [0/83817 (0%)]\tlog Loss: -16.323970\n",
      "\n",
      "Train set: Average log loss: -16.3133\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5912\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2626 [0/83817 (0%)]\tlog Loss: -16.324030\n",
      "\n",
      "Train set: Average log loss: -16.3134\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5912\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2627 [0/83817 (0%)]\tlog Loss: -16.324103\n",
      "\n",
      "Train set: Average log loss: -16.3135\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5913\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2628 [0/83817 (0%)]\tlog Loss: -16.324154\n",
      "\n",
      "Train set: Average log loss: -16.3135\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5913\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2629 [0/83817 (0%)]\tlog Loss: -16.324251\n",
      "\n",
      "Train set: Average log loss: -16.3136\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5913\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2630 [0/83817 (0%)]\tlog Loss: -16.324313\n",
      "\n",
      "Train set: Average log loss: -16.3136\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5914\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2631 [0/83817 (0%)]\tlog Loss: -16.324342\n",
      "\n",
      "Train set: Average log loss: -16.3137\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5914\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2632 [0/83817 (0%)]\tlog Loss: -16.324403\n",
      "\n",
      "Train set: Average log loss: -16.3138\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5915\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2633 [0/83817 (0%)]\tlog Loss: -16.324528\n",
      "\n",
      "Train set: Average log loss: -16.3138\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5915\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2634 [0/83817 (0%)]\tlog Loss: -16.324581\n",
      "\n",
      "Train set: Average log loss: -16.3139\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5915\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2635 [0/83817 (0%)]\tlog Loss: -16.324602\n",
      "\n",
      "Train set: Average log loss: -16.3140\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5916\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2636 [0/83817 (0%)]\tlog Loss: -16.324700\n",
      "\n",
      "Train set: Average log loss: -16.3140\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5916\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2637 [0/83817 (0%)]\tlog Loss: -16.324751\n",
      "\n",
      "Train set: Average log loss: -16.3141\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5916\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2638 [0/83817 (0%)]\tlog Loss: -16.324819\n",
      "\n",
      "Train set: Average log loss: -16.3142\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5917\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2639 [0/83817 (0%)]\tlog Loss: -16.324877\n",
      "\n",
      "Train set: Average log loss: -16.3142\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5917\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2640 [0/83817 (0%)]\tlog Loss: -16.324922\n",
      "\n",
      "Train set: Average log loss: -16.3143\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5917\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2641 [0/83817 (0%)]\tlog Loss: -16.325020\n",
      "\n",
      "Train set: Average log loss: -16.3143\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5918\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2642 [0/83817 (0%)]\tlog Loss: -16.325051\n",
      "\n",
      "Train set: Average log loss: -16.3144\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5918\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2643 [0/83817 (0%)]\tlog Loss: -16.325120\n",
      "\n",
      "Train set: Average log loss: -16.3145\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5918\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2644 [0/83817 (0%)]\tlog Loss: -16.325169\n",
      "\n",
      "Train set: Average log loss: -16.3145\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5919\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2645 [0/83817 (0%)]\tlog Loss: -16.325266\n",
      "\n",
      "Train set: Average log loss: -16.3146\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5919\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2646 [0/83817 (0%)]\tlog Loss: -16.325275\n",
      "\n",
      "Train set: Average log loss: -16.3147\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5920\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2647 [0/83817 (0%)]\tlog Loss: -16.325358\n",
      "\n",
      "Train set: Average log loss: -16.3147\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5920\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2648 [0/83817 (0%)]\tlog Loss: -16.325384\n",
      "\n",
      "Train set: Average log loss: -16.3148\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5920\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2649 [0/83817 (0%)]\tlog Loss: -16.325475\n",
      "\n",
      "Train set: Average log loss: -16.3149\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5921\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2650 [0/83817 (0%)]\tlog Loss: -16.325525\n",
      "\n",
      "Train set: Average log loss: -16.3149\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5921\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2651 [0/83817 (0%)]\tlog Loss: -16.325569\n",
      "\n",
      "Train set: Average log loss: -16.3150\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5921\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2652 [0/83817 (0%)]\tlog Loss: -16.325626\n",
      "\n",
      "Train set: Average log loss: -16.3151\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5922\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2653 [0/83817 (0%)]\tlog Loss: -16.325744\n",
      "\n",
      "Train set: Average log loss: -16.3151\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5922\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2654 [0/83817 (0%)]\tlog Loss: -16.325812\n",
      "\n",
      "Train set: Average log loss: -16.3152\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5923\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2655 [0/83817 (0%)]\tlog Loss: -16.325875\n",
      "\n",
      "Train set: Average log loss: -16.3152\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5923\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2656 [0/83817 (0%)]\tlog Loss: -16.325899\n",
      "\n",
      "Train set: Average log loss: -16.3153\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5924\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2657 [0/83817 (0%)]\tlog Loss: -16.325947\n",
      "\n",
      "Train set: Average log loss: -16.3154\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5924\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2658 [0/83817 (0%)]\tlog Loss: -16.325985\n",
      "\n",
      "Train set: Average log loss: -16.3154\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5924\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2659 [0/83817 (0%)]\tlog Loss: -16.326079\n",
      "\n",
      "Train set: Average log loss: -16.3155\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5925\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2660 [0/83817 (0%)]\tlog Loss: -16.326154\n",
      "\n",
      "Train set: Average log loss: -16.3156\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5925\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2661 [0/83817 (0%)]\tlog Loss: -16.326176\n",
      "\n",
      "Train set: Average log loss: -16.3156\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5925\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2662 [0/83817 (0%)]\tlog Loss: -16.326285\n",
      "\n",
      "Train set: Average log loss: -16.3157\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5926\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2663 [0/83817 (0%)]\tlog Loss: -16.326306\n",
      "\n",
      "Train set: Average log loss: -16.3157\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5926\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2664 [0/83817 (0%)]\tlog Loss: -16.326420\n",
      "\n",
      "Train set: Average log loss: -16.3158\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5927\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2665 [0/83817 (0%)]\tlog Loss: -16.326488\n",
      "\n",
      "Train set: Average log loss: -16.3159\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5927\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2666 [0/83817 (0%)]\tlog Loss: -16.326528\n",
      "\n",
      "Train set: Average log loss: -16.3159\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5927\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2667 [0/83817 (0%)]\tlog Loss: -16.326606\n",
      "\n",
      "Train set: Average log loss: -16.3160\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5928\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2668 [0/83817 (0%)]\tlog Loss: -16.326665\n",
      "\n",
      "Train set: Average log loss: -16.3160\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5928\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2669 [0/83817 (0%)]\tlog Loss: -16.326787\n",
      "\n",
      "Train set: Average log loss: -16.3161\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5929\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2670 [0/83817 (0%)]\tlog Loss: -16.326810\n",
      "\n",
      "Train set: Average log loss: -16.3162\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5929\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2671 [0/83817 (0%)]\tlog Loss: -16.326812\n",
      "\n",
      "Train set: Average log loss: -16.3162\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5929\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2672 [0/83817 (0%)]\tlog Loss: -16.327030\n",
      "\n",
      "Train set: Average log loss: -16.3163\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2673 [0/83817 (0%)]\tlog Loss: -16.326989\n",
      "\n",
      "Train set: Average log loss: -16.3164\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2674 [0/83817 (0%)]\tlog Loss: -16.327039\n",
      "\n",
      "Train set: Average log loss: -16.3164\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5930\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2675 [0/83817 (0%)]\tlog Loss: -16.327187\n",
      "\n",
      "Train set: Average log loss: -16.3165\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5931\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2676 [0/83817 (0%)]\tlog Loss: -16.327237\n",
      "\n",
      "Train set: Average log loss: -16.3166\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5931\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2677 [0/83817 (0%)]\tlog Loss: -16.327275\n",
      "\n",
      "Train set: Average log loss: -16.3166\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5931\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2678 [0/83817 (0%)]\tlog Loss: -16.327252\n",
      "\n",
      "Train set: Average log loss: -16.3167\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5932\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2679 [0/83817 (0%)]\tlog Loss: -16.327460\n",
      "\n",
      "Train set: Average log loss: -16.3167\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5932\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2680 [0/83817 (0%)]\tlog Loss: -16.327434\n",
      "\n",
      "Train set: Average log loss: -16.3168\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5932\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2681 [0/83817 (0%)]\tlog Loss: -16.327517\n",
      "\n",
      "Train set: Average log loss: -16.3169\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5933\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2682 [0/83817 (0%)]\tlog Loss: -16.327616\n",
      "\n",
      "Train set: Average log loss: -16.3169\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5933\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2683 [0/83817 (0%)]\tlog Loss: -16.327720\n",
      "\n",
      "Train set: Average log loss: -16.3170\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2684 [0/83817 (0%)]\tlog Loss: -16.327845\n",
      "\n",
      "Train set: Average log loss: -16.3171\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5934\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2685 [0/83817 (0%)]\tlog Loss: -16.327819\n",
      "\n",
      "Train set: Average log loss: -16.3171\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2686 [0/83817 (0%)]\tlog Loss: -16.327978\n",
      "\n",
      "Train set: Average log loss: -16.3172\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2687 [0/83817 (0%)]\tlog Loss: -16.327982\n",
      "\n",
      "Train set: Average log loss: -16.3173\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5935\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2688 [0/83817 (0%)]\tlog Loss: -16.328066\n",
      "\n",
      "Train set: Average log loss: -16.3173\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2689 [0/83817 (0%)]\tlog Loss: -16.328164\n",
      "\n",
      "Train set: Average log loss: -16.3174\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2690 [0/83817 (0%)]\tlog Loss: -16.328194\n",
      "\n",
      "Train set: Average log loss: -16.3174\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5936\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2691 [0/83817 (0%)]\tlog Loss: -16.328211\n",
      "\n",
      "Train set: Average log loss: -16.3175\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2692 [0/83817 (0%)]\tlog Loss: -16.328350\n",
      "\n",
      "Train set: Average log loss: -16.3175\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5937\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2693 [0/83817 (0%)]\tlog Loss: -16.328423\n",
      "\n",
      "Train set: Average log loss: -16.3176\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2694 [0/83817 (0%)]\tlog Loss: -16.328527\n",
      "\n",
      "Train set: Average log loss: -16.3177\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5938\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2695 [0/83817 (0%)]\tlog Loss: -16.328610\n",
      "\n",
      "Train set: Average log loss: -16.3177\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2696 [0/83817 (0%)]\tlog Loss: -16.328632\n",
      "\n",
      "Train set: Average log loss: -16.3178\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2697 [0/83817 (0%)]\tlog Loss: -16.328719\n",
      "\n",
      "Train set: Average log loss: -16.3179\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5939\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2698 [0/83817 (0%)]\tlog Loss: -16.328756\n",
      "\n",
      "Train set: Average log loss: -16.3180\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5940\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2699 [0/83817 (0%)]\tlog Loss: -16.328921\n",
      "\n",
      "Train set: Average log loss: -16.3180\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5940\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2700 [0/83817 (0%)]\tlog Loss: -16.328962\n",
      "\n",
      "Train set: Average log loss: -16.3181\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5940\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2701 [0/83817 (0%)]\tlog Loss: -16.329075\n",
      "\n",
      "Train set: Average log loss: -16.3181\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2702 [0/83817 (0%)]\tlog Loss: -16.329083\n",
      "\n",
      "Train set: Average log loss: -16.3182\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2703 [0/83817 (0%)]\tlog Loss: -16.329113\n",
      "\n",
      "Train set: Average log loss: -16.3183\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5941\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2704 [0/83817 (0%)]\tlog Loss: -16.329151\n",
      "\n",
      "Train set: Average log loss: -16.3183\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2705 [0/83817 (0%)]\tlog Loss: -16.329298\n",
      "\n",
      "Train set: Average log loss: -16.3184\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5942\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2706 [0/83817 (0%)]\tlog Loss: -16.329344\n",
      "\n",
      "Train set: Average log loss: -16.3184\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2707 [0/83817 (0%)]\tlog Loss: -16.329368\n",
      "\n",
      "Train set: Average log loss: -16.3185\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2708 [0/83817 (0%)]\tlog Loss: -16.329483\n",
      "\n",
      "Train set: Average log loss: -16.3186\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5943\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2709 [0/83817 (0%)]\tlog Loss: -16.329526\n",
      "\n",
      "Train set: Average log loss: -16.3186\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2710 [0/83817 (0%)]\tlog Loss: -16.329537\n",
      "\n",
      "Train set: Average log loss: -16.3187\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5944\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2711 [0/83817 (0%)]\tlog Loss: -16.329628\n",
      "\n",
      "Train set: Average log loss: -16.3188\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2712 [0/83817 (0%)]\tlog Loss: -16.329651\n",
      "\n",
      "Train set: Average log loss: -16.3188\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2713 [0/83817 (0%)]\tlog Loss: -16.329750\n",
      "\n",
      "Train set: Average log loss: -16.3189\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5945\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2714 [0/83817 (0%)]\tlog Loss: -16.329788\n",
      "\n",
      "Train set: Average log loss: -16.3190\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2715 [0/83817 (0%)]\tlog Loss: -16.329807\n",
      "\n",
      "Train set: Average log loss: -16.3190\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5946\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2716 [0/83817 (0%)]\tlog Loss: -16.329958\n",
      "\n",
      "Train set: Average log loss: -16.3191\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2717 [0/83817 (0%)]\tlog Loss: -16.329998\n",
      "\n",
      "Train set: Average log loss: -16.3191\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2718 [0/83817 (0%)]\tlog Loss: -16.330065\n",
      "\n",
      "Train set: Average log loss: -16.3192\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2719 [0/83817 (0%)]\tlog Loss: -16.330144\n",
      "\n",
      "Train set: Average log loss: -16.3193\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5947\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2720 [0/83817 (0%)]\tlog Loss: -16.330188\n",
      "\n",
      "Train set: Average log loss: -16.3193\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2721 [0/83817 (0%)]\tlog Loss: -16.330260\n",
      "\n",
      "Train set: Average log loss: -16.3194\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5948\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2722 [0/83817 (0%)]\tlog Loss: -16.330305\n",
      "\n",
      "Train set: Average log loss: -16.3194\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2723 [0/83817 (0%)]\tlog Loss: -16.330364\n",
      "\n",
      "Train set: Average log loss: -16.3195\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2724 [0/83817 (0%)]\tlog Loss: -16.330504\n",
      "\n",
      "Train set: Average log loss: -16.3196\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5949\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2725 [0/83817 (0%)]\tlog Loss: -16.330498\n",
      "\n",
      "Train set: Average log loss: -16.3196\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5950\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2726 [0/83817 (0%)]\tlog Loss: -16.330572\n",
      "\n",
      "Train set: Average log loss: -16.3197\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5950\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2727 [0/83817 (0%)]\tlog Loss: -16.330614\n",
      "\n",
      "Train set: Average log loss: -16.3197\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2728 [0/83817 (0%)]\tlog Loss: -16.330675\n",
      "\n",
      "Train set: Average log loss: -16.3198\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2729 [0/83817 (0%)]\tlog Loss: -16.330770\n",
      "\n",
      "Train set: Average log loss: -16.3199\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5951\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2730 [0/83817 (0%)]\tlog Loss: -16.330764\n",
      "\n",
      "Train set: Average log loss: -16.3199\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2731 [0/83817 (0%)]\tlog Loss: -16.330933\n",
      "\n",
      "Train set: Average log loss: -16.3200\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5952\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2732 [0/83817 (0%)]\tlog Loss: -16.330949\n",
      "\n",
      "Train set: Average log loss: -16.3201\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2733 [0/83817 (0%)]\tlog Loss: -16.331078\n",
      "\n",
      "Train set: Average log loss: -16.3201\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2734 [0/83817 (0%)]\tlog Loss: -16.331076\n",
      "\n",
      "Train set: Average log loss: -16.3202\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2735 [0/83817 (0%)]\tlog Loss: -16.331147\n",
      "\n",
      "Train set: Average log loss: -16.3203\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5953\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2736 [0/83817 (0%)]\tlog Loss: -16.331196\n",
      "\n",
      "Train set: Average log loss: -16.3203\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5954\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2737 [0/83817 (0%)]\tlog Loss: -16.331257\n",
      "\n",
      "Train set: Average log loss: -16.3204\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5954\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2738 [0/83817 (0%)]\tlog Loss: -16.331308\n",
      "\n",
      "Train set: Average log loss: -16.3204\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2739 [0/83817 (0%)]\tlog Loss: -16.331372\n",
      "\n",
      "Train set: Average log loss: -16.3205\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2740 [0/83817 (0%)]\tlog Loss: -16.331438\n",
      "\n",
      "Train set: Average log loss: -16.3205\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5955\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2741 [0/83817 (0%)]\tlog Loss: -16.331519\n",
      "\n",
      "Train set: Average log loss: -16.3206\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5956\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2742 [0/83817 (0%)]\tlog Loss: -16.331643\n",
      "\n",
      "Train set: Average log loss: -16.3207\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5956\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2743 [0/83817 (0%)]\tlog Loss: -16.331711\n",
      "\n",
      "Train set: Average log loss: -16.3207\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2744 [0/83817 (0%)]\tlog Loss: -16.331718\n",
      "\n",
      "Train set: Average log loss: -16.3208\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2745 [0/83817 (0%)]\tlog Loss: -16.331822\n",
      "\n",
      "Train set: Average log loss: -16.3209\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5957\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2746 [0/83817 (0%)]\tlog Loss: -16.331820\n",
      "\n",
      "Train set: Average log loss: -16.3209\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5958\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2747 [0/83817 (0%)]\tlog Loss: -16.331884\n",
      "\n",
      "Train set: Average log loss: -16.3210\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5958\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2748 [0/83817 (0%)]\tlog Loss: -16.331993\n",
      "\n",
      "Train set: Average log loss: -16.3210\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2749 [0/83817 (0%)]\tlog Loss: -16.332085\n",
      "\n",
      "Train set: Average log loss: -16.3211\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2750 [0/83817 (0%)]\tlog Loss: -16.332035\n",
      "\n",
      "Train set: Average log loss: -16.3212\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5959\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2751 [0/83817 (0%)]\tlog Loss: -16.332195\n",
      "\n",
      "Train set: Average log loss: -16.3212\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2752 [0/83817 (0%)]\tlog Loss: -16.332209\n",
      "\n",
      "Train set: Average log loss: -16.3213\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5960\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2753 [0/83817 (0%)]\tlog Loss: -16.332379\n",
      "\n",
      "Train set: Average log loss: -16.3214\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2754 [0/83817 (0%)]\tlog Loss: -16.332332\n",
      "\n",
      "Train set: Average log loss: -16.3214\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2755 [0/83817 (0%)]\tlog Loss: -16.332445\n",
      "\n",
      "Train set: Average log loss: -16.3215\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5961\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2756 [0/83817 (0%)]\tlog Loss: -16.332450\n",
      "\n",
      "Train set: Average log loss: -16.3215\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2757 [0/83817 (0%)]\tlog Loss: -16.332571\n",
      "\n",
      "Train set: Average log loss: -16.3216\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2758 [0/83817 (0%)]\tlog Loss: -16.332537\n",
      "\n",
      "Train set: Average log loss: -16.3217\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5962\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2759 [0/83817 (0%)]\tlog Loss: -16.332660\n",
      "\n",
      "Train set: Average log loss: -16.3217\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2760 [0/83817 (0%)]\tlog Loss: -16.332736\n",
      "\n",
      "Train set: Average log loss: -16.3218\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5963\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2761 [0/83817 (0%)]\tlog Loss: -16.332764\n",
      "\n",
      "Train set: Average log loss: -16.3218\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5964\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2762 [0/83817 (0%)]\tlog Loss: -16.332853\n",
      "\n",
      "Train set: Average log loss: -16.3219\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5964\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2763 [0/83817 (0%)]\tlog Loss: -16.332821\n",
      "\n",
      "Train set: Average log loss: -16.3220\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2764 [0/83817 (0%)]\tlog Loss: -16.332994\n",
      "\n",
      "Train set: Average log loss: -16.3220\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2765 [0/83817 (0%)]\tlog Loss: -16.332922\n",
      "\n",
      "Train set: Average log loss: -16.3221\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5965\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2766 [0/83817 (0%)]\tlog Loss: -16.333066\n",
      "\n",
      "Train set: Average log loss: -16.3222\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2767 [0/83817 (0%)]\tlog Loss: -16.333133\n",
      "\n",
      "Train set: Average log loss: -16.3222\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2768 [0/83817 (0%)]\tlog Loss: -16.333164\n",
      "\n",
      "Train set: Average log loss: -16.3223\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5966\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2769 [0/83817 (0%)]\tlog Loss: -16.333272\n",
      "\n",
      "Train set: Average log loss: -16.3223\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2770 [0/83817 (0%)]\tlog Loss: -16.333265\n",
      "\n",
      "Train set: Average log loss: -16.3224\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5967\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2771 [0/83817 (0%)]\tlog Loss: -16.333377\n",
      "\n",
      "Train set: Average log loss: -16.3225\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2772 [0/83817 (0%)]\tlog Loss: -16.333427\n",
      "\n",
      "Train set: Average log loss: -16.3225\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2773 [0/83817 (0%)]\tlog Loss: -16.333431\n",
      "\n",
      "Train set: Average log loss: -16.3226\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5968\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2774 [0/83817 (0%)]\tlog Loss: -16.333537\n",
      "\n",
      "Train set: Average log loss: -16.3227\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2775 [0/83817 (0%)]\tlog Loss: -16.333607\n",
      "\n",
      "Train set: Average log loss: -16.3227\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2776 [0/83817 (0%)]\tlog Loss: -16.333678\n",
      "\n",
      "Train set: Average log loss: -16.3228\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5969\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2777 [0/83817 (0%)]\tlog Loss: -16.333680\n",
      "\n",
      "Train set: Average log loss: -16.3228\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2778 [0/83817 (0%)]\tlog Loss: -16.333759\n",
      "\n",
      "Train set: Average log loss: -16.3229\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5970\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2779 [0/83817 (0%)]\tlog Loss: -16.333804\n",
      "\n",
      "Train set: Average log loss: -16.3229\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2780 [0/83817 (0%)]\tlog Loss: -16.333883\n",
      "\n",
      "Train set: Average log loss: -16.3230\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2781 [0/83817 (0%)]\tlog Loss: -16.333865\n",
      "\n",
      "Train set: Average log loss: -16.3231\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5971\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2782 [0/83817 (0%)]\tlog Loss: -16.333988\n",
      "\n",
      "Train set: Average log loss: -16.3231\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2783 [0/83817 (0%)]\tlog Loss: -16.334003\n",
      "\n",
      "Train set: Average log loss: -16.3232\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2784 [0/83817 (0%)]\tlog Loss: -16.334067\n",
      "\n",
      "Train set: Average log loss: -16.3233\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5972\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2785 [0/83817 (0%)]\tlog Loss: -16.334118\n",
      "\n",
      "Train set: Average log loss: -16.3233\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2786 [0/83817 (0%)]\tlog Loss: -16.334242\n",
      "\n",
      "Train set: Average log loss: -16.3234\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2787 [0/83817 (0%)]\tlog Loss: -16.334236\n",
      "\n",
      "Train set: Average log loss: -16.3235\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5973\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2788 [0/83817 (0%)]\tlog Loss: -16.334350\n",
      "\n",
      "Train set: Average log loss: -16.3235\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2789 [0/83817 (0%)]\tlog Loss: -16.334315\n",
      "\n",
      "Train set: Average log loss: -16.3236\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2790 [0/83817 (0%)]\tlog Loss: -16.334405\n",
      "\n",
      "Train set: Average log loss: -16.3236\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5974\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2791 [0/83817 (0%)]\tlog Loss: -16.334483\n",
      "\n",
      "Train set: Average log loss: -16.3237\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2792 [0/83817 (0%)]\tlog Loss: -16.334453\n",
      "\n",
      "Train set: Average log loss: -16.3237\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2793 [0/83817 (0%)]\tlog Loss: -16.334526\n",
      "\n",
      "Train set: Average log loss: -16.3238\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5975\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2794 [0/83817 (0%)]\tlog Loss: -16.334567\n",
      "\n",
      "Train set: Average log loss: -16.3239\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2795 [0/83817 (0%)]\tlog Loss: -16.334638\n",
      "\n",
      "Train set: Average log loss: -16.3239\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5976\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2796 [0/83817 (0%)]\tlog Loss: -16.334704\n",
      "\n",
      "Train set: Average log loss: -16.3240\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2797 [0/83817 (0%)]\tlog Loss: -16.334717\n",
      "\n",
      "Train set: Average log loss: -16.3241\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2798 [0/83817 (0%)]\tlog Loss: -16.334774\n",
      "\n",
      "Train set: Average log loss: -16.3241\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5977\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2799 [0/83817 (0%)]\tlog Loss: -16.334804\n",
      "\n",
      "Train set: Average log loss: -16.3242\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2800 [0/83817 (0%)]\tlog Loss: -16.334925\n",
      "\n",
      "Train set: Average log loss: -16.3243\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2801 [0/83817 (0%)]\tlog Loss: -16.334897\n",
      "\n",
      "Train set: Average log loss: -16.3243\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5978\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2802 [0/83817 (0%)]\tlog Loss: -16.334967\n",
      "\n",
      "Train set: Average log loss: -16.3244\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2803 [0/83817 (0%)]\tlog Loss: -16.335069\n",
      "\n",
      "Train set: Average log loss: -16.3244\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2804 [0/83817 (0%)]\tlog Loss: -16.335147\n",
      "\n",
      "Train set: Average log loss: -16.3245\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5979\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2805 [0/83817 (0%)]\tlog Loss: -16.335145\n",
      "\n",
      "Train set: Average log loss: -16.3246\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2806 [0/83817 (0%)]\tlog Loss: -16.335210\n",
      "\n",
      "Train set: Average log loss: -16.3246\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5980\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2807 [0/83817 (0%)]\tlog Loss: -16.335302\n",
      "\n",
      "Train set: Average log loss: -16.3247\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5981\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2808 [0/83817 (0%)]\tlog Loss: -16.335317\n",
      "\n",
      "Train set: Average log loss: -16.3248\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5981\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2809 [0/83817 (0%)]\tlog Loss: -16.335370\n",
      "\n",
      "Train set: Average log loss: -16.3248\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2810 [0/83817 (0%)]\tlog Loss: -16.335414\n",
      "\n",
      "Train set: Average log loss: -16.3249\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2811 [0/83817 (0%)]\tlog Loss: -16.335432\n",
      "\n",
      "Train set: Average log loss: -16.3249\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2812 [0/83817 (0%)]\tlog Loss: -16.335557\n",
      "\n",
      "Train set: Average log loss: -16.3250\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5982\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2813 [0/83817 (0%)]\tlog Loss: -16.335557\n",
      "\n",
      "Train set: Average log loss: -16.3251\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5983\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2814 [0/83817 (0%)]\tlog Loss: -16.335622\n",
      "\n",
      "Train set: Average log loss: -16.3251\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5983\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2815 [0/83817 (0%)]\tlog Loss: -16.335692\n",
      "\n",
      "Train set: Average log loss: -16.3252\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2816 [0/83817 (0%)]\tlog Loss: -16.335754\n",
      "\n",
      "Train set: Average log loss: -16.3252\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5984\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2817 [0/83817 (0%)]\tlog Loss: -16.335735\n",
      "\n",
      "Train set: Average log loss: -16.3253\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2818 [0/83817 (0%)]\tlog Loss: -16.335911\n",
      "\n",
      "Train set: Average log loss: -16.3254\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2819 [0/83817 (0%)]\tlog Loss: -16.335793\n",
      "\n",
      "Train set: Average log loss: -16.3254\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5985\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2820 [0/83817 (0%)]\tlog Loss: -16.335977\n",
      "\n",
      "Train set: Average log loss: -16.3255\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2821 [0/83817 (0%)]\tlog Loss: -16.335999\n",
      "\n",
      "Train set: Average log loss: -16.3255\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2822 [0/83817 (0%)]\tlog Loss: -16.336057\n",
      "\n",
      "Train set: Average log loss: -16.3256\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5986\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2823 [0/83817 (0%)]\tlog Loss: -16.336105\n",
      "\n",
      "Train set: Average log loss: -16.3257\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2824 [0/83817 (0%)]\tlog Loss: -16.336119\n",
      "\n",
      "Train set: Average log loss: -16.3257\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2825 [0/83817 (0%)]\tlog Loss: -16.336240\n",
      "\n",
      "Train set: Average log loss: -16.3258\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5987\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2826 [0/83817 (0%)]\tlog Loss: -16.336212\n",
      "\n",
      "Train set: Average log loss: -16.3258\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2827 [0/83817 (0%)]\tlog Loss: -16.336355\n",
      "\n",
      "Train set: Average log loss: -16.3259\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5988\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2828 [0/83817 (0%)]\tlog Loss: -16.336347\n",
      "\n",
      "Train set: Average log loss: -16.3260\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2829 [0/83817 (0%)]\tlog Loss: -16.336426\n",
      "\n",
      "Train set: Average log loss: -16.3260\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2830 [0/83817 (0%)]\tlog Loss: -16.336455\n",
      "\n",
      "Train set: Average log loss: -16.3261\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5989\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2831 [0/83817 (0%)]\tlog Loss: -16.336516\n",
      "\n",
      "Train set: Average log loss: -16.3262\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2832 [0/83817 (0%)]\tlog Loss: -16.336573\n",
      "\n",
      "Train set: Average log loss: -16.3262\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2833 [0/83817 (0%)]\tlog Loss: -16.336619\n",
      "\n",
      "Train set: Average log loss: -16.3263\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5990\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2834 [0/83817 (0%)]\tlog Loss: -16.336706\n",
      "\n",
      "Train set: Average log loss: -16.3263\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2835 [0/83817 (0%)]\tlog Loss: -16.336788\n",
      "\n",
      "Train set: Average log loss: -16.3264\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2836 [0/83817 (0%)]\tlog Loss: -16.336855\n",
      "\n",
      "Train set: Average log loss: -16.3265\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5991\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2837 [0/83817 (0%)]\tlog Loss: -16.336918\n",
      "\n",
      "Train set: Average log loss: -16.3265\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2838 [0/83817 (0%)]\tlog Loss: -16.336903\n",
      "\n",
      "Train set: Average log loss: -16.3266\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2839 [0/83817 (0%)]\tlog Loss: -16.337042\n",
      "\n",
      "Train set: Average log loss: -16.3266\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5992\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2840 [0/83817 (0%)]\tlog Loss: -16.336997\n",
      "\n",
      "Train set: Average log loss: -16.3267\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5993\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2841 [0/83817 (0%)]\tlog Loss: -16.337103\n",
      "\n",
      "Train set: Average log loss: -16.3268\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5993\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2842 [0/83817 (0%)]\tlog Loss: -16.337186\n",
      "\n",
      "Train set: Average log loss: -16.3268\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2843 [0/83817 (0%)]\tlog Loss: -16.337220\n",
      "\n",
      "Train set: Average log loss: -16.3269\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2844 [0/83817 (0%)]\tlog Loss: -16.337320\n",
      "\n",
      "Train set: Average log loss: -16.3269\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5994\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2845 [0/83817 (0%)]\tlog Loss: -16.337373\n",
      "\n",
      "Train set: Average log loss: -16.3270\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2846 [0/83817 (0%)]\tlog Loss: -16.337416\n",
      "\n",
      "Train set: Average log loss: -16.3271\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5995\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2847 [0/83817 (0%)]\tlog Loss: -16.337446\n",
      "\n",
      "Train set: Average log loss: -16.3271\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2848 [0/83817 (0%)]\tlog Loss: -16.337541\n",
      "\n",
      "Train set: Average log loss: -16.3272\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2849 [0/83817 (0%)]\tlog Loss: -16.337580\n",
      "\n",
      "Train set: Average log loss: -16.3273\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2850 [0/83817 (0%)]\tlog Loss: -16.337658\n",
      "\n",
      "Train set: Average log loss: -16.3273\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5996\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2851 [0/83817 (0%)]\tlog Loss: -16.337701\n",
      "\n",
      "Train set: Average log loss: -16.3274\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2852 [0/83817 (0%)]\tlog Loss: -16.337772\n",
      "\n",
      "Train set: Average log loss: -16.3274\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5997\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2853 [0/83817 (0%)]\tlog Loss: -16.337789\n",
      "\n",
      "Train set: Average log loss: -16.3275\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5998\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2854 [0/83817 (0%)]\tlog Loss: -16.337900\n",
      "\n",
      "Train set: Average log loss: -16.3276\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5998\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2855 [0/83817 (0%)]\tlog Loss: -16.337991\n",
      "\n",
      "Train set: Average log loss: -16.3276\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2856 [0/83817 (0%)]\tlog Loss: -16.338007\n",
      "\n",
      "Train set: Average log loss: -16.3277\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2857 [0/83817 (0%)]\tlog Loss: -16.338047\n",
      "\n",
      "Train set: Average log loss: -16.3277\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2858 [0/83817 (0%)]\tlog Loss: -16.338072\n",
      "\n",
      "Train set: Average log loss: -16.3278\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.5999\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2859 [0/83817 (0%)]\tlog Loss: -16.338166\n",
      "\n",
      "Train set: Average log loss: -16.3278\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2860 [0/83817 (0%)]\tlog Loss: -16.338255\n",
      "\n",
      "Train set: Average log loss: -16.3279\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6000\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2861 [0/83817 (0%)]\tlog Loss: -16.338304\n",
      "\n",
      "Train set: Average log loss: -16.3280\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2862 [0/83817 (0%)]\tlog Loss: -16.338376\n",
      "\n",
      "Train set: Average log loss: -16.3280\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2863 [0/83817 (0%)]\tlog Loss: -16.338371\n",
      "\n",
      "Train set: Average log loss: -16.3281\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6001\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2864 [0/83817 (0%)]\tlog Loss: -16.338484\n",
      "\n",
      "Train set: Average log loss: -16.3282\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2865 [0/83817 (0%)]\tlog Loss: -16.338532\n",
      "\n",
      "Train set: Average log loss: -16.3282\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2866 [0/83817 (0%)]\tlog Loss: -16.338606\n",
      "\n",
      "Train set: Average log loss: -16.3283\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6002\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2867 [0/83817 (0%)]\tlog Loss: -16.338709\n",
      "\n",
      "Train set: Average log loss: -16.3284\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2868 [0/83817 (0%)]\tlog Loss: -16.338710\n",
      "\n",
      "Train set: Average log loss: -16.3284\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6003\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2869 [0/83817 (0%)]\tlog Loss: -16.338714\n",
      "\n",
      "Train set: Average log loss: -16.3285\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2870 [0/83817 (0%)]\tlog Loss: -16.338789\n",
      "\n",
      "Train set: Average log loss: -16.3285\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2871 [0/83817 (0%)]\tlog Loss: -16.338875\n",
      "\n",
      "Train set: Average log loss: -16.3286\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6004\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2872 [0/83817 (0%)]\tlog Loss: -16.338921\n",
      "\n",
      "Train set: Average log loss: -16.3287\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2873 [0/83817 (0%)]\tlog Loss: -16.338951\n",
      "\n",
      "Train set: Average log loss: -16.3287\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6005\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2874 [0/83817 (0%)]\tlog Loss: -16.339079\n",
      "\n",
      "Train set: Average log loss: -16.3288\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2875 [0/83817 (0%)]\tlog Loss: -16.339086\n",
      "\n",
      "Train set: Average log loss: -16.3288\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2876 [0/83817 (0%)]\tlog Loss: -16.339179\n",
      "\n",
      "Train set: Average log loss: -16.3289\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6006\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2877 [0/83817 (0%)]\tlog Loss: -16.339182\n",
      "\n",
      "Train set: Average log loss: -16.3289\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6007\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2878 [0/83817 (0%)]\tlog Loss: -16.339246\n",
      "\n",
      "Train set: Average log loss: -16.3290\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6007\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2879 [0/83817 (0%)]\tlog Loss: -16.339304\n",
      "\n",
      "Train set: Average log loss: -16.3291\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2880 [0/83817 (0%)]\tlog Loss: -16.339454\n",
      "\n",
      "Train set: Average log loss: -16.3291\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2881 [0/83817 (0%)]\tlog Loss: -16.339417\n",
      "\n",
      "Train set: Average log loss: -16.3292\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6008\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2882 [0/83817 (0%)]\tlog Loss: -16.339476\n",
      "\n",
      "Train set: Average log loss: -16.3293\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6009\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2883 [0/83817 (0%)]\tlog Loss: -16.339520\n",
      "\n",
      "Train set: Average log loss: -16.3293\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6009\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2884 [0/83817 (0%)]\tlog Loss: -16.339626\n",
      "\n",
      "Train set: Average log loss: -16.3294\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6009\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2885 [0/83817 (0%)]\tlog Loss: -16.339631\n",
      "\n",
      "Train set: Average log loss: -16.3294\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6009\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2886 [0/83817 (0%)]\tlog Loss: -16.339702\n",
      "\n",
      "Train set: Average log loss: -16.3295\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2887 [0/83817 (0%)]\tlog Loss: -16.339796\n",
      "\n",
      "Train set: Average log loss: -16.3296\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2888 [0/83817 (0%)]\tlog Loss: -16.339864\n",
      "\n",
      "Train set: Average log loss: -16.3296\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6010\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2889 [0/83817 (0%)]\tlog Loss: -16.339819\n",
      "\n",
      "Train set: Average log loss: -16.3297\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6011\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2890 [0/83817 (0%)]\tlog Loss: -16.339927\n",
      "\n",
      "Train set: Average log loss: -16.3297\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6011\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2891 [0/83817 (0%)]\tlog Loss: -16.339988\n",
      "\n",
      "Train set: Average log loss: -16.3298\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2892 [0/83817 (0%)]\tlog Loss: -16.340039\n",
      "\n",
      "Train set: Average log loss: -16.3299\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2893 [0/83817 (0%)]\tlog Loss: -16.340137\n",
      "\n",
      "Train set: Average log loss: -16.3299\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6012\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2894 [0/83817 (0%)]\tlog Loss: -16.340068\n",
      "\n",
      "Train set: Average log loss: -16.3300\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6013\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2895 [0/83817 (0%)]\tlog Loss: -16.340219\n",
      "\n",
      "Train set: Average log loss: -16.3301\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6013\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2896 [0/83817 (0%)]\tlog Loss: -16.340195\n",
      "\n",
      "Train set: Average log loss: -16.3301\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6013\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2897 [0/83817 (0%)]\tlog Loss: -16.340298\n",
      "\n",
      "Train set: Average log loss: -16.3302\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2898 [0/83817 (0%)]\tlog Loss: -16.340325\n",
      "\n",
      "Train set: Average log loss: -16.3302\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6014\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2899 [0/83817 (0%)]\tlog Loss: -16.340369\n",
      "\n",
      "Train set: Average log loss: -16.3303\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2900 [0/83817 (0%)]\tlog Loss: -16.340465\n",
      "\n",
      "Train set: Average log loss: -16.3303\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2901 [0/83817 (0%)]\tlog Loss: -16.340472\n",
      "\n",
      "Train set: Average log loss: -16.3304\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6015\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2902 [0/83817 (0%)]\tlog Loss: -16.340547\n",
      "\n",
      "Train set: Average log loss: -16.3305\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6016\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2903 [0/83817 (0%)]\tlog Loss: -16.340597\n",
      "\n",
      "Train set: Average log loss: -16.3305\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6016\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2904 [0/83817 (0%)]\tlog Loss: -16.340653\n",
      "\n",
      "Train set: Average log loss: -16.3306\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6016\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2905 [0/83817 (0%)]\tlog Loss: -16.340675\n",
      "\n",
      "Train set: Average log loss: -16.3306\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2906 [0/83817 (0%)]\tlog Loss: -16.340737\n",
      "\n",
      "Train set: Average log loss: -16.3307\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2907 [0/83817 (0%)]\tlog Loss: -16.340721\n",
      "\n",
      "Train set: Average log loss: -16.3308\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6017\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2908 [0/83817 (0%)]\tlog Loss: -16.340864\n",
      "\n",
      "Train set: Average log loss: -16.3308\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2909 [0/83817 (0%)]\tlog Loss: -16.340869\n",
      "\n",
      "Train set: Average log loss: -16.3309\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2910 [0/83817 (0%)]\tlog Loss: -16.341021\n",
      "\n",
      "Train set: Average log loss: -16.3310\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2911 [0/83817 (0%)]\tlog Loss: -16.340998\n",
      "\n",
      "Train set: Average log loss: -16.3310\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6018\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2912 [0/83817 (0%)]\tlog Loss: -16.340985\n",
      "\n",
      "Train set: Average log loss: -16.3311\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2913 [0/83817 (0%)]\tlog Loss: -16.341077\n",
      "\n",
      "Train set: Average log loss: -16.3311\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6019\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2914 [0/83817 (0%)]\tlog Loss: -16.341117\n",
      "\n",
      "Train set: Average log loss: -16.3312\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6020\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2915 [0/83817 (0%)]\tlog Loss: -16.341151\n",
      "\n",
      "Train set: Average log loss: -16.3312\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6020\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2916 [0/83817 (0%)]\tlog Loss: -16.341268\n",
      "\n",
      "Train set: Average log loss: -16.3313\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6020\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2917 [0/83817 (0%)]\tlog Loss: -16.341257\n",
      "\n",
      "Train set: Average log loss: -16.3314\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6021\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2918 [0/83817 (0%)]\tlog Loss: -16.341351\n",
      "\n",
      "Train set: Average log loss: -16.3314\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6021\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2919 [0/83817 (0%)]\tlog Loss: -16.341457\n",
      "\n",
      "Train set: Average log loss: -16.3315\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6021\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2920 [0/83817 (0%)]\tlog Loss: -16.341427\n",
      "\n",
      "Train set: Average log loss: -16.3316\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2921 [0/83817 (0%)]\tlog Loss: -16.341551\n",
      "\n",
      "Train set: Average log loss: -16.3316\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6022\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2922 [0/83817 (0%)]\tlog Loss: -16.341575\n",
      "\n",
      "Train set: Average log loss: -16.3317\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6023\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2923 [0/83817 (0%)]\tlog Loss: -16.341596\n",
      "\n",
      "Train set: Average log loss: -16.3317\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6023\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2924 [0/83817 (0%)]\tlog Loss: -16.341698\n",
      "\n",
      "Train set: Average log loss: -16.3318\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6023\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2925 [0/83817 (0%)]\tlog Loss: -16.341772\n",
      "\n",
      "Train set: Average log loss: -16.3318\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6023\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2926 [0/83817 (0%)]\tlog Loss: -16.341735\n",
      "\n",
      "Train set: Average log loss: -16.3319\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2927 [0/83817 (0%)]\tlog Loss: -16.341825\n",
      "\n",
      "Train set: Average log loss: -16.3319\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6024\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2928 [0/83817 (0%)]\tlog Loss: -16.341918\n",
      "\n",
      "Train set: Average log loss: -16.3320\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2929 [0/83817 (0%)]\tlog Loss: -16.341987\n",
      "\n",
      "Train set: Average log loss: -16.3321\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2930 [0/83817 (0%)]\tlog Loss: -16.342037\n",
      "\n",
      "Train set: Average log loss: -16.3322\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6025\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2931 [0/83817 (0%)]\tlog Loss: -16.342036\n",
      "\n",
      "Train set: Average log loss: -16.3322\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2932 [0/83817 (0%)]\tlog Loss: -16.342024\n",
      "\n",
      "Train set: Average log loss: -16.3322\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2933 [0/83817 (0%)]\tlog Loss: -16.342159\n",
      "\n",
      "Train set: Average log loss: -16.3323\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2934 [0/83817 (0%)]\tlog Loss: -16.342230\n",
      "\n",
      "Train set: Average log loss: -16.3324\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6026\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2935 [0/83817 (0%)]\tlog Loss: -16.342275\n",
      "\n",
      "Train set: Average log loss: -16.3324\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2936 [0/83817 (0%)]\tlog Loss: -16.342256\n",
      "\n",
      "Train set: Average log loss: -16.3325\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6027\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2937 [0/83817 (0%)]\tlog Loss: -16.342444\n",
      "\n",
      "Train set: Average log loss: -16.3326\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2938 [0/83817 (0%)]\tlog Loss: -16.342463\n",
      "\n",
      "Train set: Average log loss: -16.3326\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2939 [0/83817 (0%)]\tlog Loss: -16.342461\n",
      "\n",
      "Train set: Average log loss: -16.3327\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6028\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2940 [0/83817 (0%)]\tlog Loss: -16.342610\n",
      "\n",
      "Train set: Average log loss: -16.3327\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2941 [0/83817 (0%)]\tlog Loss: -16.342586\n",
      "\n",
      "Train set: Average log loss: -16.3328\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2942 [0/83817 (0%)]\tlog Loss: -16.342656\n",
      "\n",
      "Train set: Average log loss: -16.3329\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6029\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2943 [0/83817 (0%)]\tlog Loss: -16.342732\n",
      "\n",
      "Train set: Average log loss: -16.3329\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2944 [0/83817 (0%)]\tlog Loss: -16.342782\n",
      "\n",
      "Train set: Average log loss: -16.3330\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2945 [0/83817 (0%)]\tlog Loss: -16.342812\n",
      "\n",
      "Train set: Average log loss: -16.3330\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6030\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2946 [0/83817 (0%)]\tlog Loss: -16.342950\n",
      "\n",
      "Train set: Average log loss: -16.3331\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2947 [0/83817 (0%)]\tlog Loss: -16.342985\n",
      "\n",
      "Train set: Average log loss: -16.3331\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2948 [0/83817 (0%)]\tlog Loss: -16.343032\n",
      "\n",
      "Train set: Average log loss: -16.3332\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2949 [0/83817 (0%)]\tlog Loss: -16.343130\n",
      "\n",
      "Train set: Average log loss: -16.3333\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6031\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2950 [0/83817 (0%)]\tlog Loss: -16.343156\n",
      "\n",
      "Train set: Average log loss: -16.3333\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2951 [0/83817 (0%)]\tlog Loss: -16.343231\n",
      "\n",
      "Train set: Average log loss: -16.3334\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6032\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2952 [0/83817 (0%)]\tlog Loss: -16.343322\n",
      "\n",
      "Train set: Average log loss: -16.3334\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2953 [0/83817 (0%)]\tlog Loss: -16.343273\n",
      "\n",
      "Train set: Average log loss: -16.3335\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2954 [0/83817 (0%)]\tlog Loss: -16.343417\n",
      "\n",
      "Train set: Average log loss: -16.3336\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6033\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2955 [0/83817 (0%)]\tlog Loss: -16.343382\n",
      "\n",
      "Train set: Average log loss: -16.3336\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2956 [0/83817 (0%)]\tlog Loss: -16.343573\n",
      "\n",
      "Train set: Average log loss: -16.3337\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2957 [0/83817 (0%)]\tlog Loss: -16.343603\n",
      "\n",
      "Train set: Average log loss: -16.3337\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6034\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2958 [0/83817 (0%)]\tlog Loss: -16.343573\n",
      "\n",
      "Train set: Average log loss: -16.3338\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6035\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2959 [0/83817 (0%)]\tlog Loss: -16.343654\n",
      "\n",
      "Train set: Average log loss: -16.3339\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6035\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2960 [0/83817 (0%)]\tlog Loss: -16.343694\n",
      "\n",
      "Train set: Average log loss: -16.3339\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6035\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2961 [0/83817 (0%)]\tlog Loss: -16.343775\n",
      "\n",
      "Train set: Average log loss: -16.3340\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2962 [0/83817 (0%)]\tlog Loss: -16.343897\n",
      "\n",
      "Train set: Average log loss: -16.3341\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2963 [0/83817 (0%)]\tlog Loss: -16.343828\n",
      "\n",
      "Train set: Average log loss: -16.3341\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6036\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2964 [0/83817 (0%)]\tlog Loss: -16.343980\n",
      "\n",
      "Train set: Average log loss: -16.3342\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2965 [0/83817 (0%)]\tlog Loss: -16.344013\n",
      "\n",
      "Train set: Average log loss: -16.3342\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2966 [0/83817 (0%)]\tlog Loss: -16.344053\n",
      "\n",
      "Train set: Average log loss: -16.3343\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6037\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2967 [0/83817 (0%)]\tlog Loss: -16.344098\n",
      "\n",
      "Train set: Average log loss: -16.3343\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2968 [0/83817 (0%)]\tlog Loss: -16.344240\n",
      "\n",
      "Train set: Average log loss: -16.3344\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2969 [0/83817 (0%)]\tlog Loss: -16.344207\n",
      "\n",
      "Train set: Average log loss: -16.3345\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6038\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2970 [0/83817 (0%)]\tlog Loss: -16.344349\n",
      "\n",
      "Train set: Average log loss: -16.3345\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2971 [0/83817 (0%)]\tlog Loss: -16.344365\n",
      "\n",
      "Train set: Average log loss: -16.3346\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2972 [0/83817 (0%)]\tlog Loss: -16.344507\n",
      "\n",
      "Train set: Average log loss: -16.3346\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6039\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2973 [0/83817 (0%)]\tlog Loss: -16.344484\n",
      "\n",
      "Train set: Average log loss: -16.3347\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2974 [0/83817 (0%)]\tlog Loss: -16.344535\n",
      "\n",
      "Train set: Average log loss: -16.3348\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2975 [0/83817 (0%)]\tlog Loss: -16.344634\n",
      "\n",
      "Train set: Average log loss: -16.3348\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6040\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2976 [0/83817 (0%)]\tlog Loss: -16.344669\n",
      "\n",
      "Train set: Average log loss: -16.3349\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2977 [0/83817 (0%)]\tlog Loss: -16.344762\n",
      "\n",
      "Train set: Average log loss: -16.3349\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2978 [0/83817 (0%)]\tlog Loss: -16.344777\n",
      "\n",
      "Train set: Average log loss: -16.3350\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2979 [0/83817 (0%)]\tlog Loss: -16.344898\n",
      "\n",
      "Train set: Average log loss: -16.3351\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6041\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2980 [0/83817 (0%)]\tlog Loss: -16.344927\n",
      "\n",
      "Train set: Average log loss: -16.3351\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2981 [0/83817 (0%)]\tlog Loss: -16.345021\n",
      "\n",
      "Train set: Average log loss: -16.3352\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2982 [0/83817 (0%)]\tlog Loss: -16.345008\n",
      "\n",
      "Train set: Average log loss: -16.3352\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6042\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2983 [0/83817 (0%)]\tlog Loss: -16.345099\n",
      "\n",
      "Train set: Average log loss: -16.3353\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2984 [0/83817 (0%)]\tlog Loss: -16.345178\n",
      "\n",
      "Train set: Average log loss: -16.3354\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2985 [0/83817 (0%)]\tlog Loss: -16.345173\n",
      "\n",
      "Train set: Average log loss: -16.3354\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6043\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2986 [0/83817 (0%)]\tlog Loss: -16.345227\n",
      "\n",
      "Train set: Average log loss: -16.3355\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6044\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2987 [0/83817 (0%)]\tlog Loss: -16.345361\n",
      "\n",
      "Train set: Average log loss: -16.3355\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6044\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2988 [0/83817 (0%)]\tlog Loss: -16.345309\n",
      "\n",
      "Train set: Average log loss: -16.3356\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2989 [0/83817 (0%)]\tlog Loss: -16.345394\n",
      "\n",
      "Train set: Average log loss: -16.3357\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2990 [0/83817 (0%)]\tlog Loss: -16.345485\n",
      "\n",
      "Train set: Average log loss: -16.3357\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2991 [0/83817 (0%)]\tlog Loss: -16.345541\n",
      "\n",
      "Train set: Average log loss: -16.3358\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6045\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2992 [0/83817 (0%)]\tlog Loss: -16.345515\n",
      "\n",
      "Train set: Average log loss: -16.3358\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2993 [0/83817 (0%)]\tlog Loss: -16.345630\n",
      "\n",
      "Train set: Average log loss: -16.3359\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2994 [0/83817 (0%)]\tlog Loss: -16.345678\n",
      "\n",
      "Train set: Average log loss: -16.3360\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6046\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2995 [0/83817 (0%)]\tlog Loss: -16.345688\n",
      "\n",
      "Train set: Average log loss: -16.3360\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2996 [0/83817 (0%)]\tlog Loss: -16.345733\n",
      "\n",
      "Train set: Average log loss: -16.3361\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2997 [0/83817 (0%)]\tlog Loss: -16.345781\n",
      "\n",
      "Train set: Average log loss: -16.3361\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6047\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2998 [0/83817 (0%)]\tlog Loss: -16.345877\n",
      "\n",
      "Train set: Average log loss: -16.3362\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 2999 [0/83817 (0%)]\tlog Loss: -16.345904\n",
      "\n",
      "Train set: Average log loss: -16.3363\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n",
      "Train Epoch: 3000 [0/83817 (0%)]\tlog Loss: -16.345944\n",
      "\n",
      "Train set: Average log loss: -16.3363\n",
      "\n",
      "\n",
      "Test set: Average loss: -15.6048\n",
      "\n",
      "Current Learning rate 1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "args =  {\"batch_size\": 1024,\n",
    "         \"test_batch_size\": 4048,\n",
    "         \"epochs\" : 3*10**3,\n",
    "         \"lr\": 1e-4,\n",
    "         \"gamma\": .1,\n",
    "         \"no_cuda\" : False,\n",
    "         \"run_dry\": False,\n",
    "         \"seed\": 0,\n",
    "         \"log_interval\" : 100,\n",
    "         \"dry_run\" : False,\n",
    "         \"save_model\": True}\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "\n",
    "# Loading Train / Test Data\n",
    "hs_dataset = Heston_LHS_data_generator(n = 10**5)\n",
    "train_size, test_size = int(hs_dataset.shape[0]*0.9), int(hs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(hs_dataset, [train_size, hs_dataset.shape[0]-train_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])\n",
    "\n",
    "\n",
    "# Model training\n",
    "print(device)\n",
    "model = Heston_ANN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"]) # Adam is found to be the best optimizer in the article\n",
    "scheduler = StepLR(optimizer, step_size=10**3, gamma=args[\"gamma\"]) # Deacreses the Learning rate by .1 every 1000 epoch\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "  train_loss = model.train_model(args, device, train_loader, optimizer, epoch)\n",
    "  test_loss = model.test_model(device, test_loader)\n",
    "  print(\"Current Learning rate {}\".format(scheduler.get_last_lr()[0]))\n",
    "  scheduler.step()\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "if args[\"save_model\"] :\n",
    "  torch.save(model.state_dict(), \"Heston_ANN.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "rEIyI-TaRaCZ",
    "outputId": "16eb6362-1a2b-4bb1-9f60-07947e1de77e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WIDE 8.039468683046863e-08 VS in the article 8.04e-09\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting log(MSE) as a function of epochs\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot( np.log(test_losses), label = \"test\")\n",
    "plt.plot( np.log(train_losses), label = \"train\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log(LSE)\")\n",
    "plt.title(\"MSE over Epoch\" )\n",
    "plt.grid()\n",
    "print(\"Training WIDE {} VS in the article 8.04e-09\".format(train_losses[-1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQX439O8R4Q3",
    "outputId": "707adfbb-3420-4563-8376-1df000c402c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Heston_ANN(\n",
       "  (fc1): Linear(in_features=8, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc3): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc4): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (fc5): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model\n",
    "device = 'cuda'\n",
    "loaded_Heston_ANN = Heston_ANN().to(device)\n",
    "PATH = \"/content/Heston_ANN.pt\"\n",
    "loaded_Heston_ANN.load_state_dict(torch.load(PATH))\n",
    "loaded_Heston_ANN.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQ0JXu7pjTIb"
   },
   "outputs": [],
   "source": [
    "# Loading Train / Test Data\n",
    "hs_dataset = Heston_LHS_data_generator(n = 10**5)\n",
    "train_size, test_size = int(hs_dataset.shape[0]*0.9), int(hs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(hs_dataset, [train_size, hs_dataset.shape[0]-train_size])\n",
    "args =  {\"batch_size\": 1024,\n",
    "         \"test_batch_size\": 4048,\n",
    "         \"epochs\" : 3*10**3,\n",
    "         \"lr\": 1e-4,\n",
    "         \"gamma\": .1,\n",
    "         \"no_cuda\" : False,\n",
    "         \"run_dry\": False,\n",
    "         \"seed\": 0,\n",
    "         \"log_interval\" : 100,\n",
    "         \"dry_run\" : False,\n",
    "         \"save_model\": True}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "1TYVynE6ReL4",
    "outputId": "dd88fc14-1450-4681-f6b9-a1488221b0fd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX  0.0022023022\n",
      "MIN  -0.004330963\n"
     ]
    }
   ],
   "source": [
    "# Plotting erros histogram\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_Heston_ANN(data)\n",
    "    errors.append((target-output).data.cpu().numpy())\n",
    "  errors = np.concatenate(errors)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(errors,50,fill=False)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Difference\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim([-0.003, 0.003])\n",
    "plt.show()\n",
    "print(\"MAX \",errors.max())\n",
    "print(\"MIN \",errors.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fdy_UxgmRiVo",
    "outputId": "72a26728-24c6-4440-a6f5-2b766606dbfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Train scores----------------\n",
      "MSE 7.917703e-08\n",
      "MAE 2.222558e-04\n",
      "MAPE 4.460891e-02\n",
      "R2 9.999964e-01\n",
      "-----------Test scores----------------\n",
      "MSE 1.670718e-07\n",
      "MAE 3.111258e-04\n",
      "MAPE 1.291358e-02\n",
      "R2 9.999922e-01\n"
     ]
    }
   ],
   "source": [
    "# printing train and test erros MSE, MAE, MAPE, R2.\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_Heston_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Train scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )\n",
    "\n",
    "\n",
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_Heston_ANN(data)\n",
    "    outputs.append((output).data.cpu().numpy())\n",
    "    targets.append((target).data.cpu().numpy())\n",
    "  outputs = np.concatenate(outputs)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "print(\"-----------Test scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Boa8FA_ZoSuw",
    "outputId": "faa492fa-c559-4a5a-a950-0c0fdb4d45d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004087441742704108"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(1.670718e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2J3aEAhSflZ"
   },
   "source": [
    "# 5) BRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7zMZ3e1Si9G",
    "outputId": "e30d064a-55de-470d-a681-dc5cf3f3ee6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ INPUT ------\n",
      "S/K.......... [0.40 1.60]\n",
      "Tau.......... [0.20 1.10]\n",
      "r............ [0.02 0.10]\n",
      "Sigma........ [0.01 1.00]\n",
      "------ OUTPUT ------\n",
      "V/K.......... [0.00 0.88]\n",
      "MSE  0.00014923181750163143\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def calcimpliedvol(dataset):\n",
    "    \"\"\"\n",
    "    Computes the implied volatilities using the brent method.\n",
    "    dataset: numpy array containing moneyness, time to maturity, risk free interest rate and V/K.\n",
    "    Returns a vector of implied volatilities.\n",
    "    \"\"\"\n",
    "    money = np.array(dataset[0])\n",
    "    T = np.array(dataset[1])\n",
    "    r = np.array(dataset[2])\n",
    "    marketoptionPrice = np.array(dataset[3])\n",
    "    def bs_price(sigma):\n",
    "      d1 = ( np.log(money) + (r + 0.5 * sigma ** 2) * (T) ) / (sigma * np.sqrt(T))\n",
    "      d2 = d1 - sigma*np.sqrt(T)\n",
    "      value_moyeness = money * norm.cdf(d1) - np.exp(-r * (T)) * norm.cdf(d2) \n",
    "      fx = value_moyeness - marketoptionPrice\n",
    "      return fx\n",
    "    return optimize.brentq(bs_price,-1,1,maxiter=100)\n",
    "\n",
    "bs_dataset = bs_LHS_data_generator(n=10**4).data\n",
    "ground_truth_vol = bs_dataset[:,3]\n",
    "bs_dataset = bs_dataset[:,np.array([0,1,2,4])]\n",
    "vol_imp = np.apply_along_axis(calcimpliedvol, 1, bs_dataset)\n",
    "print(\"MSE \", ((ground_truth_vol - vol_imp)**2).mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3X7W_KAhSr"
   },
   "source": [
    "# 6) Pipeline 1: COS + BRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhQUeGPqAmt1",
    "outputId": "69defb04-1caa-4a6a-d307-f25ed0be81e2"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ INPUT ------\n",
      "S/K.......... [0.76 1.24]\n",
      "Tau.......... [0.41 0.99]\n",
      "r............ [0.01 0.09]\n",
      "Corr ........ [-0.94 -0.01]\n",
      "Kappa........ [0.41 1.99]\n",
      "V_bar........ [0.01 0.49]\n",
      "Gamma........ [0.01 0.49]\n",
      "V_0 ......... [0.06 0.49]\n",
      "------ OUTPUT ------\n",
      "V/K.......... [0.00 0.46]\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-2 # is used to generate open intervals\n",
    "# (S/K), (tau), (r), (rho), (kappa), (vbar), (gamma), (v0).\n",
    "# CASE 1 in table 11\n",
    "l_bounds = [.75+eps, .4+eps, .0+eps, -0.95+eps, .4+eps, .0+eps, .0+eps, .05+eps]\n",
    "u_bounds = [1.25-eps, 1.-eps, .1-eps, 0.0-eps, 2.0-eps, .5-eps, .5-eps, .5-eps]\n",
    "hs_dataset = Heston_LHS_data_generator(10**5, l_bounds = l_bounds, u_bounds=u_bounds)\n",
    "hs_dataset_data = hs_dataset.data\n",
    "hs_dataset_data = hs_dataset_data[:,np.array([0,1,2,8])]\n",
    "implied_vol_pipe1 = np.apply_along_axis(calcimpliedvol, 1, hs_dataset_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzV98IpxAm3_"
   },
   "source": [
    "# 7) Pipeline 2: Heston-ANN + IV-ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zrch0eIxAszs"
   },
   "outputs": [],
   "source": [
    "# Loading model\n",
    "device = torch.device(\"cuda\")\n",
    "loaded_Heston_ANN = Heston_ANN().to(device)\n",
    "PATH = \"/content/Heston_ANN.pt\"\n",
    "loaded_Heston_ANN.load_state_dict(torch.load(PATH))\n",
    "loaded_Heston_ANN.eval()\n",
    "\n",
    "# data loader \n",
    "loader = torch.utils.data.DataLoader(hs_dataset, 4048)\n",
    "# Computing prices\n",
    "prices = []\n",
    "with torch.no_grad():\n",
    "  for data in loader:\n",
    "    data, _ = data[:,:-1].to(device), data[:,-1].to(device)\n",
    "    output = loaded_Heston_ANN(data)\n",
    "    prices.append((output).data.cpu().numpy())\n",
    "  prices = np.concatenate(prices)\n",
    "\n",
    "# Loading model \n",
    "device = torch.device(\"cuda\")\n",
    "loaded_IV_ANN = IV_ANN().to(device)\n",
    "PATH = \"/content/IV_SCALED_ANN.pt\"\n",
    "loaded_IV_ANN.load_state_dict(torch.load(PATH))\n",
    "loaded_IV_ANN.eval()\n",
    "\n",
    "\n",
    "# Plugging in the Heston-ANN prices\n",
    "hs_dataset2 = hs_dataset.data[:,np.array([0,1,2,8])]\n",
    "hs_dataset2.data[:,3] = torch.FloatTensor(prices)\n",
    "loader2 = torch.utils.data.DataLoader(hs_dataset2, 4048)\n",
    "\n",
    "# Computing IVs\n",
    "implied_vol = []\n",
    "indecies_list = []\n",
    "with torch.no_grad():\n",
    "  for idx, data in enumerate(loader2):\n",
    "    data = data.to(device)\n",
    "    # LOG Transformation\n",
    "    data[:,3] = data[:,3] - torch.maximum(data[:,0]-torch.exp(-data[:,1]*data[:,2]),torch.zeros(data[:,0].shape).to(device))\n",
    "    data[:,3] = torch.log(data[:,3])\n",
    "    output = loaded_IV_ANN(data)\n",
    "    implied_vol.append((output).data.cpu().numpy())\n",
    "  implied_vol_pipe2 = np.concatenate(implied_vol)\n",
    "  # indecies_list = np.concatenate(indecies_list).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmdMT4oKvBvO"
   },
   "outputs": [],
   "source": [
    "# Eliminating Nan values resulting from prices lower than 1e-7\n",
    "indecies = np.argwhere(np.isnan(implied_vol_pipe2))\n",
    "implied_vol_pipe1 = np.delete(implied_vol_pipe1, indecies,0)\n",
    "implied_vol_pipe2 = np.delete(implied_vol_pipe2, indecies,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKD1CeSZkxcO"
   },
   "source": [
    "# 8) Pipeline 1 VS pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6heX_6z9gaRF",
    "outputId": "8423e860-9186-4273-abe4-6c7393b8da26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Vol scores----------------\n",
      "MSE 5.435221e-05\n",
      "RMSE 7.372395e-03\n",
      "MAE 3.443714e-03\n",
      "MAPE 7.440735e-03\n",
      "R2 9.945507e-01\n"
     ]
    }
   ],
   "source": [
    "# CASE 1\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "outputs = implied_vol_pipe2\n",
    "targets = implied_vol_pipe1\n",
    "print(\"-----------Vol scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"RMSE {:e}\".format(np.sqrt(mean_squared_error(outputs,targets))) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmRj5W23v81E",
    "outputId": "e43a6959-6354-4853-893d-772c31abdf2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Vol scores----------------\n",
      "MSE 2.079396e-06\n",
      "RMSE 1.442011e-03\n",
      "MAE 1.115939e-03\n",
      "MAPE 2.361352e-03\n",
      "R2 9.997805e-01\n"
     ]
    }
   ],
   "source": [
    "# CASE 2\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "outputs = implied_vol_pipe2\n",
    "targets = implied_vol_pipe1\n",
    "print(\"-----------Vol scores----------------\")\n",
    "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
    "print(\"RMSE {:e}\".format(np.sqrt(mean_squared_error(outputs,targets))) )\n",
    "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
    "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
    "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLwRkWB09WAh"
   },
   "source": [
    "# 9) Volatility surface (COS + IV-ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYbO_aqdEcyQ"
   },
   "source": [
    "### Initial volatility surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOh3UfV4gdbm"
   },
   "outputs": [],
   "source": [
    "# Generating the surface points\n",
    "r= .02\n",
    "tau= 0.5\n",
    "rho = -0.05\n",
    "kappa= 1.5\n",
    "vbar = .1\n",
    "gamma = .3\n",
    "v0 = 0.1\n",
    "moyeness = 0.74\n",
    "moyeness = np.linspace(moyeness, 1.24,11)\n",
    "tau = np.linspace(tau, 1, 6)\n",
    "\n",
    "cos_prices = []\n",
    "X = []\n",
    "Y = []\n",
    "for m in moyeness:\n",
    "  for t in tau:\n",
    "    X.append(m)\n",
    "    Y.append(t)\n",
    "    cos_prices.append(call_option_Heston_moy(N = 160, L= 12, r= r, tau= t, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, moyeness = m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_vzIM26hchq"
   },
   "outputs": [],
   "source": [
    "# Reshaping for torch tensor dataset\n",
    "cos_prices = np.array(cos_prices).reshape((-1, 1))\n",
    "X = np.array(X).reshape((-1, 1))\n",
    "Y = np.array(Y).reshape((-1, 1))\n",
    "r = np.array(r*np.ones(66)).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kB2nF_gWD10-"
   },
   "outputs": [],
   "source": [
    "# Creating a torch dataset\n",
    "data = np.concatenate((X, Y, r, cos_prices), axis = 1)\n",
    "data = torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmE2kJOhDlzY"
   },
   "outputs": [],
   "source": [
    "# Getting the implied volatility from the dataset\n",
    "with torch.no_grad():\n",
    "  data = data.to(device)\n",
    "  data[:,3] = data[:,3] - torch.maximum(data[:,0]-torch.exp(-data[:,1]*data[:,2]),torch.zeros(data[:,0].shape).to(device))\n",
    "  data[:,3] = torch.log(data[:,3])\n",
    "  output = loaded_IV_ANN(data)\n",
    "  bs_IV = (output).data.cpu().numpy()\n",
    "\n",
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "Z = bs_IV\n",
    "\n",
    "x = np.reshape(X, (-1, 6))\n",
    "y = np.reshape(Y, (-1, 6))\n",
    "z = np.reshape(Z, (-1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn3cqcg1FCaM"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "gpj28yIi1SO2",
    "outputId": "83479b92-a03d-4f69-f41f-8bc0308b94db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAIuCAYAAAC7EdIKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeXQc5Znun6reF+27Wi2pJdkG2bINtsEmbGGGkHiCyRCSGE4ScrjMHIiTkMvMzWUyMyQBspA7h4SMPRBmspBkiJOb5BJmYDwYCCGAJMu2bEu2jLValrxosfZu9VJV9w9T5epN6r0Wvb9zdCx3V1d93aqu76n3e9/3YQRBAEEQBEEQhJ5hlR4AQRAEQRBEtiHBQxAEQRCE7iHBQxAEQRCE7iHBQxAEQRCE7iHBQxAEQRCE7iHBQxAEQRCE7jEu8zzVrBMEQRAEoRWYeE9QhIcgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCIIgCN1DgocgCCIHCIIAnueVHgZBrFiMSg+AIAhCTwiCIIkbjuOkH57nwTAMrFYrjEYjGIYBwzBKD5cgVgyMIAhLPb/kkwRBECsVUdTEEjbybURhwzCMJHQAgGVZGI1GsCxLwocgMkfcLxMJHoIgiDjIozWRwibWtVMubmJhMBhgMBik14qCyGg0wmAwkPAhiPQhwUMQBBEPuaiJFDaRyAVNsgJFFDyRxxaFj8FgCIsCEQSRNCR4CIIgUlmGyqT4iCV45McVr8ehUAgOhwMsS3UlBJEkcb+wlLRMEISuiEwalosbMXE4UtSoIY9GHIsgCOjo6MC2bdvAsixMJhMlOBNEBiDBQxCEJomM1siFTSSiYNBKnow4Xp7n4ff7KcGZIDIACR6CIFSLPFojCELUMtT4+DgMBgOKiopUFa3JBPKIjyAICAQClOBMEGlAgocgCMVZqndNZEWTXNgEg0EIggCjUT+XMvF9isgTpAVBQDAYRDAYhNFopARngkgC/VwlCIJQPYkkDYtkK3FYy8ijPqFQCKFQSIr4UIIzQSwNCR6CIDJKZO8anucRCoWk3+Xb6W0ZKhNERnhiEUv4UIIzQSwNCR6CIFIiXu8a+TKUCAmb5Ej0M5ILH3mCs1j+Tp81QVyGBA9BEEuSSu8aWl5JnWV6o8UkMsE5GAyGLXeR8CEIEjwEQSCx3jUiaorWpCIOlCSZyE06+6cEZ4KIhgQPQawg4i1DzczMIBQKobCwUNpWK71r1Dy2VMiUiIuV5yNaV1AEjliJkOAhCB2S7DKUz+eD3+9HSUmJgqMmgMSSlpNBLnzE84AaGRIrERI8BKFRUu1dE2uCo0lP/0QmOFMjQ2KlQYKHIFROrnrXaC0fRq9kOsITiVz4AMD58+fBsizKysooz4fQNSR4CEIFKN27hiY5dZGLv4d4DK/XC5ZlwxoZkvAh9AgJHoLIIYksQ4moqRqKyB25jrQJggCWZcGybMwEZ2pkSOgFEjwEkQWodw2RDrkWGPJydkpwJvQKCR6CSJHIZahQKITZ2VnYbLaY0RoAqp005DkdhLIoEeGJhJzaCT1CgocgliFe7xqO48K243keJ06cwObNm2kZgEiZbCctJ3O8WI0MI5e7CEIrkOAhiPdJdhkqMlojLknRJEBoiUQFFjm1E1qHBA+xokjEQiHR3jV6Q4tLWloc83LkOsIDJCfSl+rgTJFNQs2Q4CF0SWS0Ri5sIsmUhYKWL/Q09pVLqgKLEpwJrUGCh9AssXrXJLsMRRDZhOd5eL1eLCwswOv1gud5uN1uOJ3OuK9RIocnHSjBmdAKJHgI1UO9awi1EwqFwoTNwsICFhcXwbIsbDYbHA4HnE4nDAYDenp6YDAY0NDQEGbWKkctScvJQAnOhNohwUOohlQsFChZMnPoMR8m0wSDQSwsLIQJm0AgAIPBALvdDofDgcLCQlRXV8NqtUZN8kajEZWVlZiZmcHAwACCwSA8Hg9KS0ulbZUoS8+0GKEEZ0KNkOAhcspyFgrnz5+H1WqV7nwpWpMb6PO9jLgsIxc1CwsL0qTtcDjgcDhQUlKC2tpamM3mpD+/goICXHXVVVhYWMDg4CD6+vpQV1eHyspKANqM8MQilvBhWRYmk4kSnImcQ4KHyApL5dYstQwVDAZhMplgMBgUGjmxUhAEAYuLi2HCxuv1guM4WCwWKWJTWVkJu90Ok8mU8TE4HA6sW7cOi4uLOH36NAYHB1FaWhozqpktcpEzFCl8urq60NLSQgnORE4hwUOkRaYtFKjjL5FpeJ6Hz+eLEjaCIMBqtUoRm8LCQjgcDkXEttVqxZo1a9DQ0IDe3l5MTEygv78ftbW1WRFaSiFeAxYWFijBmcg5JHiIZUmkd41IuknDJHiUQ+ufO8dxMROHAUiJww6HA6WlpbDb7arMJTGZTHC5XOA4DiaTCQcOHEBJSQnq6+thtVqzckwlqsLE64M8wTkYDJJTO5FVSPAQEkr0rom1X61PvFpESxNMKBSS8momJibAcRxOnz4NlmWlZaj8/HxUVlbCZrNp6r0BlwVBbW0t3G43zp8/j87OTjidTng8niVL2lM9npI5Q5TgTOQKEjwrDHm0Rt4wTC29a0jwECKxEofFKIAobGw2GwoKClBdXa05YbMU8hLvqqoqVFZWYnJyEj09PTAajfB4PHFL2pNFacEjQgnORLYhwaNTEuldo0YLBRI8yqHE5y4IAvx+f5SwEZd0xGWosrIy1NfXw2w2h71+aGhImhD1Qjz38tLSUpSWli5Z0p4qahA88rGI1wGe5+H3+8GyLAwGA+X5EGlBgkfjRC5Dzc3NwWw2S4InVuhYzXdLDMPktEKFuES2zwdBEGImDvM8D4vFIgmbqqoqOBwOGI0r+9K01N9jqZL2VJaA1BLhiSSyg7PYyJASnIlUWdlXFY2wXO8a+XbHjx9HS0uLZu96KcKjbUQrBXm0xufzAUBYRVRxcTHsdju1H4hBoud/rJJ2t9sNl8uV1Oea6+9bZKHDcsTq4EwJzkQqkOBREZmwUJBXP2gREjzagOO4qI7DsawUysvLYbPZKPk0CZKNuMhL2oeHh9Ha2oqqqqqES9rVGuGJBSU4E+lAgkcBMt27Rg7LspoWDCR4lCPW5y5aKcgjNpFWCmLScCwrBSKcbH4+JpMJjY2NqK+vx+joaMIl7VoSPCKU4EykAgmeLLFU75pY0RoAGUka1noODAme3CNOGouLixgZGcmKlQKROOkKAoPBgNraWtTU1ODChQsJlbRrTfCIUIIzkQwkeNIknoXCUr1rsnkHonXBoPXxq5mlrBRYlgXP82BZFhUVFXA4HLrq8LsSYVk2oZJ2vZmVUoIzEQ8SPCnS1dWFN954A/fee6/0mFK9a+SIE5dWIcGTPqKVgnwZKp6Vgt1uh9FoxPT0NMbGxlBdXa308Fc8mRYEy5W0a3FJKx6U4EwsBQmeFJmdncWxY8dU0btGjtYFg9bHn0syaaVAn7u6yNY1RSxpn5+fx9DQEPr6+mIWRWSTXAmsyDyf/v5+eDwemEwmSnBeoZDgSRGLxSIZ36kJrU9cWh9/NhCtFOQRGzFXQQ9WCumgx3MlF+/J6XRKJe3t7e04fPgwamtrky5pT4VcR5RE4XPu3DnU1dVJ3x1yal95kOBJEbPZjEAgoPQwoqAlLeVJ9YIez0rBYDBI0ZqioiLU1NTAYrHQhfp99Pg55Oo9iUucV155Jc6fP590SXsq5FrwyBGrWMmpfWVCgidFxAiP2tC6YNBLWX28i2e6VgqE/lEiiTjVkvZUj6ekuIiV5xMKhWAwGCjPR+eQ4EkRq9UKv9+v9DCioAiPOtCalYJePnc9oGQScayS9ry8PNTX12fMpV1pwSOHGhmuLEjwpIhal7S0PnFprY9QpJWCz+fDoUOHAFxeLrDb7WSlQKiWWAIk0ZL2VI+nNjERS/jIIz5qEWhEepDgSRG1LmlRhCc7xLNSYBhGShx2Op2wWCzYsGGDJpei1Pi5641EohtqKhOXl7RPT09jcHAQoVAI9fX1Kbu0qynCE4lc+Ij91CjBWT+Q4EmRnC9pCQKQwJdNrYIhUZQef7pWCmfOnFHd3Wsi0IVcXahF8MgpLCyMKmmvr69HRUVFUue8mgWPSGQjQ0pw1gckeFLEbDYjGAzm8IhBAMtHDSjCszziBSxS2GTCSkFpwUZoHyXOn2QmcHlJ+9DQEAYGBpJyadeC4BGhBGd9QYInRXJfTcQAgg9gbEtvpfEJN5PjX8pKwWw2S8KGrBQItZHrCE8qWK1WXHHFFQgGg0m5tGtJ8MihBGftQ4InRXIuLBgTGH4GAgmeKGJZKfh8PvA8H5Y47HK5JCuFbKHlz1+r49YSiUz0ao/wRJJsSbtWBY8IJThrFxI8GkJgDIDgBRh73G30vKSVSSuFbKFVwUMXafWgVUGQaEk7z/OafH+RUIKz9iDBkwbLNZnL/AGLwPCDEBjPsmPSKmJZ+szMzJJWCnl5eSvSSoEg1I68pH1iYkIqaW9oaEBBQQGA3AvsbBuWUoKzNiDBkyKKCgvBDzCWmE9pSfDEShwOBALw+/04d+6cJq0UtPT56wGtfNZivyaz2bxsAz+tRngiYRgGZWVlKCsrw/T0NAYGBhAKhaQWDrkkF1ElSnBWPyR4NIbA1IPlu8EbWmI+r7YlrVhWCl6vF6FQKKaVAsuyOHbsGK644gqlh54SWhY8Wh23miYSMZ9MFPBiPhkA2O12KdFVHu1YCchL2ru6ujA+Pg6TyZR0SXuqKGVYKggCOjs7sXr1ajgcDkpwVhgSPGmgyIWWYQCEACEEMNF/PqUmXNFKQR6tibRSsNvty1opcByn2YlXy6hJNGiB5YSNGMWoqKiA1WqVJjmTyYSZmRn09/dDEAQ0NDSgqKgobN96ifDEwul0oqqqCgAwMzOTdEl7qijV3ZlhGASDQbAsKyU4sywLk8lECc4KQIInTXiez7ldAM82g+U7wRu2RD2X7QhPpJWC/EKfCSsFLUdIAO2PnwhnKWEjT5QvLy+HzWZLaFItLCzE1VdfjdnZWQwMDKC3txcNDQ0oKSkJWxbRK4IgwGKxoL6+Xippb2trQ2VlZdZc2pVMlOZ5HizLSq1MeJ6XchIpwTm3kOBJA7H5YM79kRgLGGE+ZvflTE24opWCXNhEWikke6FPBK0LBq2Pf6WSDWGzHPn5+di4cSPm5+cxMDCA/v5+eDweaYLUK/IIVqyS9tLSUtTV1WXUpV3JqJk8ukQJzspCgicNLBYL/H5/Rr+YicIZ1oPlO8Abrgl7PNkITzAYDBM1yVopZBr6wivDShFqchf7+fl5KVoJZE/YLIfT6cT69evh9XoxODiI8fFxlJeX63ZpK9b7ilfS7vF44HA4MnJMpURkrOhSrATnYDAIo9FICc5ZhARPGpjN5tz6aclhSsDy74CPCC7FmriyaaVAhLNShIPakQsbeU4ZoJywWQ673Y61a9eit7cX09PTaG1tRV1dHaqqqlQxvkyxlJCLLGk/fvw4TCZT2kneSi5pLSdcqYNz7iDBkwZms1lRx3SeXQOWOwresAEApDsFr9eL4eHhmFYKdrudrBSyCAme3CJWAY6Pj0cJGzGnzOl0KtqMMlmMRiNcLhdKS0sxNDSE1tZWuN1u1NTUaGL8y5FI5CpeSbvH4wnLdcrkMbNJIseOJXwowTmzkOBJA4vFoojguWylUAwrvxfDE0Z4vV4IggCj0YhgMAiTyZQTKwVCP6hZqMWL2Pj9fthsNhQVFSneZTvTmM1mrF69Gh6PB6dPn0ZraytcLhfcbnfu8wYzSLLiQ17SPjg4iN7eXtTX16OysjLh/SgteJJBLnzkCc4Gg4HyfNKEZsI0EHN4soVopRCZOAxcDstb82rRUMfBYt8ElmWxsLCAoaEhqfSTyC1ajfCo5SIqN3ydn5+XkocFQZAiNnJhMzQ0hPz8fJSWlio99IwROTmbTCY0NTWhvr4eZ86cQVtbm2TSqcWbmVTFh9PpREtLS0ou7VpMBI9McBYbGVKCc+po79uiIjK1pBUKhaLya+JZKch7egAAhFqYgj9CkF0PQH2NB1caWhU8uUYubCL7NqnFF01JYk1mRqMRHo8HtbW1GBkZQXt7O8rLy1FXVwez2azAKFMj3WhLKi7tWorwREIJzpmDBE8aJCt4YiUOi2Xt4gU+aSsFxgCBKQTDj0JgXTThEqpiKWEjj9iUlJTAZrMlvVSjx3N9ufdkMBhQV1cHt9uNs2fP4uDBg5I7ucUS23JGTWSqYiqZknYtCx45Szm1r7SbglQgwZMGFotFWmISSdZKQUxIS4eQ8S9gCv4UQfMuqbkVoQxaFpzpjDtRYZNqQ8ql0NJEluhYE9mOZVnU1NSguroa58+fx6FDh1BYWAiPxwObzZbuULNGpsWHvKT9/PnzMUvatbiktRTk1J4auhI8+/btw0MPPQSO43D//ffjkUceCXv+2WefxZ49e2AwGOB0OvHcc8+hubkZk5OTuOuuu9DR0YHPfe5z2L17t/SaX/7yl/jWt74FhmFQXV2NX/ziFygtLQXHcfD7/XjnnXfw2muvobq6Gps2bUraSiEjMA4ADCBcBMPk0ZKWgmhV8CST/CkKenkvm1wIm5VCsucPy7Korq5GVVUVxsbGcPToUTidzoz1sMk02Yq2yD+HyJJ2vUR4IqFGhsmhG8HDcRx27dqF/fv3o6amBlu2bMGOHTvQ3NwsbXPPPffggQceAAC89NJLePjhh7Fv3z5YrVY8/vjj6O7uRnd3t7R9KBTCQw89hBMnTiA/Px+33HILbr75ZthsNvA8j4WFBaxbtw7XXXcdtm3bhiuvvFKxC3zIeAdMwV8hxP6VJidcQl1EChvxJ1LYuN1uEjYZJtXJmWEYVFRUoLy8HBMTE+ju7obVakVDQwPy8vLiHivXZFt8RJa09/f3w+fzweFw6Fr4yCGn9tjoRvAcOHAATU1NaGhoAADs3LkTv//978MET35+vvT7wsKCdBI4HA5cf/316OvrC9unqJoXFhZQXFwMh8OBW2+9FV/96ldhMpnwta99DatWrcIdd9yRg3e4NALrAiOMg2W8FOFREK1FeERhMzU1BZ/Ph56eHknYiJFKh8OBmpoaEjYZIBcTrjjhl5aWYmpqCidPnlzSoT3Xk2EuRYfoW3b69GmMjIygra0t6ZL2dMj1tSAywbmrqwsejwdOp5MaGUJHgmd0dBRut1v6f01NDdrb26O227NnD5566ikEAgG88cYbS+7TZDLhmWeeQUtLCxwOB1atWoV/+Id/kC76SvXhiUfIeDvM3O8gCGuVHsqKRo2CRwx3i6Xe8qaUFotFsg1xuVxwOBwkbBQkU4KAYRgUFxejuLhYinREOrTrMcITC4vFgsrKSrhcLgwNDWFwcBA1NTW6dWkHLv39Rf/DyATnldrIcMXJvV27dqG/vx9PPvkknnjiiSW3DQaDeOaZZ9DZ2YmzZ89i/fr1+Pa3vy09r3Sn5Uh4diMMfCdYJqj0UNJGjaIhEZS+iIgRm4sXL+LMmTPo6enBoUOH0NHRgZMnT2JqakpqSrlx40Zcc8012LBhg1Thk5+fT2JHh4iRjlWrVuH06dM4cOAAJicnFbFcUOKYosgSS9o3b96MYDCI1tZWDAwMIBjMzjVTSUsL8fhiZIdhGCn3NBAIgOM4zV5nU0U3ER6Xy4UzZ85I/x8ZGYHL5Yq7/c6dO/Hggw8uuc8jR44AABobGwEAn/zkJ/Gd73xHej5WlZaiMAxCxttQUdAK4DqlR7MiydWSltwfTf4jRmzEpSjqtq09shkBiXRo7+3tRTAYzHnURQnBI4+0mM1mqaR9ZGQkqy7tSi4jyavTKMFZR4Jny5Yt6O3txeDgIFwuF/bu3YsXXnghbJve3l6sWrUKAPDyyy9Lv8fD5XLhxIkTGB8fR1lZGfbv348rr7xSet5isWBmZibzbyYNOMOtqCh8HhBCAKPNP6/4pVwJX8DlWErYiP5oDocD1dXVaQkbreUe6Z1sn/uiQ/vc3BwOHjyItrY2eDweVFRUZP3YSny340Va5D2N4pW0Z+O4uSKW4IrVyHClJDhrc0aMgdFoxO7du3HbbbeB4zjcd999WLt2LR599FFs3rwZO3bswO7du/Haa6/BZDKhqKgIzz//vPT6+vp6zM7OIhAI4MUXX8Srr76K5uZmfO1rX8ONN94Ik8mEuro6/PSnP5Veo7YcHgAAY8LUfDOqit4AZ/yQ0qNJCS1PvqmOPVFhk5M2B4Si5PLct1qtkmXD4OAgBgYGsu7QruSSVjyWKmlP16Vd6QhPMk7tYrGCXtHVVXP79u3Yvn172GOPPfaY9PvTTz8d97VDQ0MxH3/ggQekUvZIsu2llSpnp25AbeXPwBluBTSo1vUseMQ7qkhhEwqFSNgQAHIbARHPVavViiuvvBKBQCDrDu1KRHgSXVqKVdLOcVxaLu1KV0Yl6tS+EqCraRqYzeasJbulQ5DLA882wsC1gjNqL5dHy4JHTqyIjdhx2+l0wuFwoLKyUjXCRg+fOZEckeIjFw7tSi1pJTt+MdFb7tKe7LKf0ktaRDjKX2U1jFojPAAQNN0Ni/+7mhQ8WrPHkAubiYkJhEIhDA0NhVmJVFZWwm63xzU3VBqtXpS1dJ4kSq4jPLGOFenQ3traiurq6ow4tCsV4Un1mJEu7f39/aitrUV1dfWyIkoNEZ5k0Op1IFFI8KSBKnN43kdgmwAYwXJHwRs2KD2cpFBrhGepiI0obPLy8uB0OpesECQyi94v0tlkOSGQDYd2NS9pLYVY0h4IBMJEoNvtjnsjQxEedUGCJw3ULHiAS1EeU/B5+A1PKT2UpGAYRtFu0ZE5NvPz81HCpqKiAg6HI+pCNzw8TH1siLRQY4l4Jh3atRbhiSSZknalk5aTYSVUxpLgSQO1NR6UIwgCOMMHYA58Dwzf937ERxvkKsITK3k4GAzCaDRKOTbl5eXweDwJL0WpNTqVCFodtx7JddJyomTCoV1NZenpkEhJ+0oQEVqCBE8aqDWHh2VZKUkvaNoJU/BnCFgeW/6FKiHTomEpYSNGbMrKypISNvHQquChi7J6yOX5k+qEnI5Du9YjPJHEKmk3m83weDyaivCsBEjwpIFal7Tkk27I+FHYvT9E0HQWAlut8MgSI1XRkKiwqa+vTzn/gCBSJZlJV+mk5URJxaFdfF0uyUXysLykfWpqSnJpdzqdiok8IhwSPGmgVsEjRngAAIwdQdMOmIK/QMDyFWUHliDLCZ5QKBSVY6MWYaPVCA+RfRKd8LQQ4YkkWYf2XJPr5OGioiIUFRVhaGgIo6OjaG9vR319fU46WYtQdCkaEjxpoNYlrchJN2T8JGy+TyFgvh9gihUcWWLIO39GRmwCgUCYsBGTBdUSsVE64TodSKhlF1GULzcJqTFpOZl9yR3a+/r6ACDMoV0JlMqlsVqtqKqqQlVVFYaGhjAwMAC3251QSXu6kOCJhgRPGlgsFlU2HgyL8AAQ2CpwhutgCu5F0Px5BUcWm0hhMzk5iampqbDOwyUlJairq4PJZKJ8kyxAn2nm4DgOXq8X8/PzUgQyEAjAYDCA4zipg7EaqvmyKXILCwuxadMmzM7OSkaljY2NighrpfrhiJElm80mdbIeHh5OqKQ9XVJ5z3q/DpDgSQOtRHgAIGi6B9bFLyFo+izAOBUZV7yIjcFgCBM2oVAIlZWVit4Rpgotaa0ceJ6Hz+eTRM3CwgJ8Ph9YlpXO56KiIrjdbpjNZjAMA5ZlpQ7G8YSPGhoPZpJIh3av14uxsTGUlZXldHlHick8MspiNpvR1NQEj8cTVtKeSnl/IsfWu4BJFhI8aaDWHJ5Yky7PtoBna2EK/g5B82ezevxQKCTd4Xq9XiwsLMDv90cJm9raWmkikDM5OZnV8WUbrQoerY472wiCgMXFxTBh4/V6AQA2mw1OpxN5eXmorKyEzWZbtpFfY2Mj6urqpDv9mpqajFo3JEMuxZXo0P72229jfHwc/f39unZoX+q4kSXthw8fzqhLO5DakpbeBRIJnjQwGo3gOE7pYUQRuaQFAGAYBE33wBx4CkHTpwAm/bsJUdjIIzaisLHb7XA6nSguLg67w00ELUdJtHrB0Oq4M32eBAKBsKWohYUF8DwPq9UKh8MBp9OJ0tJS2O32tJZIxITe2traKOGjtwhPJCzLYu3atVhcXMyZQ7tSgmc5D6+lStrTTfamHJ5oSPCkgVoniXiCgTP8GYCnYQy9jJDpzoT3x3Fc1FKUXNjECt1nY/xaQMtj1yqpnG/i8qpc3EQau7pcLtjt9qwau0YKn7a2NgDIaeK7Utcx0aHd7/dHLfNleqJWavJPxaVdLGnnOA4NDQ0oLi5O6W9EgicaEjw6JK75JmNCyPhJmII/Q8h4B8CE33kkKmxqampgsViydqHUsmjQ8tj1iJhALBc34jktChs19GaSC5+2tjYcOXIEbrc760tdajhXLRaLLjoLGs0AACAASURBVB3agdTyaMSSdrlLeyol7SR4oiHBkwHU1j58qdLooOkvYQr+K4LzL2NifouUj7C4uBiVbJltYbPU+NVwIV5paPkzFwQBPp8vLGIjJhCLy6tKntOJIrZcWLVqFcbHx7Oe46Oma5feHNrF46YqOkSXdp/PF1bS7nK5EtonCZ5oSPCkgVouFJGIgkF+dyv+LC4uYk31tSh0/BTB4AZVTgJaFjxaHbta/vbLIQgC/H6/JGomJycxOTkJg8EAm80m5dlUVFQsm0CsVgRBgMFggMfjgdvtliZ/l8uF2trajAofNQkekWw4tCs1+WeiUiqypP3dd99NSAhS0nI0JHh0QKSwGR8fx9jYGMxms7QUVVBQgOrqalitVrBCGey+u+CpHgVnvE7p4UehVdEgouWxq4lAIBDWTXthYQEcx8FisUjLUWJ1VElJidLDzRhyESJO/tkSPmoUPCJad2gHMiu0xJL2+vp6qXvzUiXtyb5ntZ4HmYQEj4aIF7ERw/aisOE4DoWFhSgvL4+5H4HxIGS4DqbgT0nwZJiVcNHINPGsQkwmkxSxqaqqgsPhiLqjnZubWxFh+3jCx+12p73co/ZzVqsO7dk6rtFojCppz8/PR319fVhJe7JiS6vX3GQgwZMBMn1S8zwf1qk1nrARIzaRx15YWFj25A0Z74bV/0WwXBd4Q0vGxp4JtC54tDj2XEwG8c5reX8mtVmFZJJkvLTibStf7hGrutIRPrk+V9M5XjoO7UouaWXruMuVtFMOTzQkeNLEaDQiFAqldIEWJwD53a080XI5YROPRCZdzrANPOOBKfgT+A1PJT32bKJV0UBcQkwgjmzUxzCMlECcynm9kljuMxFzfNIVPrmOfGTieKk4tGsxaTlR4pW05+fn6/LGIR1I8KSJ2G15qRMrnrARJwB5LkImEi1jNh6M5P1GhJbANxHg+yGwjWkdM5NoWfBoeezJIghCVKM+r9cLnufDEojLy8ths9nobjNBkjl/5MLnzJkzSQsfLQoekWQd2rVSlp4O8pL27u5u+P1+2Gy2nLq0qxldCZ59+/bhoYceAsdxuP/++/HII4+EPf/ss89iz549Ug+O5557Ds3NzZicnMRdd92Fjo4OfO5zn8Pu3bsBXMoPuOGGG6TXj4yM4NOf/jS+//3vS4+ZTCb4/X44nc6cC5t4JDrphowfgTmwG6bg8whYHsvKWFJBy47jehU8wWAwqlGfmEAsLke53W7Y7XZVmGJqnWSvDQaDAfX19VKOT1tbW0KVPFoWPCJqdWgHlDMtFfPeOI7D1NRUwiXtehdFuhE8HMdh165d2L9/P2pqarBlyxbs2LEDzc3N0jb33HMPHnjgAQDASy+9hIcffhj79u2D1WrF448/ju7ubnR3d0vb5+Xl4ciRI9L/N23ahDvvvBPBYBC9vb04fvw4JiYmcO+99+LcuXO47777cPPNN+dM2MQjYcHA2BA03QlT8GcImh6EwFZlf3AJoFfRoHbEVgaRlVGBQABGo1GqjKqoqEBjY2NWOxCvZNI59yOFT3t7O6qqquIKHz0IHjlqcmgHlDXwFC1RGhoawkraM5XsrkV0844PHDiApqYmNDQ0AAB27tyJ3//+92GCJz8/X/p9YWFBOhEdDgeuv/566c4gFqdOncLZs2fx5S9/GQaDAatWrcLatWuRl5eHhx56CDfddJNq1ktZlkUoFEpoW7Hzsin4cwQsX8nyyBJDy4JHK2OPjEaKy1GdnZ3SUtRSBq9qQY2f9Ttjp/GB8rqUX58JUZBqxCfb5EpgqcGhHVAuwhN57Fgl7WVlZairq8u4S7ua0Y3gGR0dhdvtlv5fU1OD9vb2qO327NmDp556CoFAAG+88UbC+9+7dy/uvvtufPe73w27YBw5ckR1VSXJTLoCWw7OcCuMoRcRMP8VwCgbAga0Ixpiobaxi07f8soo0elbXGbNz89HVVUVurq6sHnzZoVHnDxqEmNHL57DrvaX8MzWO7CtrFbp4SwrfJSI8ORSACjp0A4oH+GJPHZkSfuhQ4dQUFAAj8cDq9WqyDhziW4ET6Ls2rULu3btwgsvvIAnnngCzz//fEKv27t3L37+859H3R2ZzWb4/f5sDDVlEkpalhE03Q0jtw+m4C8RNH8+iyNLDLWJBi0gJhDLl6LkTt9y36h4CcRqEg5aRBAEfK/nHQgA/r5zP/7vTXejxGJPaT+Z/lvIhY/YvbiyshJGozHngkcJlHBoB5S1d1jq2PKS9vHxcXR1daGiogKrV6/O8Shzi24Ej8vlwpkzZ6T/j4yMwOVyxd1+586dePDBBxPa99GjRxEKhbBp06ao5ywWC4LBYPIDziLJCgbesA4cuwGm4F4sMLfBbFK2YkvLgicXYxcTiOXLUWJrBHE5yuVyweFwUAJxDvnjhUF0XjwHAJj0e/EPnfux59odYFMQFNkSIWL34pqaGoyMjGBgYAAFBQVwuVw5WepScokHyK1DO6Ds+01EbDEMg/LycpSVlSWcBqFldCN4tmzZgt7eXgwODsLlcmHv3r144YUXwrbp7e3FqlWrAAAvv/yy9Pty/PKXv8Tdd98d8zk1RnhSmXSDprth9T8CS+DvETI8DwOr3LouCZ5LiJ215REbv98vGUw6HA6Ul5fD4/HAZDJl5JhEaoR4Hk/3vBv22Lvjw/j5QCfubbw6qX3l4twXhY8gCJienkZbWxuqqqpQV1eXVeGjFiuLXDi0A8ovaSUjtgwGgyr+NtlEN4LHaDRi9+7duO2228BxHO677z6sXbsWjz76KDZv3owdO3Zg9+7deO2112AymVBUVBS2nFVfX4/Z2VkEAgG8+OKLePXVV6WE51//+td45ZVXYh7XYrGoTvAku6QFAJzhg+CZStiFXgzNfwNl+d/K0uiWR8uCJxV4no9q1BfL6dvtdqs6gXgl89KZHgzMT0U9/oOeVmwqcWFdYUVS+8vV35hlWZSWlqKlpSVsqStbwkctgkckmw7tgPojPHLU9HfJFroRPACwfft2bN++Peyxxx673F/m6aefjvvaoaGhuM8NDAzEfU5sPKgmUhIMjBFB06dgCTyNOvZVDCxcg0rHx7IzwOWGomHBs9TYxQTiyMooAFKjPiXbGRCp4QsF8S+nogskACAk8Pjfh/bhVzfejTxzYlHTXJ774rEil7qyJXzUJnhEsuHQDij7fslaIhpdCR4lUOOSVioRHgAIGf8SJv+/gmW8qOCfgi90DWzG6iyMcGn0IHjkHYjFH47jYLVapeWokpIS2O12uihpnH8fPILxxYW4z494Z/HNrj/g21fflvA+czVJRk7IcodyufCpra3NyLKpEgIg2c7Vcof2jo6OJR3J1YxaxaWSkOBJE91EeACAycOU6YuwBr4LJ7uAsbkvwlb4GyDHXxqWZTUjeESnb1HczM7Owuv1IhAISJVR1dXVsNvtK7LRl96Z8vvwk77Dy273yugpbCurxV2NG3IwqsSJNymyLIva2lop4nPgwAFUVFSgrq4uLeGj1CSc7DEz4dCuNBThiYauwGmiRsGTaoQHAKyWT2I00IaC0NtoMA+hZ+47qM3/uwyPcGnUGOHheT6qA7Hf75ecvp1OJ8rKylBRUYEzZ85g3bp1Sg95xaDkXey/9nZgPpTY9/+bXW9iU7kbnrziLI8qOZb6/DItfJQQPOkkDkc6tB85cgR5eXloaGiA3Z58y4FcQoInGhI8aWI2m1UneNIVDNWO/4MT03dhhj+HRtPvcGHxZhRbt2VwhEujpJeW6PQd2aiPYRhpKaqoqAg1NTWwWCxRF1Kv16s6sUZkh5GFGfxqqCvh7Re5EP6m/T+x94P3wGxQx6U3UQGSKeGjRNVSNhzau7q6lnVoVxpKWo5GHd86DWOxWKTEU7WQruBhWAOuKPwxDl68CzO8D67Q3yFo+g+YDLn5YuciwiMIAvx+f1jERu70LfeNSiaBeCVcNNSEkuJy93ttCAnJCfOemXH8U9db+OrGW7I0quRI9vPLhPBRQvBkKtKRrEO7klCEJxoSPGlitVoxPT2t9DDCSGdJS8TAFmFjwffRPr0Lc0IQpTNfQGNxYl2p0yXTgicYDIa5fIsJxKLTt9PpRHFxcUacvtW4HJcoWh23EpyYHsN/jZ5K6bU/7+/EtvI6fLBa2QafQOrRD7nwGR0dxYEDB1BeXo76+volhY8SEZ5sHFPNDu0iJHiiIcGTJnpc0hKxmFqwIe/LODj7Pfj4AYRmvo01BdnP50l1/KFQKKxR3/z8PILBIEwmk7QcVVVVBYfDkbUEYorw6B9BEPD9nnfS2sdXD+3Di0WfRYVN2eWQdJd7WJaF2+2Gy+VKSPgoVaWlhEN7cbGyuVpUpRUNCZ40UaPgyUSER6TAeifWBI+hx/cGzvhfAebysCbvCxnZdzyWEzyi07c8z2ZxcREsy4Y5fStl6qrVSAldHBPj3fFhtE+MpLWP6cAi/teBV/CTGz8BA6PcXXimJsVEhY9SgicXkY5Ih/a+vj4Eg0FFhUcyx10J338SPGlitVpV14cn08sqNXlfx3igCxPcOIZ9v4afn8Ta/EdgYLLTl0Icv5hAHNmBGLjs9F1QUIDq6mpYrVZVfGG1vKRFLA8vCFEWEqnSMTGCH55sx+evzF1BQLZZTvhorUorFUSH9oWFBbS3t6OtrS2nDu2psFKuWSR40kSNEZ5sTLprC57HwamPYkHgccH/OrxTw9hQ8E3YDJVp7zuyUd/s7Cymp6fR0dEhdSB2Op0oLy+P6/StJlbKxWMl8vLIe3hvdiJj+9tzohXXlrmxqbQmY/tMhmwJkHjCx263625JKx5WqxV2ux0bN27MqUN7qqhVjGUSEjxposY+PNkQPGajExbTV8AFvo1FMJgL9aL94l9hfcE3UGxO3BxRdPqWL0eJTt/yRn3BYBBXXXVVRt9DLlgJF42VhhhpvDg7g6ePv53RffMQ8HDrf+DFD92LIkvu+7pkWwzIhc/Zs2fR29sLu92OQCCQs+VmpQSPuJSWa4d2Ij4keNJEjeah2fpyX1XwYfz76H7UmA8iBAZBYQaHp/8Gq5wPos7+ybBtOY6LatQXCAQkp2+n04mKigo4HI6o5MZgMJiV8ecCWtLSNvJIo/ivIAiw2WzY7zuP8aAv48ccC3jxhdd+hac2b0d5eXnG978UuRIDYudilmVx4cIFdHR0oKysDPX19VkXPkpVK0UupeXKoZ2IDwmeNDGbzZqeoJOBZVhsLrwfhy8Oo8I6BgEMBHA4Nb8bY3PHULTwaXjnA2EJxKJnVG1tbcJO3yQaiGwjds6Wi5tAIACTySRFGl0uFxwOBwwGA2YDi/jdG5nJ3YnF4cWL+MWpQ/jAYDFCoVDOhIgS0Y/CwkJs2LBB8qrKtvBRKsITT2hl26GdiA99ummixghPNhCdvssWKzC2eC1KTH+AwXDZMHGaeQt+5yjWVXwdBfaWtC4wWhY8Wh67HhEbTM7Pz0s/Pp8PDMPAbrdLPZhEQR6PH/Udwmwwu9/zn02cwp9f93EsdJ9CR0cHmpqaFC9tzjSi+JB7VWVb+Ci9pBWPbDm0E/EhwZMmVqtVdTk86SAIQsxGfTzPw2q1wul04hbHh/Dz2WHcUXQci+Ck1/rQj6PeL2G96RsoMm9MeQxaFg1aHrtWESczuZGr+CN3qBf9zpJNfD/vm8MLg0ezNXyJAM/h74++hr9xNKG5uRl9fX3o7+/HqlWrUFhYmJVj5loMRIqAXAgftSxpxSMbDu10DYoNCZ40UWOVVqJEOn3Pz89LCcTicpQ8rC/n2NlNeHkqhNuLTkBurBEQpnBo+n9itfMLqLV/PKVxkWggloLnecnvbHZ2Vkp8NxgMcDqdcDqdqKyszFiDyT3vtSPAc8tvmAH65y5ib2gIH3TegI0bN2Jubg69vb0QBAGrVq1Cfn5+Ro+nhOCJhVz4nDt3LqPCR60Rnkgy6dCeishbCQUXJHjSRI1VWpFwHBfVqE90+hbzFcrKyuDxeBL2w9leeju65o/izdkm3JDfCz8uf1kEcHhv/mnMhXpxZd7DYJnkLlha/uKRWMssYhKx+COas4p+ZyaTCS6XC6WlpVk5b3pnJ/AfZ3oyvt+leMs3hn0j7+HDNWuQl5eHq6++GjMzMzh16hQMBgOampoyZlipdIQnEpZl4XK5UFVVhXPnzuHgwYMoKSmBx+NJWfgomcOTqm1Hug7tJHhiQ4InTdTUeFC88xUFTVdXF3w+H1iWlfIVioqK4Ha7E04gjkeJuRTXFV6PP029iRNeN1bbz4BD+P7OLr6C+dAgNhQ8AauhLN23pwlWwkUjG0RW9Ym2IGK00el0wu12R/mdnTx5MqZrfaZ4uuddKCFfHz28Hy1FlXA5LhlSFhQUYPPmzZJhpdlsRlNTExwOR1rHUULwJOrOLhc+4jJPKsInV52WM33ceA7tjY2NcDqdS76WfLRiQ4InTZRY0pInYooThOjYLjbqY1kWjY2NSTl9J8ufF38YHTPt6PUXo9ToRYF5EogQPbOhHrRP/RU25D+OQnNLVsZBaAcx+V0UNaJLvVpsQeR0TIzgT2OnFTn2XNCPvznwMn5+06dgYi8LvKKiImzZsgWTk5Po7u6G3W5HY2Njwnf+sVCj4BHJRMRHCcPSTB5X7tB+8eJF9PT0LOvQToInNiR40iTbZemBQCCqUZ88EVMs+7bb7WEn+MTERFbFDgA4jU7cUvzneGXiP9C6UIPtRi94djH6PfAXcXD6Iaxxfglu+8eyNh4iPTK9FCdvMil3qReT38VeTNk+T1NBEAR8L02D0HQ5evEc9pxoxZfXXR/1XElJCYqLizExMYFjx44hPz8fDQ0NsFqtSR0j18uv6bizi8Ln/PnzSQkftZWlpwrDMCgpKUFJScmyDu1KRbXUDgmeNGFZNiMXDTGBWC5uRKdvMc8mGadvMZck21/0G4s+iHem/4SZ0DT+a6YJHys6AR+ijUsFhHBy/inMhU7hirz/CZZJLFeIUD+RZq7z8/Pw+/0wGo0pnbtq4NVzfTg+Pab0MPDce+3YWl6LreW1Uc/J7/zHxsbQ2dmJwsJCNDQ0JFzdo9YlrXiI+S2VlZVhwmepiiYll7Sy9dlGOrT39fWhoaEBxcXFYBhGsaiW2tHG1UdHxJscDAZDWOlsutUJuUqeNbFm3FayHb++8AIEsPiv6TX4cGEPouM8lxhd/E/Mc0O4quApmNjk7kaJ7LLcBVLueSaPOAKQcsQKCwvhcrmymleTbYI8h3/uaVV6GAAAAcBXOl7Bi3/+WRTHsZ6Q53qI1T1qjX5k0p1dTOw9d+4cDh06hOLiYng8nijho+SSVraFViyHdvEzoAhPNCR4MkCsL1Msp2+v1wuGYaSlqIKCgqxNDizLguf5nLQs31KwFW9N/QHnA+ewKJjQNteAzXkDCMXZfibYjZ75f8f6/P+R9bERqSEmEcsrpEKhECwWS1iuTeRSqh74zelunPHOKD0MifHFBfzdwX145rq/BLvEdYJhGFRVVaGiokJK9K2oqEBdXd2S1Ze5FAOZFh8MwywrfLRSlp4OokO71+vF4OAgpqamYLFYknrvWr1BSQZ9XakA7Nu3D2vWrEFTUxO+853vRD3/7LPPoqWlBRs3bsT111+PEydOAAAmJyfxwQ9+EE6nE1/4whfCXhMIBPDXf/3XWL16Na644gr89re/lZ4TT+rf/va3+MY3voHW1lYcPHgQHR0d6O/vx/z8PBwOBzweDzZv3owtW7agubkZdXV1KC0thdVqzcqJlsvyaJZh8RdlO6T/nw85MeirBIP472t8cR96vdlr1U8khiAI8Hq9GB8flyr7Dhw4gM7OToyOjoLjOJSVlaGlpQXXXHMNNmzYgKamJlRWVsLpdOpO7MwHA/jhqQ6lhxHFW+cH8bO+wwltK+a7bNu2DWazGQcOHMDAwABCoehbEK1GeCIRhc+2bduQn5+PQ4cO4eTJk/D7/YotaSkRWbLb7Vi7di2ampoQCATQ2tqK0dFR8Hx0moGclSB2AJ1FeDiOw65du7B//37U1NRgy5Yt2LFjB5qbm6Vt7rnnHjzwwAMAgJdeegkPP/ww9u3bB6vViscffxzd3d3o7u4O2+83v/lNlJeX49SpUxgaGsKvfvUrPPjgg+ju7sbc3BzGxsbw5ptvYt26dairq0NFRYXiZnBihCdXNDtb0GhrQr/vUiLd8cVylBq9sJlmY27P4wL6vftRYW5CvjG3hokrFbGDtnxJiud5qadNLir71M7z/YcxFci8QWgmeKrrLWwprcHaooqEtmdZFrW1tXC5XDhz5gza29ujzCq1krScKLEiPgaDARUViX1mmUTJxGGj0Sh1axaNSsVzIdaYVkrvMF0JngMHDqCpqQkNDQ0AgJ07d+L3v/99mOCRdypdWFiQvnwOhwPXX3+9lPku58c//jFOnjwJAPB6vSguLsaNN96I5uZmFBQU4KqrrsIPfvADVU0SSjTA+2jZx/D08D9J///jfD22F/aBZ7wxtzfxXXhtcg8+Vv4oWIbcgjOFPE9M/BGd6sXqqFgdtCcmJtIqb1aCTE6g44sL+NlAZ0b2lQ2CAo+H2/8Tv/uzz8BhSjy/z2AwoL6+HjU1NRgeHkZbWxvcbjdqamp0E+GJRC58jh07htOnT2NxcTFmjk+2UDJxWMwfIof2cHQleEZHR+F2u6X/19TUoL29PWq7PXv24KmnnkIgEMAbb7yx5D6np6cBAP/4j/+IN998E42Njdi9e3fYHYMa1XGuIzwAUGurx4a8q3B07vKksW+6AduLBhBCtOjhMQYzswZtM7/CdYX35HKouiDSGFPMEwMg5dlkqtHkSuDZUwewyMXLPFMHwwvTeOzI63hyy0eSfq3Yu6W2thZDQ0NobW0Fz/M5vX7lWmAxDIO8vDyUl1+KIh86dAhFRUVJVbKlipK9cCKjS+TQfgl9LcAnyK5du9Df348nn3wSTzzxxJLbhkIhjIyM4LrrrsPhw4exbds2/O3f/m3YNizLguNy47WTKEpZHGwv3QEDLt858GDxxxk3jIjdEdbKnMTRuVdwZrErV0PUJKFQCDMzMxgdHcWpU6dw+PBhdHR04L333sPMzAysVivq6uqwadMmKU+strYWJSUlmq6YyhVD81P4f8PHlR5GQrw0fAK/P30i5dcbjUY0NTXhmmuuAcdxOHToEM6ePZuT64USCcSi8KiqqsK2bdtQWFiIQ4cOoaenB4uL8epJ00epZGkgfnRJdGjftm0bDAYD2tvb0dvbm9VecmpCV9JOXKsWGRkZgcvlirv9zp078eCDDy65T7ES5c477wQAfOITn8CPfvSjsG1EPy01KWWlBE+puQxbCz+Ad6bfkh6b4y04Ol+OFuc58BEF64IwDrflBuyf3IO7K78LmyFf9pxyFwylEJOI5RVSi4uLYW0LysvLk/I9I5bnBz2t4FQYqY3HY52vYUNxFerzipbfOA4mkwk2mw0tLS0YHh7G0NAQGhoaUFFRkbXvnRLfafkxxUo2sY/P4cOHUVRUBI/Hk3TTxuXgeV6x7+hy0aVIh/bh4WGsXbs2hyNUBvXM0Blgy5Yt6O3txeDgIFwuF/bu3YsXXnghbJve3l6sWrUKAPDyyy9Lv8eDYRjcfvvtePPNN3HLLbfg9ddfD8sJAi51W/b7/arKf1BiSUvkQyUfwcHZA/Dzl8XNmWAeivwmVFv6gIjGhDbmJPy8Da9dfAa3l/3vHI9WOcSeNvLWBaIxpsPhQF5eHqqqqrJWyUdc4ujFc3j9fL/Sw0gKLxfE3xz4T/zy5rthNqR3GTebzbjiiiuwuLiIgYEBDA4OorGxEWVlZRk/75QWPCJy4XPhwoWsCB8lk5YTXU4THdqVmityja4Ej9FoxO7du3HbbbeB4zjcd999WLt2LR599FFs3rwZO3bswO7du/Haa6/BZDKhqKgIzz//vPT6+vp6zM7OIhAI4MUXX8Srr76K5uZmPPnkk/jMZz6DL3/5yygrK8NPfvKTsOMq4ae1HEq6djuNefhg0Z9h3+TLYY8f81pRbGqBlT0a9jj/fpRnaPEwjs69gg1523PWKToXyN3q5caYYhdtp9OJmpqaKGNMIvuowUIiVU5MjeGn3Ufw1xs2p7wP+XfMarWiubkZPp8P/f39kvApKSnJ2PdQKcETb/JnGAaVlZWoqKjAhQsXpG7VmRA+Sict07UkGl0JHgDYvn07tm/fHvbYY489Jv3+9NNPx33t0NBQzMfr6urw1ltvxXwOuLSkpRbHdBElIzwAcFPxn+Hd6T9hlgsvS39zhsftJRsR5I+EPW5jTsIAG96dfgHVlmZFBVuqiMaYoVAIQ0NDmJ+fh8/nA8MwUiditRhjEpf444VBdF48p/QwUmKdowr/fOAAbqqpx5qS0pT2EUuA2Gw2rFu3DgsLC+jv78fAwACamppQXFyc9piVyuFZ7pixhE9BQUFK/mQiSkd4aMk7Gt0JHiUQc3jUhNKCwcya8aHS7fjNhb0RzzB4c9qImwoKEBIud7PlhXHUWm/A4GIf/nvyaaxhPqVqwRMKhcKqo+bn5yVjTI7jYLPZUFZWBpvNppnmfGr+vLNBiOfxdI92m18GvQxCPI9H3/oDXrjj4zCkcJ4tJUAcDgfWr1+P+fl59PX1ScKnsLAw5TGrZUkrHpkUPkpHeLRy3cklJHgyAC1pxebaguvw1tQfMBa4EPb4HBfAeHA9iox/Cnvcih4YYMd06CwG817HRmFTLocbE57n4fP5wsSNmEQsLkdVVlaGGWOKLf0JdfPSmR4MzE8pPYyU8FiL8d7IJADg2NgFvHC8C59p2ZDSvpablJ1OJzZu3IjZ2Vn09fVBEASsWrUqrKdZoigR9UjlmJkQPmoqS19u25WC5gTPxMQE/H6/9LO4uIjFxUX4/X74fD585CPJ96dIF1rSijMGhsVflN6Bn5x9Luq5ttkZ3FF6JQJ8j/QYujhUfwAAIABJREFUL0xIUZ4x6zH0+dpwpemGnIxVNMaUV0eJxphiJ+Jsep8RucUXCuJfTkX36NIKBZwTwIL0/+8faMOf1TegOi8va8fMz8/H1VdfjenpaZw6dQoGgwGrVq2C0+lMeB9KTK7pRFrSET5qLEtfipVwTdOc4Fm/fj0CgQDMZjOMRiOMRqP0OwDceuutOS8PpwhPfNblrUe9rQFDvoGwxxkGeHcmD9fkWcNK1S04AQMc4BDEn2Z/DJd9TcatJ+TGmOK/wWAQZrMZTqcTDocDbrcbDodjRYWF9ZQovhxvHhuAjdFmjkOZyYGu82Nhj3lDQTz+9h/xLx/+i6z//QoLC7F582ZMTU3hxIkTsFqtaGxshMMRu9eWHKUiPOl+JnLhMzY2lpDwUTLCk+yxV8J3HtCg4Dl79qzSQ4hCLEtXE2qI8IjcXvYx/PPwU1GPT4QWMcddBYehVXpMECZRZ23GwGIfAoIXr07+M+4s/3pK1hNiErG8Osrn84FlWamnTWlpKSURa5hURf3+zgEUGWwYLpnO8IiyT52xFOPC+ajH3xwewn8P9OPDjU05GUdRURG2bNmCixcvoru7Gw6HQ/Jii4fac3iWg2EYVFRUoLy8PEz4eDyeqPetdNLySrpZSxTNCZ5XXnlFShANBoOYmZmB0WjEvffeC6vVqsgfmZKWl6be1oAW5wZ0zR+Neu5PMwu4vbQOQf609JgZx8EKDvBMCOcDp3Bg9v9ia8HOJY8hGmPKIzdi8rAobioqKnJmjLlSIiVqINnPeXLOi8N9Z8ELAtbfWolj3mjxoFbsrBk95yfjPv/Nd97CtpoaFFgy20QvHgzDoKSkBMXFxZiYmMCRI0eWjHwo2Wk5k0QKnyNHjiA/Px8NDQ2S8KGkZfWhGcEjflF+97vfYWpqSvoydXZ2wmAw4O6771as8R/l8CzPX5TtwPH5LvARTQfBAIfnKrHeMQIBl+w5BOEiyvh6XDBcKhc+NPsiaiwtqLGuDTPGFMWN3++XjDEdDgeqqqrCkoiJxFkJQu31owPg378ZmDsagGk1i6Cgnu/KUjRbq3AwGF+gTfi8+Ke2d/H4TbfkcFSXBEBZWRlKS0sxNjaGw4cPo7i4OMqsU+sRnkiWEj5aWtJaKWhmRhBP2H/7t3+Leu7OO++E1+tNq1wyHSwWi+q8SNQU4QGAMnMFri24Dq0zb0c9NxrwYY39KpiZg9Jj+eZ+THAF4BCEAAH/deF7WDv+WZgEm9TTprCwEDU1NaozxtRqLozWxpsq+zv7pN8vjvlwVXM1DgRHFBxRYrBgcHpidtntfnPyBG5ftQbXVMe31ckWcgFw7tw5HDp0CKWlpaivr4fZbNad4BGRv+/x8XEcOXIEgUBAsQ78SkaX1IzmJODc3BzGx8dx9uxZ9Pf3Y3h4GIODg4pGWNSYw6M2wQMAt5Vuh4WJ7VD8x6kAWP5y8zTWOIPi4OVGZ352Dhfr2rBlyxasXbsWdXV1qjXGVONnT1xieHwa741OhD021DaNYnP8vBO10OKoxrjXm9C2X3vrD/CHlHN+ZxgG1dXV2Lp1K+x2Ozo6OtDX16fIRJzLaAfDMCgvL8fWrVthNBrR09OD48ePw+fz5eT4IsnmD6ntGpotNCN4xOWZL37xi9ixYwc+85nP4POf/zyuuuoq3HTTTSgvz2wlTzKoMYdHbUtaAOA05OG6vNhl5hwDHF+oDXusxDEEI3M5mXho8RCOze3L6hgJfbO/M9ozy7/IoXG+RIHRJIEAzM4mLmCGZqbxw85DWRxQYoheTdu2bYPZbMbs7CyGhoYQyqEYUyKqxDAMzGYzNm/ejLKyMhw5cgTd3d05Ez60pBUbzSxpiX+8xx57DIFAACaTCQaDAUeOHMGpU6fg8/kSKovMBhThiUZMIpb3tOF5HpXWWpjYUgRNE1GvOR0KYTVaYEAXAEBgplFracHA4uUliHdmfoFqy5UoNdfl7L0ki9KffTpoddyJIAgC9h/pi/ncex0TqL7BjrN8YhGUXLPaXo5TZ5KrKPu3I4fwkcYmrCpWXsyxLIva2lqcOXMGBoMBbW1tqKmpgdvtzrrnk1LLy6LoKC8vR1lZGcbHx3H06FE4nc5lq9kydWwiHM0IHpHa2stRgFAohI9+9KO4/vrrsXXrVpSWlipyclutVqlJnVrIVYSH5/monjaiIBWro1wuFxwOh3Rh8/ib8PXeb8Fsmo/a31vTLP68qADc+7YTFqEbRiYfIeFSBI0TgvjvyafxyYpvw8TGXh5TA1oUDnoMawcCAczNzWF+fh7HBs/h7MW5mNsxYFByPg9ny9UpeMyB5NsmBN+3nfj3Oz4OViV/W4ZhUF9fj5qaGgwPD6OtrQ1utxs1NTVZm6CVmvzlxxWXuiKFT0NDQ1ZyfKgPT2w0J3jeffddnDlzBgzDgOM4jI6Owmq1oqCgAIAyfzg1LmllOsogCAL8fn9YxMb7fj6BKGyKiorgdruXTSIuseTjf3kexv8ZfBJmU3hkzC8IGPStRq21AwDAYwq1lnVhUZ6p0Cj+NP08bin+64y9v0yyUi4eakJevSf+iMI7Ly8PTqcTPRNLR2GHe2exsb4KR7zqMhOtMuWh+9x4Sq89cuE89p7oxj1rWzI8qvQwGo1oaGhAbW0thoaG0Nrairq6OlRXV2dcnCgV4Yl13Ejhc+zYsawIH4rwxEYzgkc8ef74xz+ivb0dxcXFMBgMKCsrw49+9CPU1Sm3xKG3TsuhUCjMYkHse2SxWCT/qHSNMWsd5XjA/QX8cORpmIzh6/nHvX44BReKbaMAALPQBSNTIEV5AODEwuuota5Hk31rSsfPJlpe0tIC8uXSmZkZHD9+HCzLStV7ovCWl0OHOB5/6nlt2X1PdS7CcqURfl65hN9I8rxmnEPq15en2t/FLXUeVCZhAZErjEYjmpqaJOHT1taG+vp6VFVVZVSkKHUTEu+4cuEzMTGRceGTjMhbSdcqTQkev9+Pr3zlKzAYDOA4Tkp8m5iYgNfrVbQPj9oETyJLWoIgwOv1hokbuTGmw+FARUUFGhoaYDJlvg3/2gIPPhX8H/j12L/CaLg8VoYBOryFuMl8AVZDCAKmUWtuwYA/PP/iDxefQ4W5CXnG0shdK4qWIzxquviJ56dceMt7LjmdTtjtdng8nmWNLDt6RzGzsLjkNgAwPbGIDUwlDkAdZer5BgtOT6WX6LoQDOKJd97C7tu2Z2hUmcdsNmP16tWoq6vD4OAghoaG0NDQgIqKCk1/n5ZD3r8o08Inmc9Nz5+xHM0InmeeeQavv/46nE4n5ubmwLIsrFYrZmdnMTU1he9973vYsmWLIuFLNTYejIwyBAKBsInD6/WC53nprjgvLw9VVVWwWq05/fw+UNqCqeCn8Pr0XrDs5fEKrAFHZtzYWjwIAPAFDkIQSsEwl++8/cICXp38Af6y/OtgGXWFb9UkHBJFyYteKBQKOz/n5+fDzs94xq3T09MJRRnlvXeWY7B9CqXX2TERUD6fZ7WlEge59DtBvz40gFcH+vGhhsYMjCp7WCwWXHHFFVhcXMTAwAAGBwfR2NiIsrIyXU/KsYSPaNWh1I28HtGM4Nm6dSsaGhrQ1dWF1tZW3HzzzdiwYQPefvttdHR0KFqCbTabVdN4kOM4eL1eTE5OYnp6Gp2dnQgGgzCZTNJdsdvtht1uz3p1RKJ8tOoDuBiYxRHfy5Bf06bgxKi3AC77DPLNPhyfNKHYGb7UcC7wHjpmf4NrCz6Z41HHh5a04iP3NxOTieVRRafTiaqqKjidzoTOz0RucLz+IN4+cXrJbeQE/DxWz5Zgwqqs4DExBvSNZc7r64l3/oitrhrkW9Sb7C9itVrR3NwMn8+H/v5+SfiUlJSsKOHT1dUFu91OwidDaEbwXHXVVWBZFi+//DI+/vGP49Of/jQA4MYbb8SXvvQldHZ24tprr10xER75xCEuSXm9XimXwWw2w2w2Y+3atZowxvxs3Ucw0z+HodBb0mMMw6DHV40K6yyMrIDmvLM4EyqByRAubjtmf4dKczPqbOtyPey4aFXwZHLcHMdFRW04joPVas1pVPHtE0NYDCaXk/PeoQmsurUEvd74vlXZpsVejcPjY8tvmCDjXi+eOtCKr99wc8b2mW1sNhvWrVuHhYUF9Pf3Y2BgAE1NTSguLl7+xRqGhE920IzgES/ETqcTBw8exM033wwAmJycxMjICK677jrFxpbtHB55uF8UN/KJI1YSsc/ng8/n04TYEfli4yfxxMlpTDHHpMdCrAnHZmtwdeEZFJgX0TNvQZEjMqdBwK/OfRfwfQZ/6b4aq/KV7Tui1TvQVMcdWcEnF9/i+VlRUYHGxkZF/M1ejdFscDkYMDD1GYDqLAwoEQRgbGr5nKNk+dWJbtzetBqbqpR6Y6nhcDiwfv16zM/Po6+vTxI+StkJ5Qq58JmcnJSET0NDg2J957SMZgSPGN5+5JFH8JWvfAW33norVq9ejf7+ftx99934xCc+AQCKlOJlqkqL53n4fL6oJE15uL+ysjIhY0ytLqv8/Zq/wldPfA8B44D02BhfgMnFiyixLqDZOYrhUGlUlMdu8mNw4f/hs28P4jc37oTLuXQSazbR6mefCOKSqbgcNT8/j1AoFFXBZ7fbVSH8Ls55cbB3NKXXjg7O4erGahz2nc3wqJZnraMKx4cvZmXfX3vrD/jdXTth0mDZstPpxMaNGzE7O4u+vkt5WU1NTcsmrWsdhmFQWlqKkpISTE5Ooru7GzabDY2NjSR8kkAzgkeksLAQzz33HLxeL0ZHR1FfXw+TyYRQKIRAIACj0Zhz0ZNshEcQBCmJWN60DwDsdjscDkfcJM1EYVlWk5MuwzB47Mov4W+7vgmj9f3eIwyLo/M1uNnyHvLNfszHjPIAnsJJjM4N4QfHW/HktbfleOT6I1bUhmEYqe9SWVkZPB5PVir4MsUbxy47o6fC+CEvbC1G+Ljclqlz3uyJxf7pKfxr5yE8ePVmVYjSVMjPz8fVV1+N6elpnDp1Sipvd6qw9D6TZEv4aPU8SBbNCZ7e3l50dHTAbDaD4zi8++678Pv9YBgGU1NTuOGGG7Bt27acjslqtcbN4eE4Lqzse2FhAcFgEGazWSr9drvdcDgcGRVqDMOozksrUUysEV8o/DR+MPNjmC2XOi4HDBb0zFZhbcE5NDtHcTpUBrOBi3rtVtcgXjpViMPnW3B1pTJhe61FeMSmfX6/HwMDA/B6vWHnqNPpRElJCex2u+aame0/kvxylpzZKT/WC1Vox5kMjWh5qhkHTo5nN3foh50H8SFPg+YnusLCQmzevBkXL17EiRMnYLVaVRH1yPb3fynhk0p0VevnQaJoRvCInSPfe+89PP/886iqqgIAmEwm6Wd+fh4tLS3Yt28fHnroIXAch/vvvx+PPPJI2L6effZZ7NmzR1oqeu6559Dc3IzJyUncdddd6OjowOc+97n/z96Zh8lVlmn/d2qvrqpeqnrv6r07W2chSydBAy5Bg4hBZTFm5kMYZQTxAwaXkREdBxg/dUYEDOLgzAgqiqAQkJCQAAGJZOks3Umnk973fa99P98fTVW601tVd62Q+7qUTp1z3vepU+ec9z7Pcj/s2rUrcMxHP/pRent7A/1P9u/fH2hY6u+lVVdXR11dHevXr8disWC325FIJFPeiIuKiqKSV5Noi+7FSJKp+GrKzfyX+X9RyCe8X11eAwXuEZIVTqxmBQrtdC+PTOLjo4Xn+PGZg/wx+++ibTYQ3+fe7XZPCUdN9iyKokhqairFxcUJlfs1GzqHxjnXuTCF4sloOjxM1pVa+p3TW6FEAjq3Gpi5BUa44Pb5+ME7b/H1bGNE54kW9Ho9lZWVgcXfX9IdyX5VcyFaxTMzER+VSpWwL7uRRsIQHv+b5bXXXsu11147635er5clS5Zw4MABjEYjlZWVbN++nRUrVgT22blzJ7fffjsAL7/8Mvfeey/79u1DpVLx4IMPUltbS21t7bSxn3nmGTZs2MDY2Bg1NTU899xznD59murqajo6Ovjnf/5nVqxYwRVXXEFWVhZqtTpmzDkeu6WHAkEQSJVq+GbxP/Gfbf+BQuZEFCScMhdwpb6RCl03bbN4efRqG4b0Y+xr3cjVxeUxsD72mE2072J5gsmexerqatLS0t4XZAdC096ZCx6PSNFICv2ayBMevVRN41B0iNXJ/j7eliuJrj88cpi8+A8ODlJdXU1ycnJMnoOiKEbVGzr5uw8MDHDmzBlOnz4dF96ueELCEB4/zp07x4svvkhSUhIulwtRFBkdHeWTn/wkRqOR3bt3U1ZWRklJCQA7duzgpZdemkJ4Jie4Wa3WACnRaDRs2bIlkAw3G/bs2cPRo0dZvXo1X/nKVygqKmL79u385S9/icA3Xhji2csQDPz2F2qyuS3va/x398+RyzxYUdNsSadUOzSrlwdgRXofv235E1cX3xdly6N/7idX8ZnNZqxWK6IoolarA6J9RqNx3h5n7ydMdEZfXDhrMhqrh1n6iXTqbUNhG3MmFAh6RojsHJPxXE8X/2C1kKmJfO5LtO6JyW0b+vr66Onp4fz58xQXF09pNxJJ+Hy+mNxrgiCQmpoayAE9e/YsKpWKkpKS931+UzBIGMLjdxHa7Xa6u7sDTeYUCgVKpRJRFElKSsJsNpOfnx84zmg0cvTo0WnjPf744zz88MO4XC7efPPNoGy49dZbkUqlXH/99Tz66KOBC9rtdseN8KAfib6wTSYNq1NLucF9K38e+h9kEh9NzixyVWNzenkA1hbU8j91b/PlFR+JpukRIzyiKE6r4nM4HFNaLVzcmT4e7I4FznUN0j1sCtt4AgJCvYCQD5E6Q2qJnIah8AkNBgO7z8t9+/fyX5/5XFQkA6L5XPITH61WS3JyMidOnCA9PT0qaQXR9vBMhs/nQyqVYjAY0Ov1gfwmpVJJaWnpB5r4JAzh8d8o69atY926dZhMpoDWh16vD9ysa9asobd3/m7Hd955J3feeSe///3veeihh3j66afn3P+ZZ54hLy8Ps9nM9ddfz29/+1tuvvlmYKJkPpHDR/GIixffKzPWMOa+iYPjf0QikVJjzmdzWitWiwLFDBVbACqZh3bhd1jca9DKE0uvYzbRPr/XJlatQBIF4QpnTUZvh4V15XmccCyszH0+rFDncMLTH5Gx58K7A/388sBrfGbFSvLz8yO2UMdCFNZPPHJzc8nOzqanp4eqqiqysrIoKiqKGMmLlYfHP7f/NxQE4RLxmYTEKrl4D6+88gpbt25lxYoVVFRU8KMf/Yju7omHUE5ODp2dFyoqurq6yMvLm3WsHTt2sHv37nnn9I+h0+nYuXMnx44dC2y7tOCEHzN5G7bnbuGypE8hijAqaum2pbJS143LO7s3I1Nr4r877mPUHb2FJBRPiV8xe2hoiNbWVs6cOcOxY8eorq6mt7cXURTJzs5mzZo1bNy4kVWrVlFcXBwQmvygX3sznWeP18ebp1tm2Hvx6DtuQSMNfxm+IAp0RSl3ZyY8PzTAuN3O4cOH6enpiZiHMtrX6+TFXyKRYDQaufzyy1EoFBw9epSWlpZAE+pIzRttzORd8hOfjRs3YjQaqauro6amBovFEtj+QUDCEB7/DXjixAkee+wxfve733Hw4EG2bt3K5s2b+f73vw9AZWUljY2NtLa24nK5ePbZZ9m+ffuUsRobGwN/79mzh/LyuRNbPR4PQ0MTcXW3280rr7zCypXx08bg/YjZSMOXCq+hSPbhibYTjhyUUjdWx9wLkEwxwgsDD9LnbI2UuVMw28PD6/ViMpno6emhoaGBkydPUlVVRUNDAyaTKVBZUllZyfr161m2bBlGo5GUlJSoKRQnYkjr4vN9oqmbUUv4VYoBLOMuVnqzwz7uak0u/e9VzMUCAzYrLw31U1lZiclk4siRIwwMDIT1eoiVh+fiOSUSCQUFBWzevBmJRMKRI0doa2vD6505NB6ueaOF+cjWxcTn/PnzUbQutkiYkJYfZrMZpVLJ0qVL2bt3L1qtlvLy8kAYSyqVsmvXLrZt24bX6+Uf/uEfqKio4Pvf/z4bNmxg+/bt7Nq1i9dffx25XE5aWtqUcFZRUREmkwmXy8Xu3bvZv38/hYWFbNu2Dbfbjdfr5aqrruK2226bZlssL/L3G+byktxV9kUePG9iTDjDGVMeK3XdtLozZ83lAbD7Bnlp4Ed80nAnxUmrI2U2cEFYcnh4OKFE+94v1244k5VnQsORIXI/qqPHEabScREs5vAttgvFH86e4dqyJaxdtmxK087y8vKw9K6KRZhnrmeyVCqlqKgIo9FIR0cHR44cIT8/H6PRuGjvTCw9PMGeZ4PBgMFgwGaLbZPcaCLhCI9KpcJun8jZ0Ol01NbWcujQIUpLS4GJh/Y111zDNddcM+W4Bx54IPD3o48+Ouv4bW1tM35+4sSJWY95vywU8YT5wkL3L/1H7qv7Kf1SHwW+EWwOOQrN3IuGmzH2Dj/Clb4vs1L74bDY6Rftm6xtY7FYMJlMpKWlodVqSU9Pn9Ln7BIiB7vLzTtn2yI6h88L2QM6epLDQ3iWJGXQ0DkalrEWAxH4/l8P8ufrvxBo2mmxWAIe8/Ly8kW1cIh1SGs2yGQySkpKyM/Pp729ncOHD1NYWBgojFkIYp20HMrcKpUqgtbEFxKG8PhvlLy8PDZu3IjVaqW0tBSLxcLbb7/NT3/6UyA2vbQuIfyYj/AIgsADy+7hO+f+H6etbjbqWuf18gD4sHJw5ElsHhMbUz8Vkk3+diAXi/b5vTYGg4HCwkI6OjrQ6/UJ2dE5EUNak3Gorh27K/JtIJrOjLDiE5nU2RbfzVzpUgHhqyhbDJpGR/ifmpPcsa4SmOhdtXbtWsbHx2loaEAul1NeXr6gjt3xEtKaDXK5nLKyMgoKCmhra+PIkSMUFRWRk5MTst3xkrR8CVORMITHj/z8fH74wx8yPj6O0+nk9ddfJzs7/DH1S4gtgkn8VUhlPLjsW9x3/iHa7Sasnvm9PBNjOzg8/gxWn4mP6b8wbftsTVznEu17P+D94Kk8sIDO6AuBgIDnnA9pkYB3ESQxR5FMbc/iSVM48cSJKq4uKaM4NS3wWUpKChs2bGB4eJjTp0+j0+koLS0NyTsQ74THD4VCwZIlSygsLKSlpYX29nZKSkrIzMwMeqxE8vB8kJBwhAcmwks/+tGPaGtrQyaTsWHDBu666655k48jjUs5POFDsM1PNTIV95d/k39vfIjK5Dr6PakoZfOTHqnETbX5JSzucbYobpzitZks2peamhqyaN/7Sc8mkTBmsVPV2BW1+fq7rKxbkkuVc+Fl6rmSNHrpC6NVi4fb5+Nf/3qQpz7zOSQXXfP+EueBgQFOnjyJwWCgpKQkqFy0RCE8fiiVSpYvX47D4aClpYWWlhbKyspIT0+fd8xE8/B8UNathKOB/f39fPWrX+XGG2/k0KFD7Nmzh9zc3EAScSRKDIOBQqGIO/FBSNwQRSikIVOZyt1F93Lemo/DGTyHl0u8NDne4sWRnyORSMjLy2Pt2rVUVlaycuVKioqKMBgMC+pYn6jnPZHxxukWvL7onveuY2Z0soWp9+qkSmp7F9/rKxKo6u3hhfpzM24TBIGsrCwuv/xyNBoNx44do6WlZd4qp3jN4ZkPKpWKFStWcNlll9Hf38+xY8cYHh6e8x6PddLyJQ/PzEjIs6JUKrnppptQKpXo9XruuOOOgHJmtMp3L4a/gWg8IZH7aYXa7b1Em8sNOd9CJ3Pi9ASvMiyXeBmXnWc//4NGuzCF4ouRyG9LiUzUXo9wddZMsFncrHBlLujYpcpsnGEshQ43/uPw3xi0zV4qLwhCQNfGX97d0dEx632baB6ei+FP5F65ciVdXV1UVVUxOjpzsnksvf2XIg2zI+EIj1arpaioiN/85je0tbVRXV3NT37yEzIyMjh8+DAHDx6MiV3xSHgSObQSjO3+VguDg4O0trYi7bSjGt2G3RVaqbdc4mXMc54nu+/H7XMtxmxg4ee9rWcEhzM2HkpIbKLWPWzibEdscmHqjw6RIw+tQaMMCc0D0W0jESpMLic//Ns78+4nkUgoKipi06ZNuFwujhw5MqN4YaITHj80Gg1r1qxh+fLltLW1cfz4ccbHx6fsc8nDE59IuBweiUTC4cOHefXVV9HpdIFcj7KyMu655x7S0tL42Mc+FnW7lEolLtfiF8tw4v1EeLxeL1ardUoiscfjQaVSBRKJs7KyWKleybNdJno8x4LK5fFDLvHi8LXzi67v8I95D6CWRld23evz8f/++FfUdoHv3r6VDP0HT/Z9MYiFd8cP0QdpXSp6s4IXDiyX6DkXLh2fCGJfSxPb21v5WGHxvPvKZLJAlVNzczPt7e1Tcl4SNaQ1G3Q6HWvXrsVkMtHY2IggCJSVlZGcnBzzpOVweKrfj0g4wqNWq6mvr48rwTaITw9PIoa0/KJ9Y2NjWCwWamtrAz3TghXt22G8m/9o/ieQzd9TbTLkEi9uXx+Pd97HPxp/QLLMsKDvsBCiuf94E/WdQ1RkGLjrwRf5169vY1npwkIlHzSIosj+CPTOCgXdTTZWFmdRawuihYkIFkfieNMePPQ2G3Py0ATZcFOhULB8+XLsdjtNTU0BDR+JRPK+8PBcjOTkZNavX8/Y2BgNDQ3IZDJ0Ol3MSIfP5wt6ffQ/pxLZuxsKEo7wANN+TFEUp1zYsfjxLnl4QofP55vmtXG73SiVSpRKJRKJhOLi4pBF+wRB4Prsf+Sl4YdQSELLkZBLvOAb5hed9/PlvO+SoTCG+rVCPu9Ol4f/2XscAKlaxui4nW/9+C/cc+uVbL08upWH8Xy9zARRFGnsHaFzaHz+nSMM+xkPsjIJHnHul4wVmmzqOmIvNBgsei0WHq06yr98+IqQjlOr1axatSogXuhyuVAqF5bgvVCqXUccAAAgAElEQVRE06uUmprKhg0bGBkZ4ezZs8hkMjIzMxekWbQYXAppzY73xVkRBCHw9hArphqPhCeePDwul4uRkRE6Ojo4e/YsVVVVnDhxgs7OTtxuNwaDgYqKCjZu3MiaNWsoKipCqVQuWOumVLsCo/xTjLtCVxGVS7woJeM82f0gnY6GkI+H0IjDn/5ay9D4hLx796gJEXB7vPzHrw7yv386hi9KlUeJ+pZ38Ex7rE0AYKjPxjp57rz7ifbECzf8rraG0wMLa8DrFy/Mzc1lbGyM06dPR62dQSxCS3q9noKCAlJTUzl9+jS1tbWB7gDRQKiEJ1Hv+4UgIT08M2FsbIzU1NSYzS+Xy+OO8MTCw+NvtTDZa+NyuaaI9hUWFpKUlDTnTRkO22/K/T8cG97CA+d/S1HKIEb1OFpFcGFHucRLEmae6vkPdmTdQblmXdDzhvIAGbPYeeaNmsC/h812inJS6Oud8Fg892o17T0j/PNtHydJHVxI4YMEr0/krdq2WJsRQPvRcVI3qhhzz9y8tFCl51zXUJStWjgkCORrksmQJ/Hrd0/y79dcRZJiYekEGo2GrKwsMjIyOH36NMnJyZSWlkbU6xMrPRxRFElOTmbZsmUMDg5SXV1NSkoKJSUlEW/lEEsNoHhHQhMeh8PBwMAAbW1tvPHGG3zve9+LWVm6UqnE4YhMh+aFItIeHrfbPa3VgiiKJCUlodVqSUtLIz8/PyTRPj/CRdY2Gop5dNXX2fnX33JK4yRdZSFPPUZR8ghJ8rl1k+QSL1qs/L7/F3w2/WbWJF8Zdtt/s/8UNudUO9IyNAHCA3C0uoN7f/gSP7hrG9kZC+9lFAwSLaRV3zfOqDV+7juHzcNSezrHZDMLIOp9WtqJXVf0OSFCTpKWbKUWhU+Kxeakc8RE74iZXiYSrL/rfZ2ffv7qaYKEQQ3/nrclPT0dg8FAf38/J06cID09PWKNdGNVou3/roIgkJmZSUZGBv39/Zw8eRK9Xk9JSUlASiVSc1/CdCQk4bHZbPT19bF//35effVV+vv7ue6662Jqk1KpjDvhwXCRBn/5t8ViCTTJdDqdyGSygNcmLy8PjSY8OjYQXu9UabKeH6+7jv/79l5Eg4RBh46akXzSlWaKdCMUJo/M2oNLLvGSjI0XB3+DzWfi8tRrw2ITQNfgOC+9O13cbWyG5Pe27lHufnA399/5CVYtzQmbDZMR72+F/pyvyY1a/9bQE2uzpqH+2BCFW1Npt08tO9fLkzgTR0KDqXIlmRIlerUOm91N94iJ4VEbw8webnqzoZX/OlTFHVdsDHm+i/Mss7OzyczMpKenh2PHjpGbm0tBQUFYk31jtfhf7GXxf9+srCx6e3s5fvx4xIjepRye2ZFQhMd/w+zcuZO3336bnTt38vDDD1NWVhZr0+K2SitU0uDxeKZ4bSwWCz6fL9BqISUlhby8vAWpD4eCcIfjPppbxD2XbeKRU8eQKTxo9HYGXckMDidzfLiAYrWNT+QL2HyNeJlKXOUSLykyG68N78bqNXOV4Ythsf1Xrx6fURm4bWAUvUaB1To1RDpucXDff+7h6/9nC1dfuSyIb5248Hg8AWJjNpunNGrV6XRkZGSQk5fPuReqY2zpDBAFUjpVkD714xJZBsfF2LSR0MrkFCalopUocDm99I2ZGR6x04abNiwhjfXLQ8cpyzDwiWWlIR03k7dFIpFgNBrJycmho6ODI0eOUFBQQF5eXlgW7ViFd2YjHYIgkJubS3Z2doDoZWVlUVRUFLboxCXCMzsSivD44XQ62bRpE8XFxfT09OB0OsnKyiI9PX3+gyOEeExankutWBRFHA7HFGJjt9uRSqUBr01OTg4ajSYmYcJI5B99efk6mk2j/KW5ifE+OepkO3KNGxEJLXYt/9UAqfL1fDRLSnbSAGZfMyIT589Pet4ZewOb18T2zK8uyvazbQO8XdM64zZRhLyCNBrOTU8S9Xh9PPLUX2nvHuErN21GKk3sB5soijidzineQ7vdPsV7OFuj1jdqmnF64iMp/2K014+zpiCbGtsEwVEJcur7R6Iyt0IioViTSopUjc/tY9BkpXfEQiPDYZvj/lfeID8thWVZwT9z5wovSaVSiouLMRqNgU7lxcXFZGdnL4qwxDKkNde8fqKXm5tLV1cXR48eDZuH61LS8uxIKMLj/2H27t1Lb28vL774Ivfffz9qtZo1a9Zw//33k5wc2RyH2RCvhEcUxYBo3+RQgNfrnSbap1ar4+bij1TC9Q83baXFNMbZwSHsJjUOsxJtuhVkIiAw5vayu8sLpJKt2sSVmQJ6dTcmbztyiZdUmY0T5iqsPjM7sr6xoPMliiJP/OXonPv4ZHOP++KBWjp6x7jv9q1ok8KX9BnJHJ6LE9rNZnNAhkCr1aLT6UK6Dg/EWHtnPpiqXSiWSXH5vKxQZXNycPFK0AKQplKTIpUjcXpJEuRoZWpwiDgtHuQeAY9O4Ez7EBA5JWeH28M9f3qVZ265AYMmuLLrYMiHXC6nvLycgoICWlpaaGtrC7ph52xzxiqkFcy8Eokk4NHq7Ozk8OHDGI1G8vPzF0x8Lnl4ZkdCEZ7JyMnJ4Y477uBrX/saIyMjPP300zHNoYmHkNbkt2WLxcLQ0BADAwMoFIpAKCArK4vS0tKYJXcHi0hWmP1262fZ9pdnGLTaEUUJ5kEdcqWLpDQHvkkP1T6Hh+c6ADIp0eTxoUwfaYo2YIDz1vP8uucBbs39/rQH8Xx9wA7VtlPbOneJb+vg2Lzn4ERtF/c8tJt/u/tq8rJSgvjmcyOcZNfr9U7x2lgslikJ7f7S3YUmbo5ZHRxtiF5n9IVgZNDOZRU5VHm76R6ZP1FZIgikyhWkyBSkKpJQ+qRI3OCz+3Ca3VjGHIwP2/F5HZiZSNQen/T/AKuX5tDVZUKVIsUR4T5dvSYL33zxNZ784nbkQSzOoXhb/J3KbTYbTU1NtLW1UV5eHnIlbqxUh0MlWlKplKKiIoxGI+3t7Rw5coT8/HyMRmPI5OVS0vLsiO9Vbw68++67/PWvf8VsNlNQUMAtt9xCWlpazOyJtofnYtE+s9mMx+MJvC37q6TS0tLIzEw8xd5IeprkEil/2nYj2/7yDA73xKLgdioY75OTlGJHnuTlYrrSYnXT0gqQz6qUMlbpx2h3dLzXiuIh5JILiYdzERWP18eTr1TNa6PV4WJJQRod7XOHQbr6xrn7od38y+1bWVcRukhiOOByuabk2/iVsf1em5ycHLRabVgXnrfORL8z+kLQdmSMDVfkc2p0kOwkDSlyFRpBjtInQXCBz+bFbnZjGXVgGrEjii7MuDCHmFfjx9C4maExK2tzcjhsjny+0MnOXv7f/nf43tUfmfeeXUg+TVJSEqtXr8ZsNtPY2AhAeXk5Op0uqONjFdJaaO6QTCajtLSUgoIC2traOHz4MIWFheTm5gZNYkKdO168+tFAQhEe/8X7zDPPsGvXLj7+8Y9TXl7O7t27ef3113nkkUfIy8uLiW1KpRKzOTK9cSYvKBaLJSDa5W+1YDAYKCoqmpbtH02xq0SDXqXmN1ddx47XXuCCM0bANp6EYPKRnGFDlIrMtKSeGXdyZlxNiiKfDVnd/Kz937gj7z50ivkbSO45cp7OweBUgZNSVRCEpp7F6uT+n+3l9i9+iO1bK4IaeyGYrVpPLpej0+nQarWkp6eTlJQU8YdorFtJBAun3YNY5SGly4eHiQqo8GXSTEVaspqu4QmiVHuul/LlaTSaI9+g9M/VdSzJNLBj/ao591sM+dDpdKxbt46xsTHOnTuHSqWirKxsXhXjWHk7FjuvP7RXWFhIa2trSDlNl0JasyOhCI/fPfnUU0/xyCOPsGnTJgBuueUWtm3bRn19PXl5eTFh9UqlkqGhxQmKzSXa519QDAbDvKJ9fsST0nI8YnlaBj/58FV8850DTGRHTEAUJYwPaJErXaToPTiZOTQw7kqiqt/I2swuftL+PT6b8RXWp66Y1cNjc7h46rWTQds3aAlejdbnE/nFM3+jo2eU27/4IWSyhT3w/HZPbtbqJzderxe1Wo1OpyMlJQWj0bggjaXFomfETG17bDqjh4plOek0Nw0hEPlzZMxJZbhl4iVHBJxdNmSpAp4oaCv95MAhig1pbCqa3csYjudyamoqlZWVDA0NUVNTQ2pqKiUlJbOKF8aySisc8yoUCpYuXYrT6QzkNJWUlJCZmTnr+KEQnkTT3VosEorw+JGenk59fT0rV67Ebrdjs9mQSqVR71kyGaGGtCaL9vnDADOJ9i1GhTTee2nFA7bll3J+5SD/XVsNFy1KbqeCoV45ulQXKo0Ph88z7XiTS82pgQnS8+LQo5wa/xifVnxkxrmePXiGUUvwInndwyZy9UmMjARPfF45WEdn7xjf/dpVJGuDU3T1X4tWq5XW1lZcLheCIAQ8iPGW9/V6dWJ4dxBF3Jbp10ykYHVMff6MmJ0sS9NRKy4sPBYKvKLIt158jWduuYH8tJnzycL1IioIAhkZGaSnp9PX18eJEyfIyMiY0csdy5BWOL0s/pwmh8NBc3Mzra2tlJaWzpjMfalKa3bExxMsSPh/xHvuuYf77ruPPXv2sHz5cvbv388nPvEJli9fDsTmB1QoFDMmTYuiOM1rc7FoX35+PklJSWFPrrvk4QkOd6/ezP6WFjpsM4UkBcxjSiwmLzlZCmyCA+9FzSH9pGd9Zjcdnjf4ubWOL6q+MGWfoXErz719JmTbMnNTQiI8ADXne7j7od38213bKMi9kNc2WYrA77VxOByB7s4SiYSsrCwyMzPj1iUeD53Rg8XS7HRamqPTRiJZq6K5d3qwrLPLSn6plk575EnPuMPJXX96ld/efD1a5fRk9HCTD0EQyMnJISsri+7u7hnFC+O1LH2hUKlUVFRUYLPZaG5upqWlhbKyMgwGQ8Tnfj8goQiP/0fctGkTBw4c4NChQ/T29nLrrbdSWFg4Zd99+/Zx99134/V6+cpXvsJ3vvOdKdt/+ctf8vjjjwd0Z5588klWrFjB8PAwN9xwA1VVVdxyyy3s2rVrmh3bt2+npaWF2trawGdKpRKr1cr+/fsZGhpi3bp1AdE+v9cmWqJ9fsxXLXQJF/Dolqv5/L7nECUz/y6iT0pPL2jUKnIyZPS7TVO2m1xqTgzksS6jD5myl9+6nuCzw1/mQ4a1APx630kcrtDf9u3iwiptegdM3PPQbr5601pK8jSBpHa/FIE/mVilUgWuxfPnzwcdLo0VGnqG6QgyByqmEEW8tshWSU1GYW4aNa290z73+nwkW2RIpExLxI8EWoZG+ZeXX+eRGz41rf1EpPJpJBIJ+fn55ObmBsQL/Ym+scpnifS8SUlJrFq1CqvVSlNTU4D4+At3LhGemZFQhMcPk8nEyMgIRUVF5OfnMzY2RkdHB263m9HRUTZs2MCdd97JgQMHMBqNVFZWsn37dlasWBEYY+fOndx+++0AvPzyy9x7773s27cPlUrFgw8+SG1t7RRC48cLL7yAVqvF7Xbz4osvUlNTQ01NDadPn0YQBHp6evjQhz4UkcqUUCGRSPB4oudST2SUpevZIM+hytvLxaGtybDapTR1+CjLysCltDLqvuB9MbnUHOnNZ1naOIakIV4dfYJq0xauVl3L3mML67re0j+CWiHB5Qp9ubI53Dzy22PsvHY1X/j0unlLwBMhBBrv2jt+LMlOpzVK3h0A+xySHO29o1SuzOXo2HRCFAm83dTG4389yv/9yOYpn0fa8zBZvNCf6KtQKMjIyIjYnLMhWsnSGo2GNWvWYDabaWpqorm5GW+E5QgSGQlFeDweDzKZjO9+97s88cQTGI3GwGdKpZKkpCSGhoa4++67KSsro6SkBIAdO3bw0ksvTSE8kwUKrVZr4EbUaDRs2bKFpqbpD1aLxcLDDz/Mk08+ydatW6mpqWHNmjXcfPPNdHR08Ic//IH//M//jPBZCB6JsIDFE25cU0H1a/24M3zMRXpAoKnfgVoupzjHQKf7QijB4ZPRa85nyCkjVztCn/AOP+urRVAXgy30njluj48lBQaamxbWg0kU4Zm/nGZozMndX7oirr0388Hr8/FGTUuszZgfoohoj96io1EraO6ZW76gqb6f7CINffboNC7973dPUp5h4OoV5YHPohVqkcvlLFmyhMLCQqqqqjh37hzLli2LqhJ/tJOldToda9euxWQycezYMU6ePBlS+f4HBQn19PMnTf785z/H4/HQ1tZGV1cXbW1t1NfXc+rUKTo7OwOeHz+MRiPd3d3Txnv88ccpLS3l29/+No899ti883/ve9/jG9/4BklJSRgMBn7wgx/wuc99jpKSElQqVcyFBy/GpRye0LC1rBi9Kgn5qARmLEifCrvbR127E5lnquek32VD7SylzZROuzmNFO0o6/7uDJqyhYVi3JLFC2q+9k49R6s7Fj1OLFHd0suwObR8plhgSZaBtp7RqM1XbNTjnec+d7q9ZDtVEww4SvjXPQep671QTRftEnGlUklqairl5eX09PRQVVXF2Fjky/QhduXwycnJJCUlUVJSwvnz56mursZimTt/64MU/koowuOHzWbjd7/7Hd/4xjc4cOAAbrebxsZG2tsnREuCXeTvvPNOmpub+fGPf8xDDz00577V1dU0Nzfzuc99bsbt8dxa4hKCg0ImZfuKJUjcEmRmgWBID4LA2IAc0T31odHoGkLlK2HApqPdasDhk7L2k+fJuaoj0J8rWAxaXcFYMi+e21uN1zv73PF+vSREsrIogjO659Ad5POuqWOISn1OhK25AIfHwz1/3svQe/IKsbi2RFFErVazevVqli1bRnNzM6dOnZqXBCwWsSqH98Nfvl9QUEBdXR2nT58O6Ld9kJFQhMcfm3z88cd5+umnSUtL4+GHH+bQoUO88MILPPvsswDk5ubS2dkZOK6rq2tOQcIdO3awe/fuOec+fPgwx48fp6ioiC1bttDQ0MBHP/rRwPZLhOf9gc+vXoYASO1SpPbgSI8oSLAPqZmSXyzAmNPCoElLjykZi1dNu1lPcXkfFX93DlnydG+gTwCXFjS5KgqNFyT0R60OcnIW3zriXPMAbx9pXvQ4sYDT7eHt2rZYmzEvyrMMtHZHp0kogEoho6k7+FyhjsZhDMrg5ArCgX6zlXtf2IvL442J12NyGE2n07F+/XqKioqoq6vjzJkzERNnjZdKKb1eT2VlJbm5uZw+fZra2tpp3zke7IwWEorw+OH1evn4xz/O/fffz5VXXklbWxu5ubkBpePKykoaGxsDmiLPPvss27dvnzKGX6YcYM+ePZSXlzMX7rjjDnp6emhra+PQoUMsWbKEt956K7A9HgnPpZBW6DCmJLO5cEI8TWaWIgky59uLFMeQekrEwCTakY0LjDvVNA0bECTQZU0DpY91O2rRLhvFB7iTwJEGrhQQZTBic6BOnhomS8uYX8U5GLzyVh1OZ+Ilsr97rgObM3a98oKFJMrenZJ8A+45vHYXw+pwUSzqohraqunu58F9b8fE6zHTnGlpaVRWVpKdnU11dTXnzp2LyLM7XoiEIAikp6ezadMmMjMzqa6upq6uDocjeE2w9wsSivD4L6ClS5fS2NhIfX094+PjHDt2jNbW1oAWgVwuZ9euXWzbto3ly5dz0003UVFRwfe//31efvllAHbt2kVFRQWXXXYZDz/8ME8//XRgnqKiIu69916eeuopjEYjdXV189qmVCrjLofn/eDhiYX9169eHvhbNiJFCNIGj0+Ga3iqUKSQ7kFikmD3KGgcysAjSnGhoMeeQsVHmsj7dBtepQ9BnJom3TQ8yuQK+fEwXVt1zf288U79rNvj9Xo5UB3/nqmyjDRaoujdARAXsKaebxlgbVpW+I2ZAy+fOc+exujnkM3mVfKLF27evJmUlBSOHz9OU1NTwle1zkUqBUEgMzOTzZs3o9frOXnyJA0NDXF7z0cCCVWl5YfX6+X5559n3759GAwGHA4HX/ziF7nzzjuBCc/GNddcwzXXXDPluAceeCDw96OPPjrr+G1tbXPOX1RUNK1kXalUxrRb+0yQSCQfqIs5XLiiOJ8srYZ+ixUBAdmgBHfmfJVbE3C5FEjGfMhTJ64FQQJSlQefR4JbJqVp2EBh6hgpKgd9jlR0mVbWf6aO2rfKcJkuhBqsTjcrCg00t01UgLUNjJGqUWCzLv5N9I1jTVx5eRlazUXkLE7eSC+GyebgSH3n/DvGGDJ3dM+fXC6ZUWwwGAy0jaPRS7H6oldN9vTpBpYbc7g6imXi84WWBEEgNzeX7Oxsurq6OHr0KHl5eeTn58dUUmShCCZsKAgC2dnZZGVlMTw8HLf3fSSQUB4e/w953XXX0dfXR0dHB6dOnaKxsZEHHnggYs07g0G8engSOaQVKw+VVCLhupVLA/+WIEE+LAEhmCRmcNiUeC0X3iWEJBHBMfFQ8YkSWkfTGLBokEt9+AQZ46KKtZ84h7586uIlKi88iHyiiLEgjXDgbFM/e18/G5axooGDZ1rxhBC2iQVK09No7opUW9BZ5sxPX5CYJcC4xUEZ0S1ZFoEHX3+X9pHoVEpB8Lk0EomEgoICNm/ejM/n48iRI3R1dSXc8zOUsKEgCOj1+ghbFF9IKMLjh0wmC4Sy3nzzTd566y2+853vcNttt/HWW2/NWIIeacRjDk+ih7SiZb8oilitVvr6+mhqauLUqVMUeexIJz04JF4J8tHgK7dsJhU+x4XbS57qAbt/PIEecwodYymIIqjkXsa8GnJWDFD2kWYQJt66m4ZGUMovjOGThe9N7OjZToaHI99uIBw4cCr+w1kKb/TfkiXSxT2+mztGWapZfDJ8KLC43Nz9p1cxO6Lzchiq4rFUKqWkpISNGzditVo5cuQI/f39CfMcvdQpfW4kVEjLz9aPHDnCV77yFZRKJTqdDpVKRV9fH729vRQVFfGlL31pzqqsSEAmk8Vd/DfRk5YjQXi8Xu+Upq0WiyVQuqrT6UhLS6OgoACFQsGBMTsHm9oCx0pcUqQWL6JOwDefXYKAbVSNJt2GIJ/YV6b04vZKEN7zlI/YNbi8MorTRpBJRECCmCyy8poGGv9WhHNMzZJ8A00tE56D1sGxsJ2Ts839vLz3NLf+/YemfB5vD/a+UTOn2/pibcacKElPpaktut4dqVSgpW/xc9p7najSpDiiqM7bOjzGfS8f4NEbrkEa4cV5odVScrmcpUuXTmnWWV5ePqVnVTxiIYTngxTSSkjC09fXx8aNG/n1r38d2PbOO+/w3HPP8fDDD8fEtnhk1R90D4/L5QqQGn9HeolEEmjaOl/7jxtWL59CeABkNikyFdhk3nl9PSIS7MNq1Jk2BAlIFCISs4CovnCkxaWkYSidEv0ISpkXldyLxyelYEsno82p2AYulKdbHS6WFKTS0b54UTtRhLPtg3R1j2LMi9/+O68nQLKy0hv9e780P53z3QtT356MEZOdtXm5HDZHp+2EH+80d/DY20f5p49dHtF5Flse7m/WObln1ZIlS0hJia5nLFhc8vDMjYQiPP4fcunSpQENHP8PnJeXx9atW2NoXfwh0T08wSZdi6KI3W6f4rVxOp3I5XJ0Oh1arZb09HTUanVID4ON+bkUpCbTMTa1UahnBAozk+lymPEoxDkDw15RimNIhSrDgSCAXOfBZZaD+sLv4vTKaRhKp1g/ilbhQioRkSogpcyEO70GzUAJVvPEG7gmVQ1hIDwA51oHeOGVau766sfCMl64YbU6aTnZM8HO4pCMAeRoVTR1Rte7A6BQhu/RXXu+h/JlehrN0VOHBnjqyCnKM/RcOylfLtwIFwHw96wymUw0NjYikUgoLy9Hq9WGwcrw4RLhmRsJRXj8KCoqwmKx8M477+B2u3E6ndjtdkpKShgfH6eqqoqrrroq1mbGHO9HD4/P58NqtU7x3Hi9XtRqddg70guCwOdXLeORd45N29Y7YEWKBIkoghR8chGfUsSnZFoxl8crxz0iojBM5C3IVJ4poS2YIEbNwwbyU8bQJ00Ig6nkXhTpXmTXNON+KwNXv4YBS/jUUr0+kc4hEw2N/Swpj26ZcjD47yf/ysm3W9i8ycgRc/QacYaCVLmaISIjXjcbBAHa+sJX/i6KIAx5kKkEPFF+Xvzbq29RqE9lVW5krr9wCwAmJyezfv16RkZGOHv2LBqNhrKyMlSqCxWWsXzmXiI8cyOhCI//4m1qauKLX/wi+fn5iKKIUqnEZrOxdetW7rrrrpgSnnhR2ITE9/CIosjo6Chutxuz2YzVOtH4UKPRoNPpyMjIoKSkJNBjLRL4TMUSnnj3BM5ZchwEQQAfSJ0CUueEzaIMRD8BUgACOJ0KJOM+ZCluJHIRiUVAlE59MIoIdIyn4vTKyNFNVBxKBFBrnGR+spfRs2l0n4ScNDWjo+FZZM+1D/L87pP8yzevnrAhTgjyu39r4uDBCb2ghqNdbNhs5LgpvkhPkSGFpvbo6u4AlOYbaFhgOfps6Bk0sbEil3fHoxvacnm9/NOf9/L7W24kUxcecc2LEYnnsV6vZ+PGjQwODnLq1CnS0tIoKSlBoVDEdA2IVQ+vREFCER7/RbR69Wqam2eO7Xu9Xu67775omgXEZ/5Donh4RFHE6XRO8do4HI5AWCotLY38/Hw0Gk3Ub+YUlYqtS4p59VxwfZwEQZgosvIK5HrU/PsntvJWZxuHujtpNY0iyHxINV7kWg8ukxySLiakAv0WHU6PjILUC+KDKrmX7DVDjOY4MNRWhI3weDw+xl1uTtV0kqKLj2t4ZMTKE784OOWztmPdrNqUw5nx6BOM2aCJ0eNTrVbMv9MCcPZcHwXlyXRYTfPvHEYMWmzc8+e9/O/ffRaVPHGWJL+QX0ZGBr29vVRVVZGVlYXRaIwZ6ViImnU8rl2RQuJcXZNgNpvZvXs3cvzJ0dkAACAASURBVLkcl8uF2+3G5XLh8/lwOp2kpaVx6623xtrMmCMeCY/P58Nms03Jt3G73YGKO61WS3Z2NiqVirq6OgoLC9FoIvPmFyxuWLU8aMIDgAirNSn86svXI5VKuMyYzT1sBuDt9lbuO7cblF5kSR7cHgnCDHfhmEONa1hKcdoIcukEKRIE0GdZGNVV4zubh8QTHmG0+s4hnn/xJP/w9yvDMt5iIIoiux57A7N5atmyzycycKKfJWvTaTAvrOt8OFGoT6GhPfoeJxHoGIiMjo3X50NnkiBIRMQoL4Jnewd4YO9b/PtntibcAjxZvLCzs5Pjx4/j8/liEl66FNKaGwlJeKxWK8899xzp6ekIgoBMJkOhUCCTyXC73ZSWlsbaxLhArAmPx+MJ5Nv4Q1KiKJKUlIROp8NgMFBYWIhCMfMba6zt92N1bhZL0vU0DM3vXRB88JU1q/hwlgbpDDopHyksZpf6Rr5e8xwSmQ+pQ8Anm/k72twKGobTKUkbQS2/IHmgTnKScuMA5j9kE4z683xwujx4FQLVp/v42EdS5z8ggti7t5aTJ2duQeB2ebGdGaWgIpkOa2w1hLQzsdQooCRPT/NA5Lxc7b2jbFqZy5Gx6Ia2APacbWBJpoFbNq+N+tzhgEQiobCwkIyMDI4fP87hw4cpKioiNzc3aiTuEuGZGwlJeLKzs/nLX/4SazOmQSqV4vF4kMvlsTYFiK6r0ul0TvHa2Gw2pFJpoAQ8Ly8PjUYTklx7vBAegM+vXs6P3vzbnPuofFJ+9tlPUpGdRktLy6z7rcs08q2ST/KTtn3IZg1tTcDtldE4nE5R6ijJqgtej5QcC65PDOM8kL6wL3QRmrpHGO8zccWH526iG0l0d43y1P/OfY5tNhcpjVYyilUMOmPT/LAgLTbeHQCdTgkDkZ2jsb6f7CINfXZrZCeaAY8cPExpup4rygqjPne4IAgCWq2WlStX0traypEjRygpKSEzMzPiz+RLhGduJCThsdls7Nq1K+DR8Xq9uFwu9Ho9t912G7/+9a/52te+FnW7/GrL8UJ4IgF/CfjkfBuXy4VCoUCr1QaSiZOSksJSJRUvhOeaZWX8/NAxrK6Z+6XlKjT8+ubrMGiTsNvt89r9udKVNJuH+fNI1XuhLSnCLJ4enyihZVRPXrKJDM2FRSh95Sh9w0q8JxffIsDmcKPMTeLtd1r4wo3Rr9jyeLw88rMDuIJolTA+ZielzYc9T4Ylir2g/EiWyom+/2MinNU5GPlwntPtJd+pok+0RF0OQAS+8/IBfvel6yk2hKeVSrThT1pWKBQB8cKmpiba2tooKyuLqHjhJcIzNxKS8AiCQH9/P6mpqUilUhQKBXK5HIlEglqtJisrNiW2crkcp9MZ85yTcMHr9U4pAbdYLHi9XpKSkgIl4EajEYVCEZE3l3giPEkKOVcvLeXPZ85P3SDCR3Lz+c+bPhnyOfjmZR+h9dAwp9wtSB0ivjnvRoFuUwpOj4y85HEEYWItyryin75hOWK7aq6Dg0LXiJW9r9ez/doNqNXRJe3PP3ecxsbgXRfjo05KkjWc19hxBdPuI0zIT0umvm3xgn8LgTEzmc7R6CQUN3UMUbkqh6rR6KtcW5wu7nr+VZ750vUkqxd/XUcbF5MOlUrFypUrsVqtNDY2BlSbIyFeGErScrw8W6OJhCQ8arWan/70p4GEZZ/Ph91u5+DBgzQ2NnLttdfGxK54bCAaLPyl35NViQVBCJSAZ2VlUVpaGtES8IsRT4QHJpSXJxMeiQ/u2lzJ339ozZT9QrF714c/xw1vPEWvdnjO0JYfQzYNTq+UotTRCYFCqUjGp3vo/30+wtjiSIrN4aEwX89Lr1Sz48bKRY01H/zJ62azmdraTp77Y1XIY3S1j1Car6VB5yFafp4UqYJYNbow6LVRIzwAHY3DGPJUDMcgdNgxOs6/vnKQWzdcRpJCjlopR/3ef1UKWVwnNs9Wlq7RaLjssssYHx+nsbERmUxGeXl5WF+QF1KWHs/nMtxISMIjiiIvvvgie/bswW63I5FIcLvdvPDCC9xyyy1cd911MSE9iUB4RFHE4XBMybdxOBzIZLJAlVRhYSFJSUkxd43GG+EpzzCwOieT070DqEUpu274FGsKsqftF4rdgiDwmyv/juve+hW+JDtutyzQe2s2mJ0qGt9LZlbIvChUXvQ39DL6dB64F1e51T1i5ZW9Z/jUJ1eSkqJe1Fh+TPYUTu5fptFokMtVPP/cORb6M/d3WlhfkcUx31jEwy95qbqYeXdEJvqKRRNWh4vlYhbDRJfwCCIsVWg4XdXJVw+1Tt8ugFoxQXzUSjlJk8hQ4L/vfT7Y30e7s3rqNsV7xyjlqN8bQ62Uo5BJw7L4zxdWSklJYcOGDQwPD1NbWzujeOFi5g41T/KDhIQkPD6fj3vuuYcnnngCrVaLTCbD6/Vy9OhRvv71r5OdPX0RigYUCgVu98w5HrGAz+fD6/XS29sbIDgejweVShXIt8nJyUGlUsXlhR9vhAfg+lXLwCXyyPXbSE6a+QEV6rnUKBT8+vK/Y+fhp/D5RIKRinR45BMVXPphkuQeNMlOXDf2Y/19Doup3DJZnawqzuS5F05w261bQj7e4/EErjV/Zd5kT+HF/cue+MVBBvoXt5A3ne3n8vV5HLYNRZT06OXKSOcLzwpjZjKdI9HVxwE419LP2lVZnBqNzjdfkqJHGPXS0Tl7rpIogs3pxuZ0g3l+ParXaoPzyUklAmqlnK2XlfHNG69EtsBu9MHm0RgMBvR6PQMDA5w8eRKDwUBxcfGsVavBzv1+ziFdLBKS8EilUpYsWcKnP/3pKZ9v3bqVNWvWzHJU5KFQKGLm4Zm80FgslkAJuD/sl56eTlFRUULdDPFIeLYtLeOa5eXzkppQ7c7XpfIfqz/PvbXP47LK5g1tAXh8E+0oygzDqOUe0rKtuK8ZxvXq4iq3ekYttNUPsP3Tq8nKTJ51P5fLNYXcTK7M0+l084pFHq9q47V9Zxdlqx/1J7oj2oIiL1XH+dbYeHcA0qMczpqMwdZxtBkKLG5XxOYwqFSUSFM4W98fsTnmg9cn4nC6OXCikaw0Lbdu27CgcUJRWhYEgaysLDIzM+np6aGqqors7GwKCwsXlD5wKWl5biQc4fFXBP3xj38MfGa1Wmlvb2fnzp0xlfX2V2lFEn4SMznfxm63T1loJpeAV1VVUVBQEFGbIoV4JDzBvPUt1O7N2QXcbd7Kz9rewO2Wzhvagvd6cI1MkB6VzEPmslF6hxR4j81OVObD8JiNleWZ/P7ZY/zTXVdNueb8/5scBtXpdKSnp4dUmTc+bmfXz99csI0zoeFoF+svz+PEePibeRoUKgaIDeEAGBiPfom4H2MWBxW52Rx1h5+MyCUC61OzaWga5KwrdmTHjyXGDOraB/ifvVVsXJZPRWHoBTALIR2CIJCXl0dOTg6dnZ0cPXoUo9FIfn5+SGNdIjxzI+EIz3XXXccf//hHDAYDFouFrq4u9u/fz/79+3G73Vx55ZUx82IoFIqwEh5RFAOJnX5y41cl9pObzMxM1Gr1nAtNPPX3CgXxSHiCxULt/kL5ZbSYR9jdU4MnyCbhHp+UpmED5YYhlDIvWR8aoHdYjti88Byc3nEzjWd7Wb5UjT5NjkKhCJAbvxL2Qq8pURT5xeMHGRsLXyNUP9qP9rBqYzZnTOHr/J2TouV8a6yCWZBl0NI9FFt16bMNfVRUZHB2PEweNBFW6zMw99k53ROLIv+ZIXnvmvb6RH7wm9d5+ts3kaQMbT1ZzPPWL16Yl5dHW1sbhw8fpri4mJycnKDGvER45kbCER673c6Pf/xjNm/ezPPPP8+pU6fYuXMnTz/9dET1DYLBYpKWvV5voPR7phJwfz8ppVIZ0rh+0nCJ8EQPiz3X9637OK2WYU6O9IAmuOavftJTZhhGKfOScU0vA8/kw8jCyP/wmJ2lZQYOVw3zg+9+JqzXz5tvnOPokdmFGRcDn09k4OQA5WsNNJrD45HJUKoZEqObMDwZWRnJ9JpiqywNYOu2oUqR4pilkW6wyE3SkuVRU38udiHCmaCQSWnqvuAd7Boc57EX/8Z3dnw0pHHCQTpkMhllZWUUFBTQ0tJCe3s7paWlZGRkzHkvXiI8cyPhCM+BAwd48sknuf/++xkfH+eBBx5gy5YtuFyumC/swRKeybkPflVivzqnv5eURqMJSwl4opIGSFzbw2H3E1uu5/o3nqLLZQJFcGO5fbL3wltDKORe9Df2MPxrI4JrYZVbZq+Pluouzp7rZeWK3AWNcTH6+0z86lfvLOhYhUKCx+3DN8/pcLu82GvHKFix+BYUsfbuAIxawu8JWwgGx6yszcnlsHlhHhmlILA2JYe6xj7GvLEncBejPC+ds21Tw2ovvVvHhysKuWJVcdDjhLNjuUKhYNmyZdjtdpqbmwMaPnq9fta5Q1kDE/FFeDFIOCool8u58847qamp4bXXXqOmpobPf/7zfO973+Ppp5+OaVn4xSEtf0hqYGCAlpYWampqOHbsGGfPnmV4eBilUklhYSHr169nw4YNLFu2DKPRSEpKStj0biQSCT5fcF6CeEOiEp5wQCqR8NuP7CRNUIdUsu3yymgaTsftkZCkcaH7Qh/B1X1NR8+gibJlWTz9u8Nh+R28Xh+PPHIAhz20SkYRWLoihxJdEmV6LUsLDcx3UmxWF7ImGxmLLPXNVIV2/sMNQ2oS7f2RaRa6ENSe76FMF2K/NRHW67NJGZFy+nwPHm+cPo9m+aF/+PuDDJuCJ50L6Vg+H9RqNStXrqSiooL29nZOnDiByTTdg3nJwzM3EvbMiKLI0qVL+elPf0p1dTU333wzf/7zn6dcBPv27WPp0qWUlZXxox/9aNoYv/zlL1m1ahWXXXYZW7Zsoa6uDoDh4WE+9rGPodVq+frXvz7lmKuvvpo1a9ZQUVHB7bffjvc9967T6WR0dJS33nqL2267jQceeICqqiqam5uxWq0kJyezdOlSKisrWbt2LeXl5WRnZ6PVaiN6gSYyaUhU28Nlt06h4lcfvhG1I7Qwpssro2nUgNsjITXDhmr7wpN4LT4P5xv6OVo1XQ8lVLy0+xTn6kLzDuTkpWIsMaAwO2k524NaKaPlRDslKUkU5c698I6N2jB0e0leYE5fdoqG8y2xDbvkZcW2mevFEEWQDHmRBbmglySnUiHVc66uD4tz/rYhsYJaIaOhe+b7ZMzq4N9//2bQ93QkSYdWq2Xt2rWUlZVRX19PTU0NVuuFhPZLhGduCPP8iHG/2vjtv5hRe71elixZwoEDBzAajVRWVvKHP/yBFStWBPYxmUwkJ09Us7z88sv84he/YN++fVitVk6dOkVtbS21tbXs2rVr2jGiKLJu3TqSk5Ox2SbYv8/nY8WKFXzmM59h48aNMc8pAjh9+jRLly4NOfcnHtDV1YVEIiE3NzzhlGhBFEWOHz9OZWV41IoPdbfyT9UvIypCezNWydyUpQ4jk/voPZKJ992FSdkvzU7DY/Pw2E+/MGMH+GDQ0jzIt7/1PB5PcN9BrZZTVJ7Jufo+VuWlUf/uRM5PToGenp4LHo/s4hRcChX9w7NXMRkL9TTonDhC7Lu1JiuTuubYVg4VFehp6Y1cd/SFYlVFLofHZyevqQolS1V6aht7Y+ohCxarirI50zq3Xs83b7yS669YOe9YnZ2diKIYlerY4eFhGhsb0el0lJaWcv78+aDVm/2ht8Xo/sQpZmXjCU8FBUEIkJ3JoZtjx45RVlZGSUkJCoWCHTt28NJLL0051k92gIBAGkxIgG/ZsmVG5Uv/MR6PB5lMxqc+9SkOHTpEVVUVf//3f8/mzZv51Kc+FRdkBybOz6WQVmLBr4Y9ODhIS0sLycNmPq8oItTIlMMjp3nMgMcjIWvjACxZWC6IUwodXaO8+Vb9go53uTz87OEDQZEdEVhWkYMkSc7Z872UZafQcPhCgnNvxwgZ2Rfu277WcUYbB1iZn0aKbmZS39U+QoUriVAymbKSNZxviW3uTopOFZdkB+Dc+T4KNNOb1kqBjfoc5AMiZxoSg+wAuINIxH5s999o65v/94iml8VgMLBp0ybS09M5efIkJpMpEHW4hOlIeMIzGZMvsu7ubvLz8wP/NhqNdHd3Tzvm8ccfp7S0lG9/+9s89thjQc2zbds2MjMzKS8v51vf+lbAexKPrSUSmTQkqu2hxO8n53k1NzdTXV1NVVUV9fX1mM1mdDod5eXlfOeq7WT2ZSJaQ0tAtnvktPTrEUWBrKt7ITN02YS27lEKS9P5w3PHcC4gLPG73x6ms3P+hSI7N4WCsnTqGvsxmx0Y07X0n+6etmhmXCSGKPpE6k914ukZZ1VxOirl9Py31oZBNkjT5s398SNHrcEX42uvICd+u4V7vD6SzVKESedoeaqBEm8ytXW9WB2R1SMLJ7RqBY1d84d9XW4v//qb13F75iYU4UxaDgZ+8cLNmzcjlUo5ffo0LS0t8xKfRHy2LhbvK8KzENx55500Nzfz4x//mIceeiioY1577TV6e3txOp28+eYF8TSVShV3hOdS0nL8wOfzYbFY6O3tpbGxkZMnT07J80pJSWH58uVs3LiRNWvWUFJSQkZGRkBn6dqypQgtaoReJRJv8KTKJlHQ0pKBRCJi+HwPonoBb4AqKUPDVl7ddyakw2pqOnn5pZo591Gr5SxfnUffsIX2jglilKpV4mkfxTlDgvNgz9iMxMXl9HC+qg212cHKknSkkqnnqPFML5cnpc9LejJ1Gs7FuDILwOaKnzY1M6GtZ5RNablkqZPYkJRF+/lhugdjJ864UJRk6/EG+Yxs6BriV68em3OfSCQtBwN/eGrDhg1IJBKOHDlCR0fHnM//S1Va7xPk5eXR2dkZ+HdXVxd5eXmz7r9jxw52794d9PgqlYrrrrtuSpgs3nppQWKThkS2XRRFzGYzPT091NfXc/z4cY4fP057eztutxuDwcCqVavYuHEjq1atori4mPT09Dlzrb686TKkggSGFMiatCgswSfjWrUyWs5noVB5SLmpD1EIjQQ3dQ6TV6jn+RdPYrEGR+otFgePPfrGrNv91VdSzUT4yu9RUcql6G0exmbJyxnqM5FbOHNZLoDF5KD+WBsZwPLiqQSn/kQ3m5Iz5rQ7V6PBN1/te4ShS1LS3BN+xehwQiGRoB30oRqAupbYqyQvFA5XaF7L371xilNNPbNuj7aHZzJ8Ph8ymYyioiI2bdqEy+Xi8OHD9PT0JOyzNJx43xKeyspKGhsbaW1txeVy8eyzz7J9+/Yp+zQ2Ngb+3rNnD+Xl5XOO6X87h4kcnj179rBs2bLA9mi0lggVlzw8kYfX62V8fJyuri7OnTtHVVUVNpuNzs5OvF4vWVlZXHbZZWzcuJGKigoKCgrQ6/UhK4IrZTLW5E1I3btd4G5VIbQrEYL09lh0EpqbstCm2VF/NnTF3P/P3ptHSXZXd56f92LfIzIiMyMj932rRaqqrJJkgRCLZQQqTBuwzLSnGSzbooXBpt09Oo2HpgHP+HSfoaFHsjHtHiOMBcZ0j6BNWy1kbINaylItUq257/ueGZEZkbG89+aPrIjKPSMiXyxZFZ9z6pzKWF78Ynnvfd+993uv3q5ndTXMf3npSlKP/+af/oyF+d37rXh9DqobPHT1z+D335nGLaDQYDYyObz/yd7pPLgoc2E2QP/FYWpsRurK77id+i6Mc9qxe41dsdVMV45rdwCqy105T6ntiaJwoqQYX0DHzYtj1OutuV5R2pj1GvomUtsXFAW+9BevEgjuLvxzFeGJv3ZcbMWbF3Z0dLCyskJnZydzc3NH4piaKY5c48Fk0Wq1PPfcczz22GNIksQnP/lJ2tvb+cIXvsCZM2c4f/48zz33HK+++io6nQ6Xy8ULL7yQeH5NTQ1+v59IJMJLL73EK6+8gtvt5vz584TDYWRZ5tFHH+Xpp59OPKdQw6Mu+bj2aDS6Y2CmIAjYbDasVmtijtmVK1e2OALV4tMPd/Ab3/tvd27w61G6tYil68ie/VNVghaCioa+sRIaa2aYfaee2M+Stz33jMxTVe7kv/34Oh98/wncRXuLjp//rJef/WPvjtuNRi11TaXc6p1Gnt/53Z4oK6LnwsEW+KnRxY0zTxInlonb4qmp1csqApNzgY0RFOe8XF/ZOoKi3GpjeTp3c6vihPO08LTcZsET1jF46U6EY/jWDNZqPat5noLbjQafh2tDqUenppdW+b9/8DO++L++b8d9ubSG79Z4UK/X09raSigUor+/P9G80OFIz7V5lLlrBQ/A448/zuOPP77lti996UuJ/3/961/f87nDw8O73n7x4sU9n6P2LC01KER40mf7wMxQKIRWq03MMauursZsNmf14HbCV4rLbGQpuCkqIosoU2bE5ShCdRhpv6GjdoX1CSP92hLqT80xPa9DuXVwtCSOxWNiemKZ7/31RZ757Xft+pj5+VW+8Sf/uOU2BWhpK2Ny1s+N7t3tzMcrihL284NYnl+lqqmE0cHkr86HuqYRBGg/WclkYJ3Zy7M03u+hL7Axp8pjNdOdB6kZs0GXd+ksq15Hs8lO341p/Nt+XqFQlGPWUjoXcx8ZS5XgIXoD/Y9LfTzUXsMvnt6aGch1x/+9XttkMnH8+HFWV1fp6+tDlmXuv//+nM2ezAV3teDJNvmY0sq1aDgM2Vq7oiiEw+Et4iYcDqPT6RIDM5MZ0potfvl4M39+YWchsBLSoenTITiDxHx7RwiU8ihroxYGgZr3TjOzWAHTyfVp6h6ew+e185O/6+JDHzxJRflWJ5EsK/zHr7/K2qY6n9IyBwarnq7+vcVEg9fBQBKRnc1YzKn3llIU6H17DJ1OQ8vJCib7V6mstTAWXKPSauNGHkR3aiuLuD6Se+EFGynGU6Veprrn6Q3s3admrHsOo0/D+gEOpnzCZTPRf0hh+e+//4+cqPXiLbpj0c/35n/x5oUrKyuqdfQ/KuTvt3IEKaS01CUTa4/bwGdmZujv7+ett97aYQPf3BG7oaGB0tJSzGZzXogdgN84d39iqvN2YhJIC2a0XUaE0N7rFbxhVuYsjMy6cX94CsWc3JWuooDTa0WWFb7z3Z1ulf/+42tcuzoOgMGopfVEOTNLqwyP7H1iKXNZWLg5hSyl9l2PD84lNU1+N6JRie5LI8Qmlmlc1dLgdOR8ZlacWJ7srw1FLloUG70XxwkE1vd97OpqmJN71EXlK9XFh+9ivRqK8KXv/N0Wl1cui5ZTwWaz5c0xLVvcW/Iuw+SjS+teTmnJskwwGNwSuZEkCZPJhM1mw+VyUVVVdeQ6jRq0Wk6Ul/L2+N5X3FJMB30atEVhYhW7iBk96LQxFlasiIKC51enWfmWD0E5+EDdNTxHicfC650D9PXP0NiwUUg9NrrIt194fcN91eplej7AzT3SV3HsZj3ClJ/QWuqR0VX/OiU1dmbG0x9EGVwNc/O1ATxldorrbUwF9z+xZxqtVsh5OqvIZKReZ6P77b2dSLsx2TOPrlggmieC7SBW1tT5rt/qn+S7f3+Vf/qe+4HcFi2nylFZp1rkvww9QhQiPOqSytolScLv9zMxMUF3dzeXLl3i8uXLCRt4cXExJ06cSNjAa2pqcLvdR07sxPmdhzsOnvsiiEhLJrS3jAjBnQc2xS1h9MvM+W0shM2YfyW5uVGyrOCp3Lg6/tZ3OlEUhWhU4j989Sc43RZqGovpHphleSW073Z0GpGSqMLibCCp191rG2owP+VHc3WeJvvO7sHZpKzIRCRHaSGtKHC2pBTNcIjuG6mJHQC/f50GgykDK1OfYoeFoemlgx+YJH/6NxfoHd+oJzsqEZ57kUKER0WMRmPe1fAc5QiPKIq7Cp5YLLbFKRUfC2KxWLDZbJSVlWG1WtFoUutKfJSIFy8vJxGRkCQd9GvQOsPEKmNbJs3I5VF0UwZmsSM4VzA+ukTs7w/u8Ns9MofLZeb6jQneujpGz60pjDY9Q1NLSAtJRFwUhWa7mf7b6a90mRsPoNWJxKKH/40HA2HCr43SftbHzfX9xVqmSCLAlhHaPW7Wx9founS47yM0FUZ0Kch7jzPKCyo8duaW1avXikky/+aFn/Dn//KjRyrCc69REDwqko8uraMe4ZEkicXFxS02cI1Gk3BKVVZWYrFY7skrql8+1sy33ty/i3ECQURaMaEJRFGqw8jW20N3NSC6IsRCemaWHXh9K2iPr6Jc37+3Siwm460vYmUpyJ/+2c8IrUdZXt4qEhRFAVlB2CUKc19FEd1vHH4CeyQsUd/uo79n/8GPySLFZKZeH+fMg1VcWl9NyvauFhoRppazK7TKbFa8MT39V9T5/JaWQtzfUM7lhdxOmT+IeX96c+X2Y3hmiT/+0Rs8Wm/NyfHoqB7ns0lB8KhIwaV1ODY7pVZXV1ldXUWSJLRaLTabDY/Hg8lkuifFzW489cD9fPvitZQa1MmyDmVQg9ZxO9ojgmJRMAYkQnoN08sOStv9aBfDMLG/C6pndA6r3cDU9M5xAoqiIEgyyMpGRGnTd9ZeUUR3kvbz5FD/9z34xihnT1XylhgimqWuy/WVHronU28ImQ4mrYYaUc90zwr9SU6wTxb/aADBoqDkaZSnrMjG2OxKRrb91z+7TrG+mdbW7L/3XNvhjwKFM4eKFGZpJYeiKIRCocTAzKtXr/Lmm2/S1dXFysoKFouF+vp6jh07RlFREY2NjXi93ns2krMXBq2WE77SlJ8nCCKS34Smy4To3zhAyt4YhqWNE/tMwE70XBBs+zu3IlGJivqdYxoURUERFBStiKLXoFm7U8hfV+pg+MJwymvej5HeWQwm9XuJ9F8Zo3VFxK7LznWhbpehp6qjKNxfUoJ7QWC835/UBPtUmZ3xc9JTovp21aLUldk6re/8z0H8e3RhziTpevBV1AAAIABJREFU2OHvNYFUiPCoSCGltZO4DTwetQkEAkSjUYxGIzabDYfDQXl5OQaDYcfOt7a2dmSiU7nid95xhk9+72/SupaWZS3KsBmtLUKsOorii6CdNxCzwGzYiucXguh/YkWQ9j6I9k0sYDLrCAU3RI3CRh2KIAGa22JKJyIGoxT7HCx3TyNJ6p5kY1GJihoHY0PLqm4XYLxvllKvHWuDnclg5tJNoigwNH3wRPnDUOtyYFlR6L80kdHXAQhPrIE+uU7YWUWB2aX0XX3J4A9F+fd//Rr/7rcez6qgKBRLH0xB8KjIvW5Ll2WZtbW1LeJGkiTMZjNWqzVlG3iuxdpR4ITPi8tkZDmUnsVWEESkVSOaW1qUijCiKUpM0oFGYF5joujsOqY3TLCHpAqtR2lq9tL11vhtsaMgSMpGQcptZKMW/VwQYcpPMJCZK1+NkLlD2cK0H/NqmOYzZfT403eU7Ud9hZueqcyksxx6Hc1mF93XJthlmkdGmJxY5nhHGdcX8qtjdHmxPSsT3V+7OcK3//Z1fv2XHsyaCMn3hof5QEHwqIhGo8m79FGmRIMkSQlxExc4iqIknFLFxcXU1dUdqpNnQfAkx4eON/NCssXLeyArWpQREZ01ilEns+4UAYHFIh3OBglL/97fY//UInqjhvVwDFkjoNm+CwgCMZuexeUQAntJp8MxNjCHxWZkbTUzgiq4Gib881Huf0cNb/nVr//IREpOL4p02IoIjAaYYZFs70rKbBjE/IrylDgsWRE8AH/+dzdwaSI8cH87xcXFGY/2FNxhB1MQPHc5akR4YrHYlqhNtmzgBcGTHL95u3j5sJ+VIIrEggZEJHSKRNQFILDcCPK6ghgVUITb1mmRxP9XhChiqQbptpRJWKvFjcdv3KxDwbxF7ThvBXHdUscaLEsKlbUeuq9nLl0jSTIj/zDIuYdrubC2ot6JXICRGfV6wggonLa7CfUv41/0MzG6SNupKhYP6IukNmMji7Se8dK1mNlUXdIoMLmQmQjdboSjEi9dn6fSO8Xw8DBNTU04nYfv7rwXhQjPwRQEj8rkm8JOVTREIpEd08A1Gk1iGng2beAFwZMcG8XLJVydUGf+kowGMSCCPQoaQITVBhn97EFRiNR+E/5GE47uNUSVgqLrGYrubKfvtSHOnqrgLXFdFQdXXbmb/hkVUj+KwjFHEcrAClO3xvGUuxi7PdJj4NYUFpeRtVB2awwNS/kzW6u61MnIjPp1XvvRNTrH5YlqfvUdrfT29qLRaGhqasJsNqv+WgXBczAFwaMyiqLklT1wL9GgKMqOaeDr6+sJC3jcBp7LGVIFwZM8n3lHR9rFy7shKAK6RQ3R4o0TlmxRkI0y4rp6B1TZILLSasV1U50i0tH+WRweKytL6vdY2U7/lXFaG4oZdmvwR9OfuA1gtRy+23e12YxnQWH4yjBGk47iyiJGh+7UBIXXo7SV+7jar06/nWQZGpij8XQJfUvZFRq74bKZsi54AL71ymXOtVZx+vRpFhYWuHbtGna7nYaGBlU7vRdcWgdTEDwqko8/HlEUkSSJUCi0RdxEIhEMBkOigZ/X68VoNObVezjqgiebwvewxcu7oVnTIJllZMvGdxAtktBPCggqVuH46404bq0iqvA1Kwr4KouyIngAxvvnKF09nINLAUbn0j8JlxiN1IS0DPx8glVAb9BQUlXEyODOAuix3hl0BpGoCl2pU8G+mgfHFIWM9d45CElW+LfffpUX/veP4Xa7KSoqYnp6mosXL+L1eqmpqVGlHKAQ4TmYwqejMrkWDIqisLa2xvT0NH19ffT29jI7O0tfXx9ra2s4HA5aW1vp6Ojg5MmT1NfXU1JSgslkyvnat3OUBU8u1n7+eJO6GxRAt6iF21kJxaAgWdU9WcpGkZW2/bs6p0JgUb1xAcmwMO1HfHuO5jRncNX6XCwGUhdLVp2WXzAVIV6cZ+DKRt2SVqfBUWLeVewABPzrtFbt7JuUafp7ZqiyWrL+upup8xWxkIHuyskyseDna//1NWDj2FBWVsaDDz6IRqOhs7OT8fHxQ9da5lNmIV8pCB6VyeZJTpZlAoEAk5OT9PT0cPnyZS5evMjQ0BDhcBi3201DQwMej4cTJ05QW1uLx+PZtedNPnKUBU8ueOrcKdW/V0EW0C7dufqMuSQUQd3vxN9gQlZp2RPDC7hLsjsANLgaZvHno9xvd6T8XLvdmNLjtYLAOZuboptrdL82RDSyoUa1Og2VjSXMTu9/Up8dXULMwa7vXs/tXDu7af+u4dngbzq7+YerA4m/RVGkpqaGs2fPEgwG6ezsZHZ2Nu1jXiHCczCFlJbKCIKALMuqO5YkSdoxMHOzDby0tJT6+vodNvCj3LzvKAueXKzdpFO3eDmOdlWDbJaRzQpoIeaU0C2pd+iQDQIrberV8pSWOVg4xAT2dEjHwaWwceWfFIrC/U43azcX6J/ZOpZDo9NQ1VTKQO/B3/vCXICWU5XcGpxN7nVVYqhvnpIWG7M56kQ/rKIL7jD8X9/9B9prvBQ77kS8dDodTU1NVFVV0dfXl7ajqyB4DqYgeFQmPk/LZDKlvY24DXyzU0oQhES9TXl5ORaLJSlRVRANuSFXa//0wx089VfqFS8DG6mtBS1hYxREkOwymoCCGFOxlqfBpFotz+K0f6OgJwdRzDsOrjDRA1IUVV4nIwsH1+802x0YR4OMXtk5bFXUiNQ2e+lLYXjq2txa1j8fRQFvVMss2Rc8jeUe+sazM6PsIPzBMH/4lz/lq09/EHFbqM1oNHL8+HECgUBajq5UBM9RPa4eloLgURm9Xk84HE5a8Gx2Sq2urm6xgdtsNqqrqzGbzWkr93ycpZUsRyHtlm/cV65+8TLcSW3F3BKIECuKJWFTTx41ozwzE8t4q4uYnsiNM+iOg0u7r4PL5TTvK3gqLBZ8yzDw96O73i9oBOrby+jtSs15NTm2SMPxMvrHstsfZ2xwmdJ2BzNr2a2lMenVb+p4GC50j/GDn1/nY4+c2PV+m82WlqOr4NI6mILgUZm95mkpirJjGvj6+jo6nS4hbjJRPHyUoyRHmVx87tFolEAgwCNVpfywZ0T17WsC4kZqy6RkxKauZpSnyG3NmeCBgx1cCjCzvHvazaXX0yKb6X9tlIE9+vwIokDjsXJ6bk2ltT4poK4gTgZZVqgRLcyQPcEjCDA0pY6wO1vp5alfeZDLfRNc6p3g2uAU0TTnwj3/wzc401RBXVnRno9J1dGVquC518QO3AOC5+WXX+azn/0skiTx1FNP8eyzz265/xvf+AbPP/88Go0Gq9XKN7/5Tdra2lhYWOAjH/kIFy9e5BOf+ATPPfccAMFgkI9+9KMMDAyg0Wh44okn+KM/+qPE9gwGA8FgkJs3byKKInq9fosNPC5uysrKsmIDP8oRngJ7E41G8fv9CQEdCoUSPZR+/WQrP+odUX2UgCAIaBe0RHwbqS21beqqRnnGlnKW1oqz3wwuX7F9R/2OUaOhPiywdHOZ3vW5vTcsQNOJCrpvTqa9tvHhJdxVVhb82Z39139zCneTlYX17AiuhrIi+iYOL3ia7A7eXV/N8boyjteV8YnHzrAeiXFtaIpLPeNc6h2ne2wu6X0uEpP4Ny/8hP/8Lz6CXrd3aULc0VVaWsro6CidnZ1UV1fj8/l2iJtCDc/B3NWCR5IknnnmGX7yk59QUVFBR0cH58+fp62tLfGYj3/84zz99NMA/OhHP+Jzn/scL7/8MkajkS9/+cvcuHGDGzdubNnu7//+7/Poo48SiUR497vfzZ/8yZ9gsVh46623uHLlCh/60IeoqqriU5/6FOfOnaOiogKDITcugUKEJzeo+bnvJ272igwe95VyTeXiZQBRupPaitvUtavqFeirFeVZnAtQ0VDM+HBuh1fGZ3CdemctV1buRJxK3FYmljcEjwiccbiZf2ua6YN6CAnQcrKSrkOInTgWQSTbn04sJtNqtGdN8OgPMcsvTrnFAv0B3vNs+5bbjXotZ5srOdtcCYA/uM6l7jFefv1tRleiBzY57J9c4E9/fIHf+eWHDlxD3NFVXl7O0NAQnZ2dNDQ0bJnRJcvyoWYX3gvc1Z/Om2++SUNDA3V1dQA8+eST/PCHP9wieOx2e+L/8RlRABaLhYcffpj+/v4t2zSbzTz66KMAhMNhent7+da3vsXHPvYxnnjiCUZGRnj22WdpbW3N9NtLikKEJzekK3i2d7+Oixu73Z5S2vN3Hu7gN//qb9Jd/r5oAiKSRUYxKsRcEpo1EUHJvyiPzZaa5TtTSJLM8N8P8OA76nljdQkEgXn/RuHwCWcRsd5lBi4PHrgdBWi7r4pbN9SZFzY16qe0zs3MgjruuGQZuDGNo97ISjizYy5EAQYmDyfpXAYD1uF1zj5Yj9W6/+/Jbjby8LFqbNIyHR0dzC6vcql3nEu9E1zqGWduZWePqO/+/ds82FbFmaaKpNazn6NLUZRChOcA7mrBMzExQWVlZeLviooKLly4sONxzz//PF/96leJRCL89Kc/TXr7kiRhtVr57ne/mxBVL7zwAuEcWS93oxDhyQ3JpCrVEje7cV+5F4fJwEpI/d+iIAjo5m+ntjJgU/c3mLDfWkVzyJ/txNA8gkDWp4TvRc/PBzh3uoqxIgH9usypNRMjV4aTeq4CtJ2q4paaw1EVKLWZsy54IpEYrWYnneHMWuMriyyMzKffiNKo0VC5IDAxG+CDH7o/qedsTiuVOK08fraFx8+2oCgKo7PLG+Knd5zLfRMEgmEUBb78nb/jL579Vezm5AX6bo4ug8FwKHfwvcBdLXiS5ZlnnuGZZ57hxRdf5Ctf+QovvPDCgc+JxWL82q/9Gp/5zGcSYgf2LlrOFfdiYVq+sFlobhY3fr8/KwXr54818xcXr6m2vc2IkoB2WUOsSFLdpi4bBPztNlw3DtdLJ7AcorrFy3B/dnvO7Eff5VFO/EIDVy8Mk2xJtQK0n6rm5vVx9ddzYwKnz86yP7uT1IdvzWKp0rEWzVwN0WF+jSJwImZhYGSak6eqqKrxJPW8vbodC4JAdamL6lIXv/KOY0iyTN/4PJd6J7jYM8Z//P9e5/MffzTl/X+7oysYDFJUVKTqjK67ibta8JSXlzM2Npb4e3x8nPLy8j0f/+STT/KpT30qqW3/1m/9Fo2Njfzu7/7ultvjtvQC9y6RSIRIJMLY2BihUCgr4mY3fvOBU3zn0rWMRTg0fhHJfDu1pbJN3V9vxN61ikY63OKNhvw6xDncVrreTl64KED76WpuXlNf7MBGTU1TcfYFTygU4bi9hM6FzIhRvVbDxFKa70lReMDkoefyxrnj/IdPJf3UZAuHNaJIS1UJLVUl/NP33k8kKh2qxt7tdlNWVoYoiknP6LoXL4bz62igMh0dHfT19TE0NER5eTnf+973ePHFF7c8pq+vj8bGRgB+/OMfJ/6/H3/wB3/AysoKf/Znf7bjvnjjwQL3BpFIZEtBcVzcRKNRjEYj5eXlOZtTZtJpM1a8DLdTWwsaIr6Y6jZ12SDgb7PivB441JX6aP8cokZAPqRwUouyKjddSaalFODY6WpuZEjsxBm8NYXZaSAYyq5ja6xrHqNPw3pMUn3bjeUebg6n97t/0FlCz+sbbR1KvXbOnKs74Bl3SLeOZj+nViqv7fF4aGho2OLoKi8vvyfFzW7c1YJHq9Xy3HPP8dhjjyFJEp/85Cdpb2/nC1/4AmfOnOH8+fM899xzvPrqq+h0Olwu15Z0Vk1NDX6/n0gkwksvvcQrr7yC3W7nD//wD2lpaeHUqQ3l/+lPf5qnnnoK2BA8hQjP3cle4sZms2G327dMnL9x4wZutzvpLqmZ4plf6OC3v5+Z4mUAMSZupLZckuo2dX+dEVtvEG04/RNiaC1M3TEfA92pNefLBL5qT9JiB7IjdgDWQ1Ha2su42p8ZYbwXq6vrnHBU8GYGojzp1i2ecXnofe1OD6sPfOh+NJrkBYwsyzkTF3Gxtd3R9cYbb+xwdN2r3NWCB+Dxxx/n8ccf33Lbl770pcT/v/71r+/53OHh4V1v329n0uv1RDOYly6QHTY3idwsbuIFxZvFzW7kS7H4qYrMFS/H0azcTm2pbFOXDQKBRjPOrjWENBu8AWjy4CCvAEoKVdjtWRI7ccb75tDpRKKx7Do6Z/oW0bkFoiruKya9lr6J1N1ZTVYro2/c+cwNRi2/+P7jKW0jl06p7WLroBld96L4uesFT7YpRHiOHocVN/lOJouXIe7a2khtqW1T99dvRHk0MqQ7pH2kdwa9QUskvPeYh0xT31rGQN8+zQQ3kW2xA+BfCdFyuorrA9mN8iwvB7mvqYKL8+pFeRp8bq4PpfY+qi1WAm8vIG0S1o++tx1riq0Nchnh2at+aDdHV2NjI1arNQerzC0FwaMyhRoe9dnL+ZAOu4kbvV6fKChWS9zkS4QHNoqX//LSdeQMrsc8oSAIImGfrKpNXTYIBJrMOHqDEJHSSpZFwjEamyvovXX4Zn3pIGgEAoHkLoIqGpxZFztx5seXEQXYY5pFxlgYWEbjBLXKrGIpRgM9RiPa/lVWt6VO3/dLqfdSy2W344Nee7Oj6/r165w8efKec3MVBI/KFCI8+UNc3MTrbsLhcEbEzW7kk+Ax6bQcKyvm2mRmHDFiWMG4BIYVgdki9aep++uN2HuDoNMgRNOr55EzUBibLM0nKum6frDYar2/kls3ciPKAOZn/DTfX0nXUHZt/AsLq5ysL+fKfHIRsP2wGvX0jiefzrJotZROy0wvbu1w3dLuZWFplL6+ELW1tUl3MFbz4ixVkhVbbrcbp9OJTpdfQ1WzQUHwqIxerycYzO404LuZuHA46CCyXdysr6/nZHZZvvLMw2czVrxsnpIREBBkcNzUsHxKUtWmLhsE/E1mHD1BFFFASCMEMdw7g9miJ7iW3eir0axnbPjgWU71rcU5FTtxQgtrOZlBtjoaQDArKIcseK8rK+LaYHIF6lpBoCVkZHhip8D7lY+d49wD9YyPj3PhwgWqqqqoqKg48PiRzxGezdyrx8GC4FEZo9GYlymtXF55HIbtkRJFUXa4pTZHbux2Oz6fD4PBkNP3m08RHshc8bI9okWzfieFYJoXCM6IRLyyqjb1RJRHK6aV2pJiMpW1xfSoNJYhWWpbyrh1bf/XrK914e+dQKMzIeUuEAXAxOgijSd89I1md8rWzLSfk2d9vD0/f6jtrEeSrNNSFDp0Lnp3+T2UlNo5+2A9oihSVVWFz+djcHCQzs5OGhsb8Xj2bkKYy6LlwmiJgykIHpXJxxqe+Dyt/ZpQ5SOKoqAoCnNzcwSDwbwVN7uRb4IH4Hx7E39x6bp6G5QVGNv5W3fdEJh1qTtNPR7lcfYEUfQahEjqymB1ObsjFIqKrfR27R9tKPPaMMsSQ+MrND3goWsy/VEIaiGs50Z1RSZDoEs/umQ3GehP0p31kL2Yns7RXe/7wIfu22JF12q1NDU1EQqF6O3tZWRkhObm5l2LfvOxaLnAHQqCR2V0Ol3eCZ58PPluR1GUHQXF4XCY9fV1AoEATqczb8XNUeE3HzzNd6/cJKbSMFnTvIJmlw4MoiTwHqGaV3UjqtrU41EeUQFFKyKkaKGeGlnC6jKz6s/OpG53mYuFW1N73m+zGbBEI3Rd20hl9b05iKe9nPmV3LnJAIZ6Z6hsKWVsOtnhF+owMb7E8Y4yri+kF12qLSvi6sDen3ecs04PPf9zd7FjMOxtRTeZTJw8eZLl5WVu3ryJ1WqloaEBg8GQeEyubekFwbM/hU9HZYxGY94VLeeb4FEUhfX1debm5hgcHOTq1atcvHiRnp4eAoEANpuN5uZmOjo6cDgcVFdXU1xcfKRqcPLtM4eN4uU2b3IzgQ5CDCsY9ihN+ZX3Hec//NoHaDMWE3NJKOn6ybcRj/IA6Iw6FDG134KiQGWSM5EOS3mth959xI5WK1JEDI1WRL7tKlJkBeZX0KT4vjKBXZujaPBcJO1pr6uhgy80T7iKGHpjbM/73/XeNmz2/QdwOp1Ozp49i9vt5tKlSwwODiLdzkXmuvFgKq99VI6lalIQPCpjMBjyrvFgPKWVCzaLm4GBAd5+++09xc3Jkyepq6vbIm7yUTgkQ76u+5mHzx5+I4qCZXr38tLzj7bx2x99EIBvvPuDaAWRmFO9FIm/3ogsQDQco7TKjkJqn/HacuYNBQog7Jc+VhQaSs0IikLf21sjDUtTAdqqnZldYBL03ZykxG3J+uuODi/QUlSU8vNcNhODU/tHhhrsduYuTiPvU/T+xC8nNxVdEAS8Xi8PPvgggiDQ2dnJ1NQUkiQVoix5TCGlpTL5ODw0WyffzWmpeFFxJBJJuKXsdjsVFRXo9fqkry4EQciZWLsbOVXhpchsYjGY/rBI/Qpod3n6+9/Rwqd/7eHE3y6zid9ve4g/6npNNZv65lqemYlValrcjHQf7IKKMzY4T5HXxuJ85mplGtp99Pfsbe0+1uDm1ms9VDWX7Xp/7+u9lB6vYSaDazwIRQGvzcLsQvbXYFxJfX+vKnaytM8A1DKzmdjNlX2bTx47WUFNXXFKryuKIrW1tZSXl9Pf38/c3BxlZbt/r/nGvRjhKQgelcnnomU1iUduNtfcbBY3DoeDioqKLfntdMjXSMlB5PO6H29r4DtpFi8LMQXT7M739b4HG/m9X3/njtv/l2Mn+C+DXQwULapmU99cyzM+GqCxvYy+mwfXbsTx+lwZEzwarcjyPlO6W+rd3Hqtm4YTlfRf2z21EotKWEKhnNjDN9N3cxJHqZWV1ezUPMUZ6p+j4VQx/csrST/Hv7b3Gu06Hc6xCLMHTIRPZSr6dvR6PW1tbXR3dzM/P08wGKSpqSnns/QKbKUgeFQmHwXPYU++2RI3u5HPwmE/8nndv/Xgaf7qyk2iaYhg05yCuO1pj3TU8S//t0f3fM6fvveDvO9v/kI1m/rmKE8sKrO2HsPrszM96U/q+UtzgUOvYS+aTlTuaUOvrnAweKEPQRTwL+7vGBvpmqT9kTZuDiUfvVKbWFSiudTB21kWPACOYPK/E4/DwtD00q736TUidX4dY9P7292LS2yce6ghpTXu+np6PXV1deh0Oq5evUpRUVHi7wK5pyB4VCZfU1rJRnj2EjdGozHj4mY38lk4HFU2ipeLuTqZ2rwhbVDBsO2i+6H7avj8b75318fH+yWtBgJ8xF7J92OjqtnUN0d5JkYXOX5fBYHAOmuBgy82pseWKKl0MTuVfAQhGSw2A8ODu59YPW4zS72TxKISrWdq6bo0dOD2hi/2U1TvY3El+4IjzmDXNCabnlA4u3WJ/d0z1NznYdh/sIit9DiYX94ZsRMUhftlG/2DB0f/PnD+vpSmou9FvGjZ7XbzwAMPMDExwZtvvklFRQWVlZWF+p4cUxA8KpOPjQdFUdxVNGwXN36/n2g0mjNxsxtHVfDk+7o/9fBpnv7+f0/+CYqCeXrr+zl7vIov/vNfBCAajW6p3QqFQlsGsP7+u97Dhf/xXxm3BlSxqcsGAX+jGWfvRhHyjavjHG/3cvPq5L5FqXHcHqvqgqeqycutqzujOyajFq1/lcWVIAaTjvGB5EY3rAcjlGsUFnOY2goFI7S1ebnan92hogDFUR3DSTxuwb9LIbqicEwy0L+PUy6OXq/lFx8/kfL6dmOzLV0QBCoqKvB6vQwNDdHZ2UlDQwPFxcX3ZP1MPlAQPCqTryktWZYJhUJbTkr5Jm52I9+Fw1HldIWPEouZ2bXkXEvGBdBs+lkfbyzhtz7Uxo0bNwgGg2i12kRheklJCSaTacdB/ZvvfoIP/PhFlDVFlWnq/gYj9r7bfXkUGJsK0Npaws2bB5+c5yZXVK2R8Xgd9NzYeXIVRfBZNAzd2HAQ1R+v5Nabg0lvd+DtEVrf2UZXEuMpMsXEwDw6jUg0xaGch6Xv5hRlx51M7fMb9bqsjM7u7Bd0zuGhfx/7+Wbe9Z5W7I79rejJspstXavV0tjYSGVlJX19fYnGhXa7XZXXBNI6Rt6LoqsgeFRGr9fnXPBsjtz4/X7m5+dZWFjAarVis9lwOp1UVVUdiUm5R1XwHIV1v6+lnr+8fHDxshhRMC7ceS81ZVZ++0Nt6HQ6amtrMZvNSR08y2w2nmo6xTfXLqsyTX17lGdpYQ1PsZWm5mJ6e/YfRLkw46e8zsPEqDpCwllsZ257IbSi0FLppLuzHwCT3UDf1ZGUtz11fRhrqZvVYG7aXawsBWk5XcX1gexGeRTAFZCY2icLVFZkZ3pbPdQpl5v+15ITOwAf/HByVvRk2K/5n9Fo5Pjx4/j9fnp6ejAajTQ2NmI0Gg/9ukd1dFC2KQgelcl240FFURKRm/i/zZEbp9OJJEkUFxdTlEZ/i1xzFITDbhyFdf/2g6f56ys3iSj7X7mbZxTivQNbaov5+rO/nPbB9Z+f6uC/j/YxFVhTxaa+OcoD0Nc9Q3ODm8pqF2MjuxeyxnE4zaoInqqGEvq6dkZ3qryGhNgBqKgvoe+t5E/EcQJLQZoby7iVI8EDsDCxjMDhh3umytToKu5mKwu7HVMVmFnaKnZaHE4mLiQ/hLX9eAV19SWHXeadJSUhPOx2O2fOnGFubo4rV65QUlJCTU1N0hPZd6PQZTk5CoJHZTLZePAgceNyuXaN3KysrBzZXjZHQTgcRRRFQYqs01zk5PrC3id9nV9Bdztw0VDlOZTYifPHj36Aj/3o+0gqBAy2R3kARicDuA3gcJlY2cciPjWycOiTuAKEIjv395oKG2ObCpNLyl0MHDBEdD963hyg/hdaGBjL7riHOHPTflruq6BreP/ImdrIskKzwcFCeGfdU0WxnfG5O0XNlVYLa1cXiEWTb3T5hIrRHUheeAiCQElJCR6Ph7GxMS5cuEBNTQ0+ny+t/asgeJKjIHhURqPRJNqMH4Y5r5WPAAAgAElEQVT9xI3dbt9T3OzGXkXLR4GjKnjyad3x31K8dsvv9yNJEhaLhY8cq+P6Pyyy6zlfUjDf7rlTW+7i+c9/WJWweZXDwa+2HOM7y9cRw4c/SG+P8oSCETRlxVjnlgjqNUT3GDS6vLBGVXMpIwPpn8SrmzyMDG4VIWWlNmZujG/5/h0eG7MT+0ecDmKlbxKD3UY4jcGparC+nJveQH03JilqtrIY2upWK7KaEoLHbTRgHAixuJZ8dN1TbOPBhxtVXWuqs7REUaS6uhqfz8fAwACdnZ00NTXhdrtTet2C4EmOguBRmXROCNvFjd/vJxaLYTKZ9o3cpLKmfDn5pspRXXuu1r29fmvzb8lut+N2u6mpqUn0BWkHnu+8xWx4Z2GoaV5BjEFlmZM//oNfUbVG4Pc6HuT18TGGBlYObVOXDQKBRjOOTVGekYE52trKsK4G6elf2PMkbTanX8em1Wnw+7d27rXbDEQm5wlvmutU2VRK39Xdh1WmwtKsn5aGUm6OZ66P0H6MDy9Qf7yMgbHsFlDHYjKtBjudmwWPAqMzG+swaTT45gQm5/fvbbSdx1Wyom8m3VlaOp2OlpYWgsEgPT09icJmiyW58R65HFp6lCgIniyTaXGzG7mcpXVYjqrggfScE6kSDoe3RG4290xK9rf0/vYGXrhybcttmnUFwxL4Sux88wsfOfSJIf77k2U58bl85eF38dT8j1lfOXzEYqXBiG1TlAegu2eGMruOtnYvt27tnj8b659DFIWkrOzbaTpRwc1NNnSdVoNDiTIxu7V3zH6CToYNK5dGBI0AooAiiigaEURh4z7xzu03u6aoaipjdCo3okcbyc1xZPDmNPZaA/7b6cPqUicjM8uIQM2SzGSKIkyn0/DYB3afin4YDhtpMZvN3H///SwtLXH9+nUcDgf19fUH7sPpCK17scj5nhA8L7/8Mp/97GeRJImnnnqKZ599dsv93/jGN3j++efRaDRYrVa++c1v0tbWxsLCAh/5yEe4ePEin/jEJ3juuecSz/n85z/Pt7/9bZaWllhd3XlloSgKsizvaOKXaXGzG0dZNBzVtWfiYBKJRLa0FVhfX0ev1yd63ZSXl6fVVuA3HjrFX16+TixemXy7547XbeM/ffGjKYuduLhRFAVJkrZ8FoIgoNFoEEWRlpJSPn7qOP/5H946tE19tyiPLMmsiSILb49Q21rB0MDOpoBrgXXK6lxMjqbWk8do0XOjb5qY7fa+K4Co0zAYjkKTbyNFKAiIWg29sgztlbcfJ2xNH6byO1EUlHCM4PQSGo0OScr+fjHYM0NlSwlj0+r2MDqIcDhGm9VL5+JGLY/LZmJkeolzRje9Y+Mpb++R97TicKg/9kEtt5TL5eLcuXNMTU1x8eJFysvLqaqq2lNMFVJayXHXCx5JknjmmWf4yU9+QkVFBR0dHZw/f562trbEYz7+8Y/z9NNPA/CjH/2Iz33uc7z88ssYjUa+/OUvc+PGDW7cuLFlu0888QSf/vSnaWzcyAHLskx/fz+XLl0iGAzyyCOPEAgE+NrXvkZ1dTVFRUVUV1fnxApeiPBkn8OuO97ILy5w4o384r1uvF5vYqL8YTHptZwsKeHy3EYUxLAMZSYL//lLH0WnPbhJYDxqs5+4if/bzjMdZ7k8OsVVFSzPu0V5lhdC+JqLmbg5gq3USWBlZ4Gx3WZhkoNP4IoCsk5AMeoICAKyWb9FsKwDmLbu3xKAePhGiwCCpCAgMIvIsXI7t1IUaWphz9GYhJFbs5grtASjMcZnV3jIUULPG6nb/CH5qeipoqbwEAQBn89HaWkpw8PDdHZ2UldXR2lp6Y79viB4kuOuFzxvvvkmDQ0N1NXVAfDkk0/ywx/+cIvg2dwAam1tLfFjslgsPPzww/T397OdBx54ANj4oT366KMsLS3R2NjI6dOn0el0fP/738fr9WbyrSXNUZ44flQFTypIkrQlcrO2toZGo0lEburr63dt5Kcmv/mOU1z+L3+LEFOoiJn5f7/8q+h1Ow8PcXGzOYojCMIOcQMk7Vb5P979Tv7ZxEusrR/O3bhblAdgcjxIRX0xa7N+DAYD4fDWFNpI7wy6PYqbFRQUrYhs0KIYdCAIKDEJxaDNbvGuoiDEbu/DBh1dvdOUVXqYmsv+NPP+m5MUV7uYW8ruaweDEY47SpjVRfGsa+h5LT2x03asnPrGUpVXt0Em+uFoNBrq6+upqKigr6+P0dFRmpubcTgciccUBE9y3PWCZ2JigsrKysTfFRUVXLhwYcfjnn/+eb761a8SiUT46U9/mvT2RVHkBz/4wZaq+r/6q7+itDQzO1Q6FARP9tlr3ZIksba2ligojgvseOSmuroas9mc9YPX6ZpyXIIey7rCn3/xYxj02oS4iQuczQdyURQTIif+d7rUFRXxe+96gK+8/PNDv4/dojwAy+sCggQuu8D0+lan0XooSsOJCvpubfRvUZRNIseo2/JYRZZBe7vmJosIMXlLJkxyWJADaznpjSPLCmVOS9YFD8BE9zz195dx6/WdF6HJorYVfTOZFB4Gg4Fjx44RCATo6elBr9fT2NiIyWQqCJ4kuesFT7I888wzPPPMM7z44ot85Stf4YUXXkj6ubtZCPOp82XBlp59BEHYEblZXV1FURSsVit2u53KykosFkteHKhkWeYzD96P1yCjFdnSS0oURbRabULkZGK9/+RkG5VOB//PP77JjankZk3txl5RnlX/OnWtlQy+1k3D6SoGBremg5SohCwKyAYNilG/USi8DUXZmGGh6LOc0pEVhO31OoLAVFimvaaIruHs9+bpvzGJvdSCfzV7TVYFFGp9Lnp+OphWkTlszFBT24q+mWwID5vNlmhc+NZbb+HxeLDb7Um/7lE8nqrFXS94ysvLGRu70910fHyc8vLyPR//5JNP8qlPfepQr6nT6YjFYnkzuqEQ4ck8iqIQDAYT4mZhYQFZlgkEAthsNsrKyrBarYmISC7ZzTEF8O6T9dy8eZOxsTFqamoSKapscabKx/Mffi//42Yv37p8k8nVvZsG7sdK/e5RnsHeWdre2UrXz7poOFtPX+/cRiTHqKVrzg+u/YtYhaiEbM3+rLnt0Z0EBh29vdMUlbhYXE7vs0qXaFSisdTJ1dXsjJsoK7FjVARupdGpejNnzpUhyxKQmf0wmxe6xcXFuN1uJiYm6OnpwWq1pvT6+XJBnk3uesHT0dFBX18fQ0NDlJeX873vfY8XX3xxy2P6+voSxcc//vGPE/9PF71eTzgczhvBU4jwqMv2Rn5x953ZbMZut1NcXIzZbEZRlC3p1FywWdzE01Lxg+LmYmJBEDAajZw9e5aRkRGuXLlCc3MzTqczI+uK9wtaWVnZ0i/IbDZzxuvmXU9+gJ+NzvCf3rjCTCC11Ils3D3KA9A7MI+lykPP+BJSsTXp1JQSiaHkQOwgKwj7RDOiNjMmKZqThoDD3dMYrTrWw7GDH5wmGlHgWIOXnhsTxGKHu2jT6jQ89oGTSbmeDkM2hYQoilRWViKKIuPj43R2dtLY2IjH49n3efei2IF7QPBotVqee+45HnvsMSRJ4pOf/CTt7e184Qtf4MyZM5w/f57nnnuOV199FZ1Oh8vl2pLOqqmpSfQ3eemll3jllVdoa2vjX/2rf8WLL75IMBikoqKCp556ii9+8YvAHcFjs9ly9K63ko+iIVlyHZ1SFGVHr5toNJpoLbC9kV+ccDickyGy+zmmkklLiaJIbW0tpaWldHd3YzKZaGho2PH+UiX+Gfr9flZWVgiHwxiNRhwOB0VFRdTU1Oy4QPgnbjePH2vir9+6yTdfu8xaCiNbNkd54r98WScSNWkJJeE820JMQjHnxpkk7hXdiSMIjK1FaK0toifLqa3gWoS21jKu9k9nZPtVPieEJG6+fbioTpxHHm2hqbmG+obKhOupsbGR4uJiVbafS+KjKrxeL729vYnGhVarNddLyyuEA06ER/MsmWPe//7387WvfQ2fz5frpQCwuLjIwsLCoSNXuWBqaopYLJa1SEk4HN5SdxMOhzEYDAnHlN1uTypyNz09TSQSoaqqKmNr3e6YihN3Te1nB08GRVGYmppiZGSE+vp6SkqSG7IYt9THozfBYDDRL8jhcGC32zEYDCldZQbCYT72f/4lM8bIrvU1u+G8GcI2FEI26mAXx1kyKJKMohVBl4NUpCSjiSYn9g1r6xjtFgJr2RXZTpeZJVEmpmJPIJ1OpK22lK5r42nX6uzG1/7kn9LQdMc5u76+Tm9vL9FoVDVx8Prrr/PQQw8dejupMj4+jiRJVFdXA7C8vJxIczU0NGzpzxU/56sxpT1P2fMAcddHeHJBPMKTLxRSWrsTjUa3RG7ijfziwsbn86V8Yo6j9ro3O6Y2bzcubNRyTG0m3gfE4/HQ29vL1NQUzc3NWw6Umwuz/X4/q6urCUu93W6npKQEs9l86BC6zWDgjz95nk9+5fuse0TCDg5M4fgbjJjmId1PQ1GUjUNnLsSOoiCmkMIJmw2UGQRSzP4dmuWlIC2nq7ihQh8lgLpKN6HlkGpRnTgtbb4tYgc2TvgnTpxgeXmZGzdu4HA4VIlm5oLtxdJOp5OzZ88yMzPD5cuX8Xq9VFdX50UNYS4pCJ4MYDAYcpLO2IujntJSY+2xWGxL5CYYDKLVahORm9LSUtUa+cHh1n2QHXxz3U02ior1ej3Hjh1jfn6ey5cvY7fb0Wg0BAIb4w02W+oz6TqrrfbQUePj8uAUhiUIuSFq32dsg0EgWK7FOpFejYkQk5AtOajb4XaTwVR+PoLA8PI6NV4Lo9M7a5cyyezI/KFriIwGLU2VHm5dS71rcjI88eFTe97ndDo5d+4ck5OTvPnmm1RWVlJZWXmk6lxkWd4h1ARBwOv1UlJSwsjICJ2dndTW1uL1eo/Ue1OTguDJAHq9Pq8Ez73WaVmSJFZXV7f0utFoNNhsNmw2G7W1tapEHdRgr8gNJF93kwnihdmbi4olScJmsxEOh4lGo7S1tW1pfpYN/vXvPMZHf/dbEAXrtEJsXma9WCRq2/27XK3UYp6IpR7liUSRrTkK+W9uMpgKei1TkysYjMasTlRfmg9S0eBgfH794AfvQlONh+WZQMbETpHbwi+8c/90viAIlJeXU1payuDgYNpTy3PFfsND43V55eXl9Pf3Mzo6Snt7+92c0tqTguDJAPkmeO7mCI8sywlxE+91E2/kZ7PZctbIb7d172UHh62N/LItbmCjnmFzUXEkEkm4zjweD3V1dVuuIP1+P93d3RQVFVFbW5u1ULnTbuKxs0387cVeALQxAcegRMQhEiwRiJm3Ch8pnShPNJazyA7ER0ikR9hkoM6hZ3g2uzZ1XUybcpTHYtZT63XSfWMygyuD9z9xH9okC9W1Wi1NTU2JqeXxrsZms/pzt9QkmeGher2etrY2VldX0WrvzVP/vfmuM4zBYGB9Pb2rnUxwt0R4ZFne0usmEAgkGvnFh2darda8aOQXT0dJkrTvjKlciJt47VI8ehMKhRKF2Q6Hg8rKygOHkNrtds6cOcPY2BgXL16kqamJoqKirKz/s7/xLn56qZ+wcnu8hQC6INiHFaJWhWCJgGS883mnEuVRpByMjdiygDSjO3EEgcH5ILU+ByOT/oMfrxLjwwvUHStjcDyJqeWKQmuDl+mRhYyLHa1Owy994ETKz4tPLV9YWODq1au43W7q6uryViik0vDQYrHkRXQ7F+Tnt3fEMRgMWzrV5pqjGOGJN/JbWlpieXmZpaUlJEnCYrEkhmc2NDTkTRHedju4RqNhcXExUbR7WMdUukiSlIjcxIuKtVptwi3l9XrTntMliiLV1dWUlJTQ3d3N9PQ0jY2NGS/61Ok0/PoHTvFnf3MJ2BjoKcobvYX0q6BbVQg7FEIlArJOSDrKoyjKhgMsVdu6iuzZZDAV9FpmZpbRarTEpOxd6OhjBx9jHDYjviIbfdcnsrAieMe7mnEVWdJ+vtvt5oEHHmB8fJwLFy5QU1ODz+fbdX/J5TG2MFoiOQqCJwPko0srnyM88SZ0mx1T8SZ0Wq0Wk8lES0tL3lxdJTNA0+PxoNVq6e7uxufzZaUIcnN6b2VlhUAggCiKiaLi2trajFzdmUwm7rvvPmZmZrh06VKij08m3++vfbiDH7xyjeVIBDQCQkRG0W28ngAYV8DgV1h3KYQ8woFRHgVAklDMuUtl7TpCIk2CWi11RSaGp7Jn2xronqa8qZiJ2V0iS4rCsaYyxvrn6JuZytqa1JiKLggClZWVeL1eBgYGuHDhAs3Nzbhcri2Py+U4oVQFTyHCU0A1Ci6tvYk38tvsmIpEIhiNRux2Oy6Xi+rq6kSUYGlpifn5+ZyJnf2Kig+ygxcVFdHR0cHg4CCXLl2ira0NiyX9q83NxCNgm4uKZVnGarXicDioqKjAZrNl7aov7ghxu90JC3tLSwsmkyljr/m7n3iEL/7pKxsTzHexNAkKmBbBsKyw7hYIebVYpneP8gi5LFK+zYFNBlNBEBicXaW8xMrUbPZEj8ugZ3vspshpwiDH6L6amaLkvahr8NDUUqba9nQ6HS0tLayurtLT04NWq93SpmG/wuFMU4jwJEdB8GSAfBQ8uYrwRCKRLZGbeCM/m82WODHvVy+SzR5CB4mbdOzgGo2GxsZG/H4/N27coKSkhOrq6pQOTpsjYPHoTTQaTaT3SkpKaGhoyIsImE6no729ncXFRa5evZrR6NY7zjVQ+f1OxpYDyHoRMabs2pRQlME8p6BIIooIwvZdIcdFygDIysY/NdFpWVleQxTU3/Re9N2cxF3tZGEpCIrCiWYfA93T+Nezn+J/7y+1ZGS7VquV06dPMzc3x5UrVygpKaG2tjapwuFMkYrYyqfB1tkm90fIu5B8TGllQzTEO+zGBU4oFEKn0yV63ZSVlaXcyC9T0an9HFNxQaOmY8put9PR0cHQ0BCXLl2itbV1z9EjkUhkS+RmfX09EQFzOp1UV1fnzZy2vYhHt4aGhrh48eK+7/cwfP6ZX+Tpr/wARAFBklH26cIsaAQiDh365Wiix40Sk1CMutwVKd9GjErqRXc2sSqI1JZZGJkMZGDrO5FlhQqXFa1Wg1XUcEvlBoLJYrMb6HigJqOvER/eOTo6SmdnJ1VVVUcmpXWvUhA8GSAfIzxqi4Z4I7+4wIk38ovbwUtKStIuht2MGtGpVAZoZvKgIYpiYkRDV1cXbrebysrKHT2DdDpdoqjY5/Op2hAxm2g0GhoaGlhdXaWrqwun00ldXZ2qheaNdSWcrPZydXQGJYmvTtGKROw69CtRTEYtFpeduYUstyfehiDJqTUZTGnjAsOzAdxOU8Ynquu0IjUlZhicxCuAbDHjsemZ94ezLigffLgavT7zHZNFUUwUMnd3dycisNnuT5XL6NJRoiB4MoDBYGBtLbcH0c0cVjRsbuQXCARYW1tLFMNmupFfOmLtsAM0M4UsywmBaDabmZiYYGRkBLfbTXFxMfX19XnTEFFNrFYrZ86cYXx8nIsXL9LY2KhqQ7fPf+YxnvwX395wa+2R1tqMoheJ2rXUl3voH5hTbR1pcVgbejIvodUQCUcyMlHdZtZR7tARm19h5PoofbukrpzFNqw+O47KMpaiClNLIdLvNHQwWq3IuYez2ylZr9fT0NCQmM9lMplobGw8sL2DWhQiPMlREDwZwGAwsLiYRD+KLJHKji/LMmtra1tszIIgJHrdVFZWZnR8wHaSaTy41wDNuGMqF3ZwRVFYW1tLpKY29wxyOBxUVVXR1tZGKBSiq6uLtbW1jDubcknc6VJcXEx3dzdTU1M0NTWpkppzuyy851Q9P3lrAEGSUMSDI0iyQUP3xELOD4Apj5BIE78C1eV2xg6b2lIUSotMeAwCgbE5xt6YoPuAC5KVuQArcwEmrm6UM5tsRspbyjGUOFlVRMaWQkgqDh99+JFmbLb0ZuAdBkVR0Ov1nDx5ktnZWS5duoTP50u5Zi8dCoInOXK9v9+V5FtKay/iJ+XtjfzixbA+nw+r1ZrTXjfbGw9me4BmMsTHMGxu5re9Z1BTU9Oun6PFYuH06dOJBn4tLS04nc6srT3bGI1G7rvvPmZnZ7l8+TI1NTWqzPb53G+/m398epBYkpPUAWSDiKQoaMI5cjBmIbqzmdG5AA6rHv9qascmQYCqYjNWOcZs7wQzVxc4zKjQUGCd/osDib91Bi1VzT7MXhfLksRsUCF6iMkYT3z4FFFpIScR3PhxqLS0FI/Hk5hh1dDQQHFxccZEWEHwJEdB8GQAvV6fV40H4Y6NebNjKn5Sjg/PrK+vzwunD2xNS0UiESKRSOJgkYsBmpsJh8NbiorD4TAmk2nPMQwHIQgCVVVVeDweurq6sFgsNDY25k1TxUxQUlKCy+Wiv78/YWE/TPt+vU7Lk4/dx7f/9gpiVAFNcicWyagBWUITzb7oUaXJYAooGg0KclKpLb1OoNpjRrMaZPzGKCNvZS5FHw3HGL42CtdGARBEAXeFE0eFG22Rk+lADH8oueNpY7OX5tYyrl+fz0mEZ/OxSKPRUFdXh8/no6+vLzGmIhPF+4U+PMmRH2e3uwyDwZBTl1bcxrzdMTUwMIDdbsftdlNTU5PxjrjJsj1ys9k2aTQasVgsdHV1HfqkmA7xMQzxf8FgEL1enygqrqioUG0In9ls5tSpU0xMTGSk1iXf0Ol0tLa2sry8zLVr1/B6vVRVVaVs2Y9HKTuO2fnBKyLhsISSpOABkEwigiJv1P9kC0W9JoOpsBJTqCyzMTG9uuM+u1lHuVNPZG6Z0Wsj9IXTmzJ/WBRZYXF0icXRpcRtJVUePHWlKFYzc6HYnoXQ529PRc+F9XqvwmGj0cjx48dZWVnh1q1b2Gw2GhoaVHVa5rIH0FGiIHgyQLYFTzgc3tHrxmQyYbPZcLlcVFVV8fbbb3P8+PGsrWkv0hmg2dbWlphpo1YKZDckSUp8hisrK4kp63a7HbvdTmlpqSrOs/0QBIGKiopEtGdmZiYr4xpyidPp5OzZswwPD3Pp0iWam5v3dLnE+xHFI2yRSCSROiwrK+Mzv/4I/+7PfpraAgSBmFlEuyYhZmnIuBDNbnRnM+MLa5iNGkKhGKVuEx69gH90lrG3J8je9K3UmBudZ250PvG3s9iOt6kMjcuWKIR2OC08/EgTkBvX0kGiw+FwcPbsWaamprh48SIVFRVUVlaqIlRSFXiFCE8B1cjkLK1IJLIjchMf/BgfoJktZ8BBbB69cNgBmm63m46ODnp6epidnaW1tfVQV0jx4uzNRcVAoiFiTU1NVouztxOvdZmenubSpUuJGoC7FVEUqauro7S0lK6ursSk+83dpIPBIAaDAYfDsWc/ovc94uIvXrrE1Fwg6bQWcFv0aNCtSTsbE6qMEI5uvEaumtRpRExGEcv4HDNXFw9Vj5MrVub8rMzdkWcmm5Ff/tcfRqffOKXlIuKRjMgSBAGfz0dJSQlDQ0N0dnbS1NSEx+PJ0irvbQqCJwOo1Xgw3sgvLnBCoRBarTYRcfB6vXnVo2U/O7gajimtVkt7eztzc3Ncvnw5cYI8iM31S/F/kiRhtVoTxdk2my3vamYEQaCsrIyioiK6u7uZmZmhubn5roz2xOeAraysYDQamZ2dZXx8nKKiIrxeb0rRtWeffi+/96X/SsoxFFEgGhc9Gcw2aecCKHotij276dnNzK/LmJT8OG6oQSiwzkPvP5n4O1cprWSPbVqtlsbGRioqKujp6UnU96g1eqbA7hQETwZIx6W1OZ0S73UTT6fYbDbq6uryqkdLMgM04zu/2ldaxcXFOJ1Ouru7mZ2dpaWlJSEC4rO6tqc9zGYzdrud4uLilIuKc43BYODkyZOJ4ZzJCr18Je5qi38/KysryLKcGHIat+xHo1F6enqYm5ujqKgo6d9+W3MZx5rKuDaYRuxCsxHp0a5lpvOxuBZGE4ygrEeJWY2Qw7qLaGUpyvQSQhYnqmeK9oea8NXf2SdyldJK9TXjg3eXlpa4fv06Lpcrr8wjdxuFTzUDHCR4JEna0utmbW0NQRASjfyqq6sxm82qC4V0r3oOM0AzU+h0Oo4fP87ExASdnZ04nU5isRjr6+tb0h5VVVV5k+I7LKWlpbhcLnp7e5menqalpeVIvLfNozJWVlYIh8NJCVCDwcCJEycSM4uqqqrw+XzJRXmeeR//7Pf/kmgaHcYV7e2anqDKdTaKgnZuI3X6/7N35uFNlXn7v9N9T1q6p/uSJi2LLGEplWH0FQUFxQUBR1msjAgKDi44dRB1RFxQWVRgUMQFuHx1XOanUwWXUQcolB2SNGm672uSpk2znHN+f/A+x6Rr2mZt87kurpna0+QkJznP9/ku982hGXipu0CHh9jyGYaECRxw0uIBhWNNPe3BnCUzLH52VklruM8ZHh6OGTNmoLa2FkVFRUhKSkJCQoLLbHBHC2Mi4CksLMSGDRtAURTy8/OxefNmi9/v3bsXb7/9Nry9vRESEoL9+/ezjbJ33303zpw5g5UrV2LPnj3s35w9exYrV66ETqfDggULsHPnTvbDaR7wUBRlMQ6u1WpZAToy5RMSEmL3Lyfx0xrsC2QPA01bQewsyOJJbBgiIyOh0WgQEBCAadOmuVX2Zqj4+flh/PjxbBBgzybu4UBRlEV2rbOzE35+fggLCwOXywWfzx/yVFtUVBTCw8OhVCpx7tw5CIXCQVP/UZGheOyBOdh/9AQ69EPvp2N8vUAFAj4622U/vDU6eBl+n3zy6ugGHRoI+DivlMqM4wItanDaHeO1ZQ/8An0RmOwFpVKJ1NRUq+91tmakQRYZWIiNjYVSqcSpU6eQlZWFiIgIG57l7881FuEMItvvJEUu20FRFAQCAY4dO4aEhASIxWIcOXIE2dnZ7DEajQZhYWEAgK+//hrvvPMOCgsL0dnZibu+puIAACAASURBVPPnz+PKlSu4cuWKRcAzffp07Nq1CzNmzMCCBQvw2GOPYd68eZDL5SgsLMRHH30Eo9GIwMBA7N69m03XO0vI7+zZs7juuussnnswA82+JqYchbkNAwkWiZ0FGQkPDg5mv7gMw6ChoQEVFRUQCASjepybYDQaoVAooNfrIRKJbDYeby0Mw7B9N+QakUxlX9fIFqjVashkMkRFRSElJcWqz+VXhZfw4RdnoOkeuhioVzcNH70Ngh6agX9Fcy+hQSrID3Rk2MgffyQYjOBcUoJjctCImo25YflsbHh3NSorK1FfX4/MzEwolUpMmzbNoaWh2tpaGI1GpKSk2OTxOjs7UVJSAi8vL2RlZSEwMLDfY0+cOIHc3FyrHpcoQo/iMfZ+bzijPsNz+vRpZGRkIC0tDQCwdOlSfPXVVxYBDwl2ALDlJeCaCm5eXh5KS0stHrO+vh4ajQYzZ84EAEyYMAEPPvggYmJikJWVxe5AP/74Y7tE58OBw+HAZDKxfTfOMtDsC3MtFfLPvKcjISEBoaGhA54XafANDw+HRCJBc3PzqBfv8/X1ZTORFy5cQEJCAvh8vl12b0TbiWRv1Go1K1zJ5XId1vjN5XIhFotRWVmJM2fOICsra1Bl6ttvmYjbb5mIb45dwcHPT0Ots36ggA74PzVmw8j2ft7tnX2qKnt16kGHGQEHGF32i58vmJQ4cErds7T1P/fnwcvLC6mpqYiLi0NJSQm0Wi26u7sREuK4kqGt1Y6Dg4MxZcoUtLS04MKFC4iMjERqamqvIG6oXoO2NpJ2J0Z9wFNbW4vExET254SEBBQVFfU67u2338Ybb7wBg8GAH38cWMejtrYWCQkJ7M95eXm4ePEivvvuOwDXdqHHjx93arDTc2LKx8cHjY2NiIuLc6qBZs+FU6PRwGg0sloqMTExyMjIGPbOLCAgAJMnT2bF+0a7VQNwbWSfy+WitLQU58+fh0gkGnA3aA1EcJEEN93d3QgICACXy0VERARSUlJsKpw2FMjiRkbYg4KCkJmZOehn5tabxuPWm8bj2x+u4uBnRVB1WRf4UAFe4NAjECY00fBp71upmMPhwLu9E1SMkz+jkVwwbWpw2tyrtBWTEoWcXAH7c0BAACZNmoRffvkFly9fxrhx45CWluaQTI+9ymiRkZGIiIhATU0NioqK2MDOPLs9VktUQ2XUBzzWsm7dOqxbtw6HDx/G3//+dxw6dMjqvw0PD7foG3G0l5Y1Bpo5OTlQKBSQy+UQiUQOy3z01bBq74WT1MLHjRvHKpump6eP6myPj48PhEIh2tvbcfHiRcTHxyMx0TrHaIqiLEpTWq0W3t7ebFkqLi7OpeQPCESZmgi5paenIzo6etC/W3BjDhbcmIPvf5biwKen0N7ZPfAfjFCY0F/dCQ7df7DkpTeB1unBBDq3AZ1JiQPTroWXG2UAbrxvdp+bNh8fH8yYMQPV1dUoKipCenq63c15aZq2W/+gl5cXkpKS2P6e6upqNrvp8dGynlEf8PD5fFRXV7M/19TUgM/n93v80qVLsXbt2kEfs6bm9/Rvz8f09fW1W8AzEgPNnJwc1sXXHn0uJpPJInNDbBiIbhARRXTUwhkYGIgpU6aguroaxcXFEAqF/Sr4jhbCw8MhFouhVCpx9uxZiEQiiwZfoklk7gXGMAxbPkxOTnaq4OJQIUJukZGRkMvlqK+vR1ZWllX9TPPmijBvrgg//FqCfxw9idaOrv7FADkcmIK94aOl4DWElp74OC52fvIQdj35Kc78XNLvcV7tnaAC/BwuRsgxmRDLC0BmWhR+2f/9tcDMCrd5V4DD4eDG5bP7/b2XlxeSk5MRFxcHuVyO6upqCIVCu3hZAY7JtPj5+UEkEkGr1UImk8HPz49t1PYwOKM+4BGLxVAoFCgvLwefz8fRo0dx+PBhi2MUCgUyMzMBAN988w37//sjLi4OYWFhOHXqFGbMmIEPP/wQjz76KPt7W334zIObnroSw+27iY6OBpfLxdWrV9HS0oKMjIxhZT7MdYPMswKkYTU6OtoldIOIMSfJ9kRERIz6G4S3tzcEAgHUajUuXbqE0NBQ+Pn5oaOjw8KKISYmZtT0OZHpNdLPxOfzrR7rvfH6LNx4fRb+c0KBvYdPoEXT2Xfg8X9Bj6/WemHCFffnIig4AJvfeQDKKzV4Zf3HaG3qXTbyMtGgtd1gQkdWihwUmkaIFyAUROPG265D7k3jweFw0N6oxq/vFgIMA4bj5fTvrTVM/IMQ0UmDKxSTzwbxsgoLC0NGRobNszGOzLSEhIRg6tSpaG5uxsWLF0HTNCiKsvq77A7X1x6M+iktAPj222+xceNGUBSF1atXo6CgAFu2bMG0adOwaNEibNiwAcePH4evry/Cw8OxZ88e5OTkAABSUlJY8Toej4fvv/8e2dnZKC4uZsfS58+fj927d1vUVKdMmYJffvnF6nMcyEDTvLHYVn03DMOguroa9fX1yM7OHnDXQ2wYSPbG3IaBZG8cMVo/UhiGQUVFBZqbmyESiey203MW/WXYaJqG0WiEUCh0mSZ6e0JRFJRKJdRqNUQi0ZAbV38tUmLvx7+hSd134MOhGPhoBxcmFGbF4vXtd1ssLgzD4NM9x/D5vl9A9ShzMV4cmOIjAC/bLkbeJhMSooIxMy8TC+6difDIvj/3T8x9AYpiJeDlDY6Lf5cBYOO+B3HDstl9Lt79TS0xDIPa2lpUVlYiOTnZpk3+CoWC3ew5Eq1WiwsXLoDD4VhVuqNp2iVL1Dak3xc2JgIeR8MwDCZPnoxff/21z98PZqAJwGHj4FqtFhKJBDExMUhKSgIACxVcYsNAsgJcLtclbRiGAnnNUVFRSE5OdvlArS/MrRjMx/bJNQoLC7PIsHV0dEAqlSIyMtLqcW53R6PRQCaTsVm9oX5mTxaX4e0Pf0OjStsr8OGYmEHVmF97+S4IMqMtmr91Oh0CAgLg6+2PT17+CfILtRaPTYUFguaN0F6AohDizWDSxATMv1OMiTMzrPqzkuIyPDX3eYDDAcfbtZP/QaGBeE/6KvyD/C02g4TBxrSNRiNKS0uh0WhsVuqWy+UIDw93uOddZ2cnFAoFRCIRFAoFdDodsrKyLKaPzfEEPP3jCXiGAQl4fvnlFzag6c9jypnj4MA192m1Wo3Kykp0dnbC398fQUFB7KIZFhY2KoX8aJpGeXk52trakJ2d7dIeNj2tGMy9wMh1GmxsH7j2misrK9Hc3AyhUNjvDXE0QdM0m8nMyspCeHj4kB+j6FwF3v7wV9S3dVgEJxwj3a8a8/jsKNx5eyo4HA4bhHK53F5+YBf/q8Cbm46iQ3OtcZrh4FqWx3to9wJfkwkpfC7y/ijEvLvEaFO1oLa2Funp6UNagPNFm9Bc3QJ4+7j0gnjTijlYt/MBi2y4eR+jtbo0HR0dkMlkCAwMhEAgGNEABdGHcrQGWEdHB8rLyzFx4kQAvwf6wcHByMzM7PWaPAFP/3gCnmGSlpaGEydOsB828y/kSAw0RwIZNSY7TnOn9bCwMDAMg7KyMqSmpiI2Ntbh5+cMNBoNpFIp4uLirJ5qsjd9TbYFBgbaLAjVarWQSqXg8XhIS0tz62ydteh0OshkMvj7+yMzM3NY71/xxUrs+eBX1LZq2MDHS0/Dp9uyi9nLi4NXXlqILEGCVe8twzA4tP0b/L+PT4JhACrYD/S4QYJRE4XIYF9cNzkRC+6ZjoychF6H6PV6yOVy0DQNgUBglVTB8Y9+xe61BwCOFzgu/LnY/t1mCGf8nrkigQ+5t548eXJIQnyNjY1QKpVISEhAYmLisO7PUqkUMTExDi8bq9VqVFdXY/z48ex/M39NfD4fSUlJ7GvyBDz94wl4hgHDMFi2bBmqqqqwd+9epKWlOTzAIU3F5qPGxGmdLJx9uU8bjUbIZDJwOJxR68zdE4qiUFZWBo1Gg+zs7BFr2Az1ucl1UqvVrF0GuUZcLtcuk20Mw6Cqqgr19fVjQqsI+H0RKC8vZ3V8rH1fza9T8YVKfPadHC1aA8DhwLubgrf+91vlrfMn4JE/zx3y+bU3abDt4UNQSuthig8HfM3KSgyDAIZGRkoE/jBvAv648Dr4+Vv33WxtbYVCoUBsbKzFwtcfS+Mehq5D57JZHn5mLPacfrHXuTEMw9rPlJSUYPbsvvt7+sNkMqG8vBwtLS3DsnS4evUq+Hy+w79L7e3tbC9mTyiKQkVFBRobG5GZmYnIyEgwDOMJePrBE/CMgBMnTuCRRx7B+vXrsWzZMrt9wEg/B8kIaLVaVuKfZASGOmpcX1+PiooKCIXCYZUB3BGVSgWZTGY3xWKiKE2CG9L8Ta4Rl8u1uRXDYHR1dUEqlSIkJGTYE3vuhtFohFwuh8FggFAo7BXgmo/uk+vEMIxFaSooKAiXpXV4672fUd2shreOhreRQWCAL/6x9wGE84KGfX4n/n0J+3YUosPXFzFh/pgyJRm3Lp2B+OTBJ5L6g6ZptmF/MHXq9/96FF/t+rfLNi/f/9yduOvxBayIqUqlYjd2RGIhKioKkZGRFmUuayHfCR8fH6slDgDgypUrSExMdLj0RWtrK1um7o/u7m72M5+VlYXIyOF/ltwAT8DjLFQqFR555BEAwBtvvDHivom+dFRommbNSLlcrs38unQ6HTvGmZ6ePiYaXSmKgkKhQFdXF7Kzs4ftT8UwDPR6PbtoEkVp8+vkKs3fZHKlpqYGAoFgTExyAUBbWxvkcjmio6MRGhray82dBDdhYWEDXqersjrsfP8/qKtow5/uno6lS8QjPjfS72fr71xXV5dFaa+vnhWD3oh7Y/4M2kSD40AvKmvgeHGw5d/rwfG/1tdGSr3kOpkrKpv395AhkKHQ3NzMZsasafS/dOkSUlNTHT792dzcjPb2dggEgkGPbW9vB0VRA2rRjQI8AY8zYRgGhw4dwltvvYWdO3dCLLbuhmhuw0D+GQyGXk3F9pRNZxgGlZWVaGpqQnZ2tkO9aZxJW1sbSkpKWOGywW6W5lYMGo2GncYxLyE6y4rBWnQ6HaRSKQIDA62yanBHiCktCUS1Wi3rMZecnIyYmJhhB7nllS2Ii+EiIMC1y8AMw6CpqQllZWVISkpCfHx8r8/335e8hTPfnne5spZgZgoeP/ggeDyeVWUZc4HWvqa5BoNkxhoaGpCZmTlgA/iFCxeQmZnp8AGIpqYmaDQaZGQMPo1H3gtHGw07GE/A4wrI5XKsXLkS8+fPx8aNG3vtGg0Gg8WiSfyLzEsezlo0Ozo6IJFIEB8fb7Wgm7tjMpnYNLBIJIK//zXpf/NF01x00bzk4a41coZhUF9fj8rKSrbm7670nG5Tq9VgGIadbiPZUA6Hw07rjKVGbpPJBKVSiY6ODmRlZVlkJpprWpEv/IvLNS8/cfDPyFs89AzaSAOf7u5uyGQy0DQNoVCIoKDeJcvz588jKyurz9/Zk4aGBnR1dbEG2QPhCXg8AY9DMRgMKCgowOnTp7FixQpIpVJ0dXXhzjvvtGhWDQsLc7lFk6IolJaWsuUeEgCMZki5p6ysDCEhITCZTKyTO1k03cmKwVr0ej2kUil8fX0hEAjconm9L8PTgUoePWEYBjU1NaitrUVmZqbDx4udBQn2uFyuhdHmxtwtKL9U6TJZnhBeEN6X7YDfCDJoIy1ztba2Qi6XIzIysldgfPbsWeTk5Dg8mKirq4Ner0dqauqgx3oCHk/A4xCkUil++uknnDlzBpcvX4ZOp4OXlxfmzp2L5cuXY+LEiS5xU7EG8qW31qzRnSB9N2ThJCXEkJAQqFQq+Pj4IDs72y0CgJFiPtXkate6L+FFW2XZyG7ex8dnxNos7gIJ7Kurq1ntnsu/yvC3Bdtdpnl5wUN/xJrX7hvx49iizFVdXY2amhoLZePi4mJMmDDB4RvBmpoaUBSF5OTkQY/1BDyegMchfPrpp2hra4NYLMaECRPg5+eHpqYmrFq1ComJiXjppZccOg49UgwGg0UGwB37PYgVAwluiBVDz5FwcxobG1FWVoaMjAyHK6o6C4PBgJKSEjAMA6FQ6PAAoGcDuFqtthBeJKUpW2bZzPtcUlJSEBsb6zYbkpFgMBggl8tZK5K1E59Be73KJZqXX//pWWRMTrHZ44008NHr9VAoFOju7oZQKIREIsHkyZMdvhmqqqoCh8NBYmLioMeS7NYoz857Ah5XhaZp7Nq1Cx9//DH27t3bp5aCq0L6Paqqqlxey4X4gZmPGhMrBhLcWGt2SoI9kgEYC9ke4FpzpFKpHLKGzVAx9wQzt2Mwz9446j0nFgQ6na7f3o3RCJlgK/mhEv98+Vunl7WSsvnY+d+tdjmHkZa5iJyFTqdDbm6uw4OJiooK+Pr6WjV55Ql4PAGPS3D+/Hnk5+fjgQcewIMPPuhWPSFdXV24evWqyziRk2ZV8wbw4VgxDPYcDQ0NqKiogEAgGDP9HkTDhmQARpoa70ubaDA7BmfQ3t4OuVzOes45+zPuCMiE0lPTt8GoNznVX2vV35fg9vXz7Pb4I832MAyDX375Bd7e3khNTe1z8s1elJeXIyAgAHFxcYMe6wl4PAGPy9DZ2YmNGzeiqakJe/bscatFlNwcW1tbkZOT49CdcF/Tbba0YhiI7u5ui1HusTDdA/zex9XfWHN/9CxNmUwmBAcHWwSirvoeEv+11tZWZGVlOVxgztEQn733njqCos+dN6Lu7eON96SvgRdlf++3kQQ+J06cwLRp06BUKqHRaCASiRziV6dUKhESEoKYmJhBj/UEPJ6Ax6VgGAaff/45XnjhBbz66quYM2eOs09pSKjVakilUrupFfdlmeEIK4aBYBgGdXV1qK6uHrZBpTtiMplYd2aRSNSrB60v2wzSI0X+uWNDsFarhUwmQ2hoKNLT092yf60nFEVZlBG7uroQEBBw7fvkE4B14wvAAOB4OT4YnT7/Ovz1yHqHPifDMKBpGl5eXlaXucwNSzs6OiCVSvs18LQlCoUCPB7Pqp5C4jfmjt+7IeAJeNyNqqoqPPDAA5g+fToKCgrcqk+EoijI5XLo9XpkZ2cP+8tlXu4gzcUAeo2EO7vcQSDK1GQhdNVMha0hIo1RUVEIDAxkp6bM7RiIvYmrXKuRYj7V5G56Reb6ROaWDD2tM8yv1XO3v44LP1wGx8fx96HNH6/DzNsmO/x5h5rt6enQTsreZWVlSExMtJs5cUlJCcaNG2fVZ5CmaXh7e3sCnn7wBDxOxGQy4aWXXsKxY8ewf/9+pKSkOPuUhkRzczNKS0utWhDMJ3HITtNoNLLlDleyYhgIhmFQXV2Nuro6iESiUVv26MvRnaZpMAzDjrC7+rWyBXq9HiUlJQCArKwslywV9NUEbq5PxOVyB71WdWWNWDvxKcDbGxyO4/qXuJGheE/6Gnx8nZdFszbw6RnwEEwmE8rKytDa2moXb8KhuLTTNA0fHx+32kAPg34DHvfPxY5ifHx88Nxzz+GGG27AsmXLsHHjRixZssRtdslRUVEICwuDRCJBS0uLRY8LEYkjN2KdTgd/f39wuVzweDwkJye75S6Ew+EgKSkJ48aNg0QiQXh4ONLS0ty6ybUvOwZSRuRyueDz+WzzMplYMRgMSEpKcpvP6nDx9/fHxIkT0dzcjHPnzg25p8nWmHvtqVQqiyZwHo+H2NjYYekTxafFIFHER7WsDvB23Gd5zj0znBrsANe+0xwOhy1zMQwzpGkuMs3Z2dkJmUzGSnnYSguHlN48DI4nw+MmtLe34+GHH4avry927NjhcIO6kUBRFMrKylBfX4/Q0FDo9XoLkbiwsDCXmMSxNeY+ZCKRyC2uWV92DD2VpYkdQ3/QNI2ysjK0t7dDJBKNGf81k8mE0tJSdHZ2QigUOsRTiWwciGO4Xq+3W1b07PeX8MKdOxzavPzWb88hZfzg+jKOhGR8emZ7+svw9KSpqQkKhQLx8fFITk4ecbBy+fJlJCcnW9UgPdYzPJ6Ax42gaRoHDx7Enj17sHv3bkyZMsXZp9QLssM0n5oiC2ZAQACampoQExOD1NTUURfg9IdWq4VEIkFUVJRNbnC2ZKR2DAOh0Wggk8lc8nXbE7Vazb5ua1y2rYVhGFZdmozwe3t7W5Sm7K2ge3/qo9C0ah3SvJw2KQlv/GeL3Z9nOPRV5jp58qRVAQ9wbRNYUVGBxsZGCASCEfWAXbp0CWlpaVZtLDwBjyfgcTtkMhlWrVqFhQsX4tFHH3Vqr4Rer7cIbvR6/YBu7mT3r1KpkJOT41bq0iOBjDS3tbUhOzvb4Y7K5BzsZccw0HNWVFSgpaXFbbJctoCmaTa7l5WVNSxRTtInRf711dPm6CDy8ze/wYd/+9QhWZ6HXl2OW9fcYNfnGCnmgc/p06cxe/bsIf29TqezUDEfzv3wwoULEAgEVkmBeAIeT8Djluj1ejzzzDO4fPky9u3bh9jYWLs/p8lkshgJNx8zNjc8tQbS65GcnDxmZPuBa1kPqVSK2NhYu/a4OMOOYSC0Wi2kUumo6GkaCl1dXex4ckZGRr/ZMvNglPRJ+fj4sNeKx+O5RE8bwzC4J+ohGA20Xf21fPx88L7sdYRFuGY51Gg0sn1SxHMvKioKGRkZQxYtBK7pWpWUlCA6OhqpqalD2sSeO3cO2dnZVt17PQGPJ+BxWxiGQWFhIZ5++mls3boVN998s80WUFtaMfSHyWRCSUkJKIqCSCQa7V9CFpqmoVQqoVarkZ2dbRORRvNJHI1Gw+qoOMOOoT9IT1NjYyOEQuGonWDrCbFgqaysZCfYzINRlUrFBqM8Ho+VW3DVoHDX2vfww8e/2lV5Off2qXjq0Fq7Pf5QMJfHII3g3t7e7LUi2l+2MCWtqqpCbW0tMjIyEB0dbdXfD8W0lKZp+Pr6jgrtqAHwBDy2prCwEBs2bABFUcjPz8fmzZstfv/BBx/gySefZP1N1q9fj/z8fLucS0NDA1atWoW0tDS8+OKLQ67jMwzDqqr2tGIgC6Y90+fEkDMrK8uq0crRAslyDVWk0V3sGPqjs7MTUqkUYWFhY0KviEy5tba2ora2FiaTCSEhIYiIiHCZYHQodKq7cF/iI2A4Q/edspZnP30M0+ZNtMtjD4bJZLLI3pBGcBLgDHYv7OnNBWDIpqREx0woFA7am3P69GmrTUs9AY8n4BkyFEVBIBDg2LFjSEhIgFgsxpEjRyyMPz/44AMUFxdjz549Djknmqbx5ptv4ujRo9i7dy9EIlG/x/ZlxUBUVUl5ytE34O7ubkgkEoSEhLBp4bEARVFQKBTo6urqNy1tng3QaDQwGAy9fMHcLWhgGAY1NTWora0dVerUPTcParUaDMNYTLnpdDoolUrw+XwkJCS4bGA6EAXzt+PKf0vs0rwcHsvFgSuvwtvH/p/p/sb4SRlxJI3g/U1zWQvZEIWHhw+o6H3q1CmIxWKr7gE0TcPPz8/t7hdDxKPDY0tOnz6NjIwMpKWlAQCWLl2Kr776yqlO515eXti0aRPmzp2Lhx56CKtXr8bKlSuh0+lw9epVxMbGWmiokGxAfHy8zRtVh0NAQAAmT56M6upqnDlzBtnZ2WOiwdXb2xtCoRBtbW24cOECEhISEBwczAak5n1S7qxP1BMOh4PExERERkZCKpWisbFxwB4XV2UgS4bIyMg+FyqS3VEqlTh79qxVu3hX4887/oRHZzzLZjJsydx7Z9kt2KEoyqKU2N3dzQ5ZxMfH23TzQN4XmqZZrZyhBD48Hg8zZsxATU0NioqKkJqairi4uF5/79HhsR73uru4CLW1tUhM/F0bIiEhAUVFRb2O+/zzz/HLL79AIBDgzTfftPgbe0DTNAIDA7FmzRrs3LkTr776KoKCgjB16lRs3boVKSkpLt0bQET7IiIiIJFIWGdqZwdj9sJ8d6lWq8HhcFBaWgpvb28kJSUhLS1tVNkx9EVgYCAmT56Muro6nDlzxqWd5/uyZAB+tzrJzMy0upTo7e0NgUAAjUYDiUSCcePGITU11WW/mz1Jyk5AfHoM6sqaARt/Pm+8b2iTTv1Bsm2kNEWuF9nsCYVCu2/2+hItJEGPNc9LNgYxMTEoLS1FTU0NhEJhL82d0XyPsCWegMdOLFy4EMuWLYO/vz/27duHFStW4Mcff7Tb8+3cuRMHDx6EUCjE9OnTsX//fpSVlWHHjh1YsWIF20vkDoSEhGDatGkoLS3F+fPnrZ5AcHX6smMgu8vY2FgIBAJ4e3ujpaUFCoUCAQEBbrfzHw4cDgd8Ph/jxo2DTCZDQ0MDBAKB0/tazBvBe2YDYmJiLJTDh0tYWBimTZuG6upqnD592q3Ke/e/sASv3LcbgO2CtCxxGhIEccP6W2JWSwIckm3j8XiIjo62yfUaLuaBD0VRQ872+Pn5ITs7m9W2IqX/0ZDtdSSeHp5hcPLkSWzduhXfffcdAODll18GADzzzDN9Hk9RFCIiIqBWq+12TiaTqc9yQEVFBVasWIHZs2fj6aefdvoiMlSIMWVaWhpiYmKcfTpW09OOobOz02LMeLARfqPRiJKSEtA0DaFQOGZubMRwsaKiAhkZGVY5QNvqebu6utjF0ryXw14aRT3R6XSQSqUICAhAZmamW3xX/5S0Dh3qbpu9L/e/uBh3PXqrVceSXilyzUivFOm9GekUqb0Y6TQXmforLy9HUlISqqqqrNb/Ges9PJ6AZxiYTCYIBAL88MMP4PP5EIvFOHz4MHJycthj6uvrERd3bafyxRdf4JVXXsGpU6ecdr4vvPACfv75Z+zbtw/JyclOOY/hYjQaIZPJwOFwIBQKXa7PwxZ2DP3R1NQEpVLp0MXfFTAYDOw1z8rKsnnAR3RUyD97WjIMBfOALy0tzerRZGdxZNuXOPryV+DY4L3yDfDFE5+vQmBoQC+vKfMNhEqlQmdnJ/z9/S1Gw13tvjAY5tNcHA5nyIGPyWSCUqlEVVUV4BttHQAAIABJREFUxGKxVeKWNE3D39/fbUqnw8QT8Niab7/9Fhs3bgRFUVi9ejUKCgqwZcsWTJs2DYsWLcIzzzyDr7/+Gj4+PoiIiMC7774LoVDo1HP+z3/+g0cffRRPPPEE7rrrLpe+kfYF0TJxdtrfnnYMfWEwGCCVSlkTQnfY+dsKEvCNZPF3BUuGoWI0GiGXy2EwGCASiVzu/AgURWFZ3Fro9dSI7ydz7pmBv/zjIbS0tEAul7Pj+kQ00zx7M5p620Y6zfXrr78iMDAQ/v7+EAgEA+rxMAwDPz8/T8DTD56AZ5TR1taGNWvWIDg4GK+99prb9YjodDpIJBJwuVyHKPY6w46hL8x3/q7c2GsPSHmPoigIhcJBBdb6s2Qgi6UjFaZHSltbG+RyOeLj45GYmOiSi/zrq97Fr5+fGbHy8rr99yNWFMFOkjIMA4PBAKFQOOo/7yMpc504cQKzZs1CU1MTSktLwefzkZSU1Odn3JPh8QQ8Yw6apnHgwAHs3bsXu3fvxuTJk519SkPC3IU8JyfHpr5UPTVUnGnH0N/5mfd5uFsafySQZu7k5GR2PJcEpKSPo7OzE76+vhbZG3fvf6IoCuXl5Whvb4dQKHQ5uQZ1swarsx4HRQ8/GONGh+KF7zciYlyERfm3s7MTMpkMQUFByMjIGPXZzeGUucxd2slnhXi49QwUPQGPJ+AZs0gkEqxatQp33nkn1q1b53ZfAuJLFR8fPywBN3ewY+gLhmFQV1eH6upqp5f3HA1ZAHU6HZsFsEWvlDvQ0dEBmUwGHo+HtLQ0l2o8ffp//g7ZmfJhv/f3PHEb7nv2jj5/Z57dTElJGRPeez3Vmvt7vQzD9OnSrtPpLHrgiCmpJ+DxBDxjmu7ubjz99NOQyWTYu3evW01CAb8rFet0OmRnZ/db7nB3O4a+IOW90NDQUWnRQNO0Ra8UaVTlcrnw8vJCQ0MDEhMTh2TL4e4wDIPq6mrU1dUhMzPTZUo9pecr8MTcFwDO8BbSd89vQ1xq9IDHGI1GlJaWoqurC0Kh0KaZXVfEmjIXTdMoKirCrFmz+nwM0g8VExODlJQUcDgcT8AzAJ6AZwzAMAy++eYb/PWvf8WLL76I//mf/3G7BYSUO/oyZ9RoNGwfhzvbMfSFuUWDSCRyW0NOaywZejaqUhSF0tJSaLVaiEQim5iwugvd3d2QyWRsI7szynY9R/lfu2s/2uo6hnzvyMkV4KVvn7L6eLVajZKSEkRERAzZWdwdGSjwMZlMOHv2LGbMmNHv39M0jcrKStTX1yM9Pd1le8FsiCfg8TA49fX1WLlyJbKysvD8889b5b7rChDBMWLO2HOiYzT0cQxGZ2cnJBIJwsPDHdLMPVL6s2Qg12wok27Ec8iVG3vtAcMwaGpqQllZmUNKPaQETAIcIsRIrtm5b69g58PvD7l5+dE9K3Hjn/KG9Dfmma709PQxIdnQV5nLYDDg4sWLEIvFg/492VCMge+IJ+DxYB0UReGNN97AZ599hr179yIrK8vZp2RBTzuGjo4OC8GxsLAwqFQq1NTUuHXGYziYN3OLRCKXaW4lOkXmEv+2LidSFIWysjKoVCpkZ2eP+nKHOUajEQqFAt3d3RAKhTbJdJlrS6lUKotrZm6q2fOarRb+BW0NGqufJyDYHwdLdiAwZHhj993d3ZDL5WAYBllZWS47vm8remZ7DAYDJBIJpk6datXf0zTtEt6JdsYT8HgYGmfOnMGaNWuwZs0a3H///U7LGAxkx0AyAX2ltLu6unD16lU27e3qGQ9botVqIZFIEBUVheTkZIe/dpPJZFGaMrdkGOia2QLSyB4dHe2U1+5M2tvbUVJSgtjY2H7HkvvDPOOmUqmg0+kQGBhokXGz5pp98LdP8cWu76xeUG9YnovH3llt9Xn2R2trKxQKBeLi4pCYmDjqrzuxqGhra0N9fT0mT55s1XvuCXg8AY+Hfujo6MD69euh1Wqxa9cuu08DjdSOoa/Hq6ioQGtrK3JycsZUj4f5a7dnxqO/ZnBHWjL0hKZplJeXo7W11aUyXY7A/LX3ZTIJWPZLkewNwzAW2ZvhZtyMRhNWZWyEVt1t1fF//39PYnyebbLIFEWhoqICLS0tyMrKskp52J0wGo1QqVRsptRkMiE0NBRxcXEIDw8fcJqL4Al4PAGPS1JYWIgNGzaAoijk5+dj8+bNvY759NNPsXXrVnA4HEyaNAmHDx+2+XkwDIMjR47glVdewY4dO3qNP47kcYmTsUajgUajYXtvzJtUbbFTU6vVkEqlSExMRHx8/Gj/sltAMh5k1z/S1z6YJUNYWJjL7K47OjoglUrdzoncFmi1WshkMoSGhiI1NZUtA6tUKrZfisvlsmVgW+o5vb7qXfz6z+JBP2sxKVHYe36bzb+PnZ2dKCkpcStPsp6YN4SrVCpW8JTH47FBKelLtFa0kBznCXj6xxPwOAGKoiAQCHDs2DEkJCRALBbjyJEjyM7OZo9RKBRYsmQJfvzxR4SHh6OpqQnR0QOPdY6E8vJyrFixAnPmzMFTTz015Buko+0YemIymSCXy2E0GiESiUZ9E7M5NE1DqVRCrVYjOzvb6kwXTdO9sjfkpjucjJszoGkaVVVVaGxshFAoHBM9XXq9nl0om5ubodfrER4ejpiYGPB4PLubarbWtePhyZth1FMDHrfsr7fj3qcW2uUcGIZBY2MjysvLLYQqXRVSUiTXzbwhnMfjITQ0dNCAfbDAh/yeaPKMYjwBjzthjRv7U089BYFAgPz8fIedl9FoxNatW/Hbb79h//79SExM7PM4V7Fj6AvizZSZmYnIyEiHP78zIdNMfD6/T6FGg8HApstJytyVVKZHQmdnJ6RSKWtJMlpGmXsqTWu1WlariGRvaJqGTCaDl5fXoF5LtuL5xW/g/E+Sfn/P4XCw/9J2RCXaV0fIaDRCqVRCq9VCKBS6jJWOeVBKJBhISZHH443o/tifaCFZ6119k2ID+n3jxo4uvRtRW1trEUwkJCSgqKjI4hi5XA4AmD17NiiKwtatW3HLLbfY9bx8fX3x0ksv4aeffsI999yDp59+GnfccQfKyspQUVGBxMTEXnYMiYmJLrVQRkdHg8vlQiKRoKWlBZmZmaNm8RsMHo8HsViM0tJSnDt3DklJSewkjlarhZ+fH7tQJicnj6osWHBwMKZOnYrq6mqcOXMGQqHQLXs8+gpKSRk4NTW1X1PNSZMmoampib3u9i7t3vPUbbjw01Uw/aw9E+YI7R7sANfuWUKhEBqNxkK2wZHfeWJeS65bR0cH/Pz8wOPxEBkZifT0dJtmt80DHJPJxGZ7PHgCHrfFZDJBoVDg559/Rk1NDebMmYPLly/b/Sau1WrB4XCwePFiPPnkk9i8eTP4fD5uvvlmTJo0CampqS5fM/f398d1112HmpoaFBcXQyQS9dncOZowF/XTaDQwGAy4cuUKoqKikJKSMqotGQgcDgdJSUmIioqCRCJBcHAwMjIyXNaPrOdCSUw1hxuURkdHIyIigg147alWnD1LAIE4HSVnyvr8/Y33zbbL8/ZHWFgYxGIxampqcPr0aVag1B6YTymqVCro9XqEhISw18xWvYkDQby4GIYBTdOgadoT+MAT8LgkfD4f1dXV7M81NTXg8/kWxyQkJGDGjBnw9fVFamoqBAIBFAqFVQJUw2XFihWQy+UQi8WYMWMGfv75Zxw/fhzvvfcebrnlFrcqEXE4HCQmJiIiIgJXr15lx5hHw6JPhBj7smQgDby+vr5sX5NSqYRIJHIbocmREhgYiClTpqC2thbFxcUuY9FAGsJJgGPu8p6UlGSTTKmPjw+EQiHUajWuXLli1/H9mx64HrLTyl7fqaCwQMy8zfGGxeQ7Hx0dDYVCgbq6OgufqeFirlekVqvZKUUej4f4+Hinl5Bqampw4sQJnDp1Cl5eXti7d69Tz8eZeHp4XBCTyQSBQIAffvgBfD4fYrEYhw8fRk5ODntMYWEhjhw5gkOHDqGlpQWTJ0/GhQsX7HrjJruEnly5cgWrV6/GkiVL8PDDD7vdLoKmaZSVlbFNve7U1NffiLG50vRgTarEliM1NRWxsbEOPHvnQ9zn/f39HTrRYz7Obz6FQxZKLpdr9wCUWA4QZ21bZ4cZhsGj059FjaLR4r/PWzkHj7z1gE2fazi0tbVBLpcPSbfIvGdKpVKhs7PTQiGcy+U6tURuNBpx+fJlnDx5EqdOnUJJSQn4fD5yc3Mxe/ZszJw502X6mOyIp2nZ3fj222+xceNGUBSF1atXo6CgAFu2bMG0adOwaNEiMAyDTZs2obCwEN7e3igoKMDSpUuddr46nQ5PPvkkysrK8M4779h1YsxeEOE2MtXhilAUZTEWrtPphm3JYI7RaERJSQlomoZQKBxV/TuDYe7GnZGRYRebAlLmIFkAMs5Prps1Uzj2gjjQ26PE99mOb/Dxi19Y/Lft3z8D4fR0mz3HSCB6Vc3NzRAIBL20xsyzbiqVyqJnisfj9dsz5QgYhkFHRwdOnz6NEydOoKioCK2trZgwYQJyc3ORl5eH8ePHj5keRTM8AY8H+8MwDP71r3/h2WefxUsvvYQbbrjB7UpEJpMJMpkMDMNAKBQ6tR+pp42GPSwZekKm2OzZ4+Cq6PV6yGQyeHt7j8iQ0/y6keyNeZmDTCm6EgzDoL6+HpWVlTa99nqdHn+etBmqpg4AAD8zFntOv+hy94Wuri722oeHh0Or1UKj0Vhk3Xg8nlM3AgzDoKqqCidPnkRRURHOnTsHb29vTJ8+HbNnz0ZeXt6Y0xnrB0/A48Fx1NbWYuXKlRg/fjy2bNnilr0hjY2NKCsrQ1ZWFiIiIhzynM60ZDDHYDBYLPyu3oRua8i1T0tLQ0xMzKDHm2fdzDVUyEIZGhrqNrtsg8HAZvps5U314XP/i3/uvCax8cDWu3DnxvkjfkxbQNO0hfaNTqeDl5cXuru7ERcXh/T0dJcqT8nlcrY8lZeXh+nTp4+F8tRw8AQ8HhwLRVF47bXX8OWXX2Lfvn3IzMx09ikNme7ubkgkEoSEhCAjI8OmJYeelgxkN2ku7+/v7+/UdDkRbhMIBC7R1OtIDAYD5HI5KIqCUChkg3ZzhXBy3QCwWbeRaqi4Ci0tLSgtLUVCQgL4fP6IXk9LXRsemVoAk96EA1dfQ0Scc+QAyEg/uXYURVlo35BsqclkglKphEajgVAodIg1CcMw0Gg0OH36NJvBaWtrsyhP5eTkuE3g7GQ8AY8H51BUVIQ///nPWLt2Le677z63a2gmaeTGxkZkZ2cPe0fVcwLHYDC4TA/HQOj1ekgkElam31VHuO1FY2MjFAoFuFwuKIrq1TPl7CZVe0JRlMXCP5JswtsbDqG1rh1b/nejDc+wf8iGggQ4PUf6uVzuoJnLjo4OyGQyVqzSlp99ogBuXp7y9fW1KE+5ujq0C+MJeDw4D7VajfXr10Ov12Pnzp1uKe/f0dEBiUTCujEPdCMilgzm+inmJqiOmMCxJQzDoK6uDtXV1cjKyrK7iawzMZ94Iwq4wcHB6O6+ZoaZk5PjVlN8tkCj0UAmk43Ik6y6pA7Vsnrk3j7VDmf4e1mRXLfu7m52Q8Hj8YY90s8wDGpra1FdXY20tDRER0cP01TViEuXLrEBTklJCRISEpCbm4vrr78e06dPt5sm0hjEE/B4cC4Mw+Djjz/G66+/jjfffBMzZ8509ikNGbLj1Wq1yM7OZvsb9Hq9Re/NaLJkMEen00EqlbLTPO6e2aBpmtUrIiPG5rYMXC7XYlff2toKuVyOxMTEEZd53A2SkWhoaBh20EvsDmyBeVlRrVYDQC+3d1tCSpxGoxFCoXDAx2cYBmq1uld5auLEiZ7ylGPwBDweXIPS0lKsXLkSN954IzZt2uR2JRKaplFbW4uysjIEBQXBZDKxlgzk32ge6WYYBjU1NaitrYVIJHKrbJ15YKpSqUBRlIVekTUjxiaTCaWlpejq6oJIJBpz2R4S9DrSidzcm4+Up/z9/dnsjb2Nh81pb2+HXC6HXC7H3XffjcDAQFbPyLw85efn5ylPOQ9PwONuVFZWIjk52dmnYRcMBgO2bNmCoqIi7N+/v5eKtCvRU9SPLJIhISFobW2Fn58fhEKh2wVuI6WrqwsSiQQ8Hg9paWkul8Hqa5E09wqzpodjIIhmU3x8/KAlztGGuW7RSMo8/WE0Gi2mpwwGg0Vg6mwbFL1ej6eeego//PADEhMT0draisTERIvpKU95yql4Ah53oqSkBLfeeis2btyI9evXO/t07Mbx48fxl7/8BX/961+xcOFCpy8axJKBpMq7urp6OU+bL5Lkxl9ZWem2ZpQjgWEYVFZWsg3djphm6Q+DwWCRvTEvK5IeDlt/viiKYhW6RSLRmFvkDAYDFAoFW+YZzgi7+dQb2VR4eXmx2VIej+fUfree5anTp0+z5SmhUIjjx48jISEBO3bsGHO6VS6MJ+BxN5qbmzFv3jykpaXh4MGDo9bcsrm5Gfn5+YiKisL27dsRFBTkkOfta7yYYRgLUb/BLBkIOp0OV69eddlsh73RarWQSCSIjIxESkqK3V+/+QQOcZ82bwp3tECcWq2GTCZDTEyM1RYFowli0WBNtov0TZEAp6urC4GBgRabCmf2tvQsT509exYBAQEW5anY2FiL1/jVV19h69at+P777+2i0u1hyHgCHneB+FXV19dj9+7d+Pnnn8EwDJ566iksXrzYps9VWFiIDRs2gKIo5OfnY/PmzRa/f/zxx/HTTz8BuFbCaGpqgkqlsuk5ANde8zvvvIODBw/i3Xffxfjx423+HCaTCRqNxsKSgdxoR2LJQGAYhpWoz8nJGXO7fSLR39raCpFIZFNBNDLST/4R92nz7I2zgwyaplFeXo62tjabv353gGS7VCqVhXYNybyRAMe8b4rH41m9qbAXBoPBYnpKLpcjKSmJLU+JxWKrvssGg2FU9+65GZ6Axx0gUwxarRY7d+6ETqfDY489hpKSEixbtgzPP/88HnzwQZs8F0VREAgEOHbsGBISEiAWi3HkyBFkZ2f3efzu3btx/vx5vP/++zZ5/r64dOkSVq9ejeXLl2PNmjXDXsQGs2SwpzicRqOBRCKxiWCbO0LG92NiYoblPk+unXnmjcj7k3+uZstgTkdHB6RSqcOyXa4EwzBobm6GXC5nszQ9tW+cbc2gVqtRVFTElqdUKhUmTJjAZm9EIpFnesr98QQ87sSHH36Is2fP4pZbbsH8+ddk2E0mE6qqqpCWlmaT5zh58iS2bt2K7767Jvn+8ssvAwCeeeaZPo/Pzc3F888/j5tuuskmz98fXV1d2LRpE6qrq/H2229blSI2b3IkGQBnSvtTFAWFQgGdToecnJwxt/Mj7vMqlQrZ2dkDlilJ5s1cPyUoKMjCDNXdggZSFmluboZQKBy15WiKoiyai82vXXd3N9ra2pyq0k2yjubTU6Q8lZeXh7y8PMTExIy5TckYoN8LOrZGS9yA3377DWfPnsX48eMxb9481NTUYN26dbjvvvuwZMkS9riRalrU1tYiMTGR/TkhIQFFRUV9HltZWYny8nLccMMNw34+awkKCsI777yDL7/8EgsXLsT27dvxhz/8gX2tffVvmFsy8Pl8p2cAvL29IRQK0dLSgrNnz9rNgdtV8fLyQkZGBtRqNS5dugQ+n4+EhAQA1/qdzKfeSOaNx+MhLi5uVNgyeHl5ITU1FVFRUZBKpWxvl7tnDvR6vYU1A+l54/F4bNOy+bXr7u6GTCZDQ0MDMjMz7R74GwwGXLx4kQ1wFAoFW57605/+hJ07d465UrMHSzwBjwtRUVGBb775BtHR0Vi0aBFomsYnn3wCHo+Hu+66C//7v/+LtLQ0TJ06FRwOBxRFOeQmevToUdx9990Ou2FzOBwsXrwYYrEYy5cvx6FDhxAWFoYLFy5g2bJlmD17NrhcLhISElzWkgEAIiMjERYWBolEgubmZmRlZbn9ojcUQkJCkJmZibKyMiiVSvj6+rLqt7GxsRAIBKP6/QgJCcG0adNQVVWFM2fOuNUkH8Mw0Gq1FhsLPz8/8Hg8REZGIj09fdCet4CAAEyaNAlNTU04e/YsUlJSejX8juT8VCpVr/LUxIkTMXv2bLz44oue8pSHXngCHheipqYGGo0Gd9xxB2JiYvDVV1/hP//5D3bu3AmlUon33nsPPj4+yMnJwfbt20f0Zebz+aiurrZ47v70cI4ePYq333572M81FJqamvDZZ5/h1KlTuHz5MrhcLpqbm9HU1IRt27Zh9uzZDjkPW+Hn54dJkyahrq4OZ86ccTuxPmshU2/m2RuSASDZnYqKCkRHR48pETYOh4Pk5GQ220OMaF1tITaZTBbWDKQxnMfjITk5edhj/RwOBzExMYiIiIBCoUB9ff2wBBtJeerEiRMoKirC+fPnERAQgBkzZiAvLw+bNm3ylKc8DIqnh8fFaGpqQnR0NM6fP49t27ZhyZIlmD9/Pl544QU0Nzfjsccew549e+Dj44Pdu3ezaeKhZntMJhMEAgF++OEH8Pl8iMViHD58GDk5ORbHyWQy3HLLLSgvL3fIzaSiogKFhYWYOXMmxo8fz+4iT548ibVr12L9+vVYtmyZW97Yurq6cPXqVYwbN87tG1ppmman3sh4cUBAgMV4cc8MgMlkgkKhgF6vh0gkcis/MVtAfJlqamogEAgQERHhtPPoac3A4XAsmovtVRYmgo2xsbEDjvD3VZ5KTk5Gbm4uZs+ejenTpztMwsKD2+FpWnZ1evbkHDhwAMXFxdi7dy92796NyspK3H333Zg5cyaKi4vx+OOPY/HixQgJCcGaNWuG9ZzffvstNm7cCIqisHr1ahQUFGDLli2YNm0aFi1aBADYunUruru7sX37dpu8zpGgUqnwyCOPgGEYvPnmm27ZDGo+vpyTk+M2N23Sv0EWSIqiLKbehjJe3NLSAoVCgdTUVMTGxtr5zF0PYs8QGBjoEAf6vjzDnOn4Tr4DH3zwAebNm4e8vLxe5Sm1Ws2Wp8j0lDtvEDw4FE/A444YjUYcP34cR48exaJFi3DXXXcBAMaPHw+hUIiXX34ZDz/8MMRisUsEJI6AYRgcOnQIb731Fnbu3AmxWOzsUxoWKpUKMpkMSUlJLlfiIbYM5m7vpH+DLJAj9VAyGo0oKSkBTdMQCoVjbpKNYRjU19ejsrISmZmZiIyMtNljG41G9toR1enQ0FA2OLXGM8yekIDnyy+/xCeffILOzk7Ex8ezwc3s2bNtblfhYUzhCXjcCSI+CFybpvrpp59w++23IzQ0FH/5y1/w3//+l52o+tvf/gZ/f388++yzFo/hqIZmZyGXy7Fy5UrccsstePzxx93ytZpMJtaBWSQSOW3RNxgMFtkbRy6QTU1NUCqVSE9PH5PS/Hq9HlKpFL6+vhAIBEMOJM01p0jvlLe3NxucOlp1ui8MBgMuXLjAlqdKS0uRkpKC3NxczJw5ExKJBO+99x62bdvGynB48DACPAGPO9KzzPX111/jzjvvRHV1NeLi4vD999/jX//6F+bPn48FCxbgypUrKC0txR133AHAMnAajRgMBjz77LMoLi7G/v37ER8f7+xTGhZk0XeEZknP6RutVgsfHx+L7I2jF0iDwQCZTAYvLy9kZWU5xIHblWAYBo2NjSgvLx808DP3e1OpVNDpdBaaU87WLWIYBu3t7SgqKsKpU6dQVFQEjUaDiRMnsto3QqGw1znW19fjsccew5YtWzBhwgQnnb2HUYIn4BkNfP3119BqtVi+fDmUSiXef/99BAcH484778S+ffvQ1taGuLg4FBcX491330VmZqazT9nuMAyDY8eO4YknnsCzzz6LW2+91S1T4Xq9HhKJBEFBQTad4iG2DCTAMRqN7Gg4cZ52laC4oaEB5eXlThWrcyYGgwElJSVgGIYt85HsG7l+pHeKWDMEBgY6vTxVVlbGZm8uXLiAgIAAzJw501Oe8uAsPAHPaMJkMmHbtm1obW3Fww8/jOLiYmzevBnjx4/Hv//9b7z//vuoqqrCCy+84OxTdRhNTU1YvXo1+Hw+tm3bNuSxV1eAYRjU1NSgrq5uWO7jRJSRBDhElNF8+sbVJ6NI4Ofv7w+BQGD3hl5Xgly/6upqNDQ0wMfHxyJ7Y4veqZGi1+tx4cIFnDp1CqdOnYJSqWTLU2R6yh2/ex5GFZ6AZ7RRWFiIjo4OLF68GCKRCJ988glKSkrwxhtvYMaMGeDxeNi2bRuAa1oYZIel0WjccrrJGmiaxu7du/HRRx9h7969/fqCuTrEfTw6OnpAP6q+tFPMszeuLMo4EKSht6qqyqnj2/aGoigLY03z6xcSEoK6ujqYTCZWxdjRkPIUCW6KiorQ0dGBSZMmsQ3GfZWnPHhwMp6AZzRCenTuv/9+vP322wgLC8Ply5fx3HPPYeHChVi1apXF8V988QVMJhPmzZs3KsXvCBcuXMCDDz6I+++/H/n5+W55Q6ZpGkqlEhqNBjk5OfD397doTu3o6HCYdoqz6O7uhkQiQXBwsEuK9Q0Von1DmosBWJSn+rp+ra2trIN3fHy8XUtDPctT58+fR1BQkEV5KioqylOeMqOwsBAbNmwARVHIz8/H5s2bLX5fVVWFFStWsE7x27dvx4IFC5x0tmMGT8AzGiFNzRs2bMDly5exZ8+eXlkNk8kEHx8fbNu2De+//z7eeust3HbbbU46Y8fR1dWFjRs3orGxEXv27HG7nhCy+29oaEBDQwN8fX0tdG8cbYjqLEiZr7a21q1Uqs1H+4n2jb+/Pxvc9CXM2B9EsFGn0w1Lpbg/+ipPpaamWpSnRlsQbUsoioJAIMCxY8eQkJAAsViMI0eOWNyD16xZg8mTJ2Pt2rWQSCRYsGABKioqnHfSYwOPeehohOy0du7cib1792LdunXYtGkTFixYAC8vL1AUBR+cAiaWAAAgAElEQVQfH3R3d6OkpATBwcF46aWX8Ic//IHtDxmpCamrEhQUhH379uHzzz/HrbfeildffRVz5sxx9mn1SX/Kt2FhYQgPD0dCQgIqKysBXLMEcXYfhyPhcDhITEzEuHHjIJFIwOVykZ6e7nJZu76aw4k1Q1pa2ohG+318fCASidDW1oaLFy+yZqxDeTyGYdDW1mYxPaXVatny1Pbt25GVleVy76src/r0aWRkZCAtLQ0AsHTpUnz11VcWAQ+Hw2GzeWq12m0nSUcLngyPm2M+et7S0gIvLy9ERERYBDIvvvgiurq6kJ+fj99++w0zZ85EVlYW+xijXbOHpJXFYjEKCgqcHjCYjxar1WrWlmEw5VsyxZSVlTVq+1oGgmEYVFVVoaGhYVhN3bY8D51Ox2ZvOjo64OXl5ZDmcIqioFQq0dHRAZFI1K9SNymJmk9PkfLU9ddfj9zcXE95aoR89tlnKCwsxIEDBwAAH330EYqKirBnzx72mPr6esybNw/t7e3o7OzE8ePHMXXqVGed8ljBk+EZrZBgh2EYC7VWciP75z//CaVSiSVLliA9PR3p6enYu3cvPv/8czQ0NGDXrl2jOtgBgKSkJBw7dgzbtm3Drbfein379iE1NdVhz29uqqlWq8EwDEJDQ8Hj8ZCRkWG1LUNsbCx4PB6uXr2K1tZWl8x02BNixDlu3DhIpVKHeZIR3zDzADUwMBA8Hg/x8fEOLS96e3tDIBBApVLhyy+/hEKhQEFBASiKwvnz59nyVFlZGVueWrVqlac85SSOHDmClStXYtOmTTh58iTuv/9+XLlyZUx9b10JT8AzSuhrwSQS7rm5uayC6aFDh/Dyyy/j3Xffxb/+9S8sWLAAH330ESIiIuyy23OVpj4fHx9s2bIFN9xwA5YvX44NGzbg3nvvtflr7su3yN/fH1wuF5GRkUhPTx/RqHVAQACmTJmCqqoqFBcXIzs7GyEhITZ8Ba5PSEgIpk6dioqKCru8B+bK0+RzSZqLhxKg2guGYUBRFIKCgnDx4kWIRCKEhoZi5syZmD17Nl555RVPecoB8Pl8VFdXsz/X1NSAz+dbHPPee++hsLAQADBr1ix0d3ejpaVlTKqKuwKektYo5+WXX8b7778PhUKBpqYmzJo1Cx988AGuv/566PV6rFixAm+99ZZdTBxdtamvvb0dDz/8MHx9fbFjx44RlUb0ej3bd0MWR0fZMnR0dEAikSAuLg6JiYljsjzR0dEBqVQ66Ah/f/SlXWSuPM3j8ZxeAjUvT506dQoXL15EcHAwOz0VGhqKgoICLFq0CE8++aTTz3esYDKZIBAI8MMPP4DP50MsFuPw4cPIyclhj5k/fz7uvfderFy5ElKpFDfeeCNqa2vH5HfVgXhKWmOVZ555BqGhoTh37hx+++03XH/99bj++uthMBigVquh0WhQW1vbZ8Az0oZmV23qCw8Px5EjR3Dw4EHccsst2LVrl1V1dTJ5QxZHYqrJ5XIRHh6OlJQUhy42oaGhmDZtGkpLS3H+/Hl2fH0sQd6DsrIynD17FtnZ2QM60FMUxZanVCoVuru7We2bhIQEl9Au0uv1OHfuHFueKi8vR1paGnJzc/Hggw9CLBb3Kk/NmTMHr776Knbt2oVNmzY56czHFj4+PtizZw9uvvlmUBSF1atXIycnB1u2bMG0adOwaNEi7NixAw899BDefPNNcDgcfPDBB55gx4l4MjyjmJ7NyD///DPeeecdfPrpp6irq8OOHTtQW1uLo0ePWhyvUqnA4/EAjMyPyx2a+mQyGVatWoXbbrsNjz32mMX7RYJCc9fpkJAQducfEhLiMjcvotcyVk04gWsBs1QqtZhi0uv1FtYMDMOwjeFE+8bZ5am2tjYLcb/Ozk5cd911rLifQCBwehDmwYMb4cnwjEV6NlLOnTsXH3/8MWbMmAGhUAiTyYR9+/YBuLa4+/n5ob29HY8++ijuvvtu3HHHHfDy8rKrCamzm/qEQiF+/vlnPPPMM7j55pvxxz/+EZcvX0ZQUBAeeeQRdnFMSkpyuuv0QIwbNw7Tpk2DVCpFc3MzsrKyxpQtA3BNxE8oFKK0tBRlZWXw9fVFQECAzfqnbAFN0ygtLWUDHFKemjVrFubMmYPNmzcjMjLSZQJpDx5GE2PrjjiGIUHLgQMH8N///hfJyclsr4nRaGQX83/84x9QKpX44osvsGvXLuzbt481IR1q4OPKTX3EdPTkyZM4ceIEmpqawOVy8eOPP2Lx4sXIz893u6kWX19fTJgwAfX19SguLoZQKGQzdaMRc2sNlUoFg8GAkJAQxMTEwMvLC1VVVYiJibG7QvFAdHd3W0xPEUf03Nxc5OfnQywWj7kypAcPzsJT0hpD9AxYSI8O+d9PP/0UhYWFuOmmm7Bs2TI8+eSTCA8Px9q1axEeHt7nYwyEKzf1MQyDrVu3YtKkSZg1axbi4uIAAI2NjVi1ahVSUlLw97//3e2CHoJOp8PVq1cRHh6O1NRUty+JmIszEmuGwaw1iEJxd3c3srOz7R5YMAyD1tZWi/JUV1cXrrvuOuTl5SEvLw+ZmZlufy1szWCTnJWVlVi9ejWam5sRERGBjz/+GAkJCU46Ww9ugMdawsPAXL58GTt27MDcuXOxePFicLlcFBQU4JtvvkFeXh40Gg0OHDgw5LLOt99+i40bN7JNfQUFBRZNfRKJBA899BC0Wi04HA5effVVzJs3z06v0jpomsZbb72FI0eO4N1333VbE1KGYVBRUYGWlhZkZ2cjODjY2adkNWS8nwQ45uKMxJrBWu0b0t+UmpqKmJgYmwXTNE1DoVBYlKdCQkIwa9Ys1ntq3LhxnvLUAFgzyXnPPffgtttuw4oVK/Djjz/i4MGD+Oijj5x41h5cHE/A42Fgnn32WRgMBqxbtw7JyckoLi7GqlWrsG3bNixcuBC333475s6di8cff5zNCNmzt8cVOHfuHPLz87Fq1SqsWrXKbV+rRqOBRCJBQkIC+Hy+Sy7ARqPRornYZDKx4ow8Hm/E2jdGoxFyuRwmkwkikWhY/Vjd3d0W01MVFRVseWr27Nme8tQwOHnyJLZu3YrvvvsOwDUZDeDadCkhJycHhYWFSExMZJvOyWSnBw994Gla9jAwzz33HKqqqpCcnAyNRoMtW7Zg6dKlWLhwIbRaLTo6OpCeng7gWi9OYmIi69c1WpWap0yZgl9//RWPPfYY7r//fuzevdstLR3CwsIgFoshl8tx8eJFZGdnO7UBm2EYdHV1scGNRqOBt7c3G9wkJyfb/Px8fX2Rk5OD5uZmnD17dtBpNoZh0NLSwgY3p0+fRldXF6ZMmYLc3Fzs2LHDU56yAbW1tUhMTGR/TkhIQFFRkcUxkyZNwj//+U9s2LABX3zxBTo6OtDa2up2hsAenI8n4PEAmqbh6+vLBjTPP/88urq6UFBQAAB48803cd1112HatGmgaRqffPIJzp07hzfeeGPU19KDg4Nx4MABfPrpp7j11lvx2muvIS8vz9mnNWS8vb0hEonYBT8jIwNRUVEOeW6ifUMajHU6HYKCglhrBqFQ6LDAISoqClwuFzKZDEeOHMHy5csRFRUFmqYhl8vZAOfSpUsIDQ3FrFmz8Mc//hEFBQWe8pSTeP3117F+/Xp88MEHmDNnDvh8/qjdZHmwL56Ax0OvxebRRx9l+z2OHDmCyspKLF26FDExMaipqcHatWvx008/4Y477sAnn3yCjIwMeHt7o7a2ttcU1miAw+Hg3nvvxcyZM/H/27vzoKjPMw7g3+UQWY4VVFChiNyoYAVEWZHDlKaCJZImHcURlUMaxWKMF3VGIw7GYGII6nCo9WiVVmMMHtC0iERtxGgxAllkAiIiI0HkEDki7G7/yPAbiXhgkIWf389fOrsuLw6wX97nfZ8nLCwMcrkc69atG5Qdbbve8BUKBerq6uDg4NDnbx5d3ae7dnBUKhWMjY0hk8ng4OAAfX19jQaHIUOGwMHBASdOnBDeQJuammBvbw+5XI7o6Gh4eHiwPNUPnucm55gxY/D5558DAB48eIBjx46J+vYhvTw8w0PdPHoup6amBkuXLsXrr7+OoKAgbNmyRTgEm5aWhqSkJCxfvhxubm4oKytDfHw8EhMT+6VbsqZ0dnZi8+bNyM3NRXp6OsaOHavpJb0QtVqN6upq3L59G+PHj4exsfELv05LS0u3yeFDhgzpNvld08Gwp/JUW1sb3NzcMGHCBJw6dQq2tra/eMwI9d7z3OSsq6uDqakptLS0sH79emhrayM+Pl6Dq6YBjoeW6cWUlpbC3NwcV69exerVq3HlyhUUFxdj9uzZMDc3R3Z2Ni5fvoxFixZh9uzZ2L17t6aX3C/OnTuH5cuXY+XKlXjrrbcGbamjpaUFCoUCI0aMgLW19TM/j87Ozm6jGX788UdhNENX92lNn2vpqTxlbGws3J6Sy+XdhuWq1WocPHgQO3fuxPnz5wdtK4LB6lk3OT/77DPExcVBIpHAx8cHu3bt4u4bPQ0DD/0yf//737Fjxw5cunQJpaWliI+Px/Tp07F06VKsX78ep06dAgDExMQgKioKwC8bSzEY1NfXIzo6GlKpFNu2bRu0U8tVKhUqKirQ0NCACRMmQF9fX3js571vAAiTw7tGM2hae3s7/ve//wkBp7KyUihPTZ8+He7u7s/1BtnS0jKoru4TUY8YeOiX++CDD5CbmytMNU9OTsaFCxewdOlSFBUVoa6uDu3t7bh37x7Gjx//SpQHVCoV9uzZg9TUVOzYsQOTJ0/W9JJeWENDAxQKBYyNjYVSlZ6enhBuZDKZxg+LqtVq3L17t1t5qr29Xbg95e3tDTs7O1EHbSJ6KgYeenGP7tSUlZXhyJEjeO211zB+/Hi4u7vj/fffR2hoKADgxo0bsLOzw4ULFyCXy3t8DTFSKBQIDw/HnDlzsGzZMo0Hg+fR0dHR7XBxR0cHDAwM0NbWBh0dHUycOFHj88NUKhVKS0u7ladkMtkTy1NE9Mpj4KFfpuvr5NFzDydPnsSxY8dw4MAB4Xm//vWv8fvf/x6bN29Ga2srbt68KXRNLS8vh5GRkWinebe3t2PdunUoKSlBSkoKRo0apeklCdRqNdra2rqVp7S1tYXRDMOGDesWbmpra1FeXg4HB4d+7XfS1tbWrTx169YtoTzl7e0Nd3d3jYewgSY8PBynTp2CmZkZiouLH3tcrVYjNjYWWVlZkEql2L9/P9zc3DSwUqJ+wcBDfaOryzIAXLt2DW+99RZiY2MRExODmJgYXL16Ff/9738B/NRFNSkpCaGhoXjjjTdQXl6OL7/8EvPnz4dMJtPkp/HSqNVqZGVlIS4uDvHx8QgICNDI7oNKpRIOFzc1NaG1tRX6+vpCuDEyMnrmLtSPP/6I7777DgYGBkLrgb70pPKUu7u7EHBsbW1FvTPYF86dOwdDQ0OEhYX1GHiysrKwY8cOZGVl4dKlS4iNjX2suR+RiDDw0Mtx48YNXLt2Db/5zW/g5eWF3NxcmJmZoaysDIcOHcKDBw+QmJiIjIwMTJ48GXZ2dtDV1e3zDs0DbQDhnTt3sGjRIjg4OGDTpk0v/XDvw4cPhXDT2NgIpVIpHC6WyWQvPJpBrVajqqoKd+7c+cXnsn5enioqKnqsPGViYsLy1Au4efMmZs+e3WPgiY6Ohp+fH+bNmwcAcHR0RF5enjAwl0hkOFqC+p5SqYSNjQ1sbGzQ2NgIU1NTFBUVwcXFBSdOnEBZWRm2bt2Kq1evIicnB62trXB2doZKpYK2tnafnetRKpVYtmxZtwGEwcHB3QYQrlq1CmFhYcIAwri4uJc6gHD06NHIysrC9u3b8bvf/Q6pqalwcnLqk9d+tPdNU1MTmpuboaOjI+zeWFtb91nvG4lEAisrK5iamkKhUMDc3BxWVlbPFUp6Kk85ODhALpdj2bJlcHNzY3mqH/Q0vqG6upqBh145DDz0wh7doRk2bBji4uLw4YcfwsjICPX19diwYQMMDAyQkZEBe3t7LF68GMnJycjPz0dCQgLGjRsH4KdeP46Oji+8jm+++QZ2dnawsbEBAMydOxeZmZndAo9CocD27dsBAP7+/pgzZ84Lf7znpa2tjdWrV8Pf3x8RERGIiopCWFhYr0OeUqlEU1OTsHvT3t4u9L6xtLSEkZHRSy/7GBoawsPDA+Xl5SgoKIClpSXMzc2Fx7vKUxcvXhTKUw8fPhTKU6GhobCxsWF5iog0hoGH+oRarcasWbMwc+ZM7NmzB42NjfD398enn36Kzs5OLFiwAFevXkVGRgYsLS2xcOFCREREYOHChSgsLIRCoUBISMgLfeyBPoDQw8MD58+fR0xMDM6cOYPk5GSYmJg88fnt7e3dbk91TYiWyWRwcnLC0KFDNVL20dLSgr29PSoqKhAQEIAFCxZg+PDhQnlq2LBhkMvl+O1vf4uNGzeyPDVAPM/4BqJXAQMP9QmJRAKVSgU9PT0sW7YMKpUKJ0+exDfffIPIyEiYmpoiKioK8+fPxx/+8AeMHj0aBw4cwJEjRxASEgJdXV1UVVVBR0fnpWy1a3oAoaGhIfbt24eMjAwEBgbi448/hlwuR2dnJ65fvw6ZTIbGxkY8ePAAenp6kMlkGDFiBGxtbaGjo/lv07a2Nly5ckUoTxkaGuLw4cMwNDTEhx9+CB8fH5anBqjg4GDs3LkTc+fOxaVLlyCTyVjOoleS5n+SkmhoaWkJ53K0tLQwZcoUtLW1wd/fH7GxsXB0dERoaChMTU0B/LQrdObMGWhpacHMzAy7du3CX/7yF4wcObJXb/KDZQChRCLB7Nmz0dnZiYiICBgYGODhw4dwc3PD5s2bYW1tDUNDQ43viqjVatTW1nYrT3V0dAjlqfnz5wvlqYyMDKxevRrJycmYMWOGRtf9qpo3bx7y8vJQV1cHS0tLbNq0CR0dHQCAP/3pTwgMDERWVhbs7OwglUqxb9++fl/jo7c7iTSFt7TopXj0B1xBQQFWrFiB1NRU4VxNbm4ujh8/Di8vL7z++usICAhAZ2cnLl682Ov2/oNhAGFcXBxycnIwZMgQTJs2DVOnTsXFixdRUFCAtLQ0WFlZ9dtafk6pVKK0tFQIOEVFRTA1NRVuT3l5eT21PHXr1i0UFxcjMDCwn1dOAx2DDmkAr6WTZtXW1sLU1BQ6OjqoqKjA7t27IZVKERcXhwMHDuDUqVPQ09PDnTt38Pnnnwu7QM9roA8gLC4uhq2tbbc5VQBw9uxZxMbGYs2aNQgJCemXN4fW1tZut6eqqqrg6Ogo9L6ZPHkyy1PUa09qNdFVDm1ubkZgYCBDEL1sDDykGT//4aZUKpGQkIB79+7hvffeQ11dHd5991189NFHmDJlCm7cuIHTp09j0qRJ8PHx0eDK+8+9e/cQFRUFmUyGxMTEPh1g+fPy1OXLl7uVp7y9vTFu3DjenqJeUSqVkEgkPX7d/PDDD2hpaYGNjQ0OHjyIrVu3wtbWFv7+/li5cqUGVkuvGAYeGjhycnLQ3NyM4OBghISEICgoCHPnzhUO7k6ZMgX/+Mc/4O7uruml9huVSoX09HSkp6dj165dmDRp0gu9zs/LU8XFxY+Vp4YNG8bfsB/xrNEM169fx+LFi1FQUICEhASsWrVKA6vUnOftl5WSkoI9e/ZArVYjMjISS5cuxdSpU3Hw4MFf1HaCqJcYeGhgUavV+OGHH7BixQps27ZNuFaem5uL06dP4+OPP0ZzczOuX78OHR2dQT2FvDeKi4sRHh6Ot99+G++8884z32haW1u73Z66ffs2nJycupWn+qoJoVg9azRDbW0tKisr8cUXX8DExOSVCDxPKzvdv38fqampOHPmDEaOHIlNmzbBysoK4eHhiI6Ohre3t/DcJUuWQKFQwM3NDWPGjIGXlxd8fX1Z1qKXiZ2WaWCRSCQYNWoUDAwMMGfOHOTl5cHIyAgNDQ1oaWmBUqlETEwM9PX1YWFh8coEnokTJ+Krr77CmjVr8PbbbyMlJUUYttoVEh8tT3V2dsLDwwNyuRxhYWGwtrZmeaqXfHx8cPPmzSc+bmZmBjMzM5w+fbr/FtWPVCoVAHT7uukKI3fv3sW///1vmJiYwM/PD1KpFP/5z3+gVCpx+PBhHD16FNu3b8eSJUtgaGiI9957D6GhobCysoKXlxfS09NRWVmJmpoa/PWvf8Xx48c5x4s0hoGHNGrv3r3Iz8+HkZER2tvbkZGRAbVajYSEBDQ1NSEtLa1fDxcPBPr6+khOTsbJkycRFBQEX19fNDY2CuUpuVyOWbNmIT4+nuUpeiGP7rA8GnSam5thZGSEr7/+GidOnMD169cxdOhQ3L17FwqFAqtWrUJubi6qq6vR0tKC48ePY9y4cdDT08OGDRtQUlKCkpISHDp0CPv370dmZiYMDAzg6OgIb29vtLe3AwC/ZkkjGHhIYzo7O6Gjo4OpU6cCAMrKylBbWwt9fX14enpizZo1L33o5kAlkUgQHBwMJycnbNmyBStWrGB5inrlSWWjrjM5XbeqLl++jA8++AAlJSUICAjA4sWLhW7lvr6+eP/993H8+HHs3LkTUVFRkEgk6OzsxJQpUxAdHS2Uo1UqFUaPHo2ZM2fCxsYGx44dg1KpRGhoKO7duwcbGxusX7++v/8biAQMPKQxXc0FExMTcfjwYbi7u8PV1RUbN27EyJEjAbCPh4ODA/bv36/pZdAA1fX90dDQAF1dXRgaGgpBpuv7puvvXbs3Wlpa+Oc//4mjR4/is88+w6VLl/Duu+9ixowZSEtLw4IFC4QhwF0dmeVyOTZv3oyWlha4uLigsbERb7zxBoCfJrXX19fDxMQEa9euRWlpKaRSKXbs2AFtbW0cOXKk35t8EvWEgYc0bu3atTA2NkZAQADGjBkDqVQq/Bb6Kocdoifp6OiArq4uJBIJkpKSsHLlShw4cAALFiwQeuEUFhaivr4efn5+OHv2LNauXYucnBwYGxsLZ+ja2tqwc+dOuLq6Yt26dejo6MDMmTOhVCphbm6O+vp6tLe3w9zcHPr6+igrK0NERATOnTuHP/7xj6iqqkJ9fT22bNkCZ2dnbNy4Eba2tt12ZrvCztOushP1BwYeGhDeeecdAD0foCR6mZ41mqGmpgYeHh64f/8+tLS0kJSUBIVCAWNj435fa3l5OVxcXJCUlIQlS5YA+Gmn1MHBAQqFAgBQU1ODsLAwtLS0YNSoUcjKykJiYiLefPNNbNiwAUlJSbhy5Qp8fHygr6+PlpYWzJo1C3PmzOk21Hbs2LEoLCxEdXU1bG1tYW5ujry8PPj4+GD37t3Iz8+HmZkZJk6cKPybru7marUaarW62/dxf86uI+oJAw8NKAw61N8yMjKe+vioUaNw+/btflrN01lYWKC9vR3Z2dmYN28ehgwZgpKSEkRGRuL8+fMAgMzMTEyaNAnbtm1DZ2cnbG1tERYWhnXr1mHatGn47rvv8O2338LX1xcA4OnpicrKSkilUgDAnj17MH36dNjZ2eHrr79GQ0MDAOCTTz4RdmukUilmzpwprOvnvXokEgl3Z2nA4bsLEdEgMXToUPzqV7/ChAkTkJqaCj09PZSWlsLe3l64TVVQUCDMrNPR0YFcLseXX34J4KcmiykpKbh//74QcBISEqCrqwt/f39MmDAB2dnZaG1thaenJ5KTk+Hh4QG1Wo2xY8dCJpN1W09XHzf+okKDAXd4iIgGkaCgIDg7O+Py5cs4dOgQXnvtNQDA8OHDoVAoMH78eFy7dk14vp2dHYqKigAACxcuREpKCr744gt4enoCAJycnLBmzRrMnz8f1tbW3T5WV5B50m4Nd3FoMGEsJ+ql8PDwx84uPEqtVuPPf/4z7Ozs4OrqioKCgn5eIYlZUFAQSktLERoaiq1bt6K4uBgzZsyAhYUF8vPzERISgsrKSqSlpeHMmTOoqKhAeHg4AEBPTw9hYWGQSqXdZrbp6uoKYUelUgln6YjEhIGHqJcWLVqEf/3rX098PDs7G99//z2+//57pKenCweyxeZZwe/QoUNwdXWFi4sL5HJ5t10HenFTp07FyZMn4enpiYiICIwYMQKmpqawt7dHXl4erKyssGXLFuTk5ODTTz+Fh4eHMO6hsbERH330EebNmwelUtnj62tpabFERaLEkhZRLz1rFEFmZibCwsIgkUgwbdo0NDY24s6dO0JPE7FYtGgRYmJiEBYW1uPj48aNw1dffQUTExNkZ2djyZIlHCvQB4YPH46KigoAwPLly1FfXw8AcHZ2hq+vL1pbW+Hs7IyjR48+9m+//fZb1NTUYMWKFbw1Ra8cBh6iPlZdXS10nwUAS0tLVFdXiy7wPCv4yeVy4c/Tpk0bMDedBruuIF1YWAhXV1ehSaeLiwtcXFy6PVelUgnXwyUSCfz8/ODn56eBVRNpHgMPEb10e/fuxaxZszS9DNHIysqClpbWY9fBu25N9TQni+hVx8BD1McsLCxQVVUl/P327duwsLDQ4Io06+zZs9i7dy8uXLig6aWIhpaW1mON/QDemiJ6GsZ/oj4WHByMgwcPQq1WIz8/HzKZTHTlrOdVWFiIyMhIZGZmYvjw4Zpejqgw3BD1Dnd4iHrpWaMIAgMDkZWVBTs7O0ilUuzbt0/DK9aMW7du4c0338Tf/vY3ODg4aHo5RPSKk3TVfJ/gqQ8S0avr0eBnbm7+WPCLjIzEsWPHMHbsWAA/df29cuWKJpdMROL3xK1PBh4iIiISiycGHp7hISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiIvbMK58AAAChSURBVNFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0WPgISIiItFj4CEiIiLRY+AhIiIi0dN5xuOSflkFERER0UvEHR4iIiISPQYeIiIiEj0GHiIiIhI9Bh4iIiISPQYeIiIiEj0GHiIiIhK9/wPs/uHJw8UKigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the initial volatility surface\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(elev=20, azim=60)\n",
    "ax.plot_surface(x, y, z,cmap='viridis', edgecolor='none', antialiased=True)\n",
    "ax.set_xlabel('Moyeness')\n",
    "ax.set_ylabel('Time to matuirity')\n",
    "ax.set_zlabel('Implied volatility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRyQIRUOEYa0"
   },
   "source": [
    "### Smoothing the volatility surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlUbJTC2253E"
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "AWU-1yMd254K",
    "outputId": "78f0535d-e9b0-44eb-db8b-70645b44829c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Smoothing the volatility surface\n",
    "xnew, ynew = x.copy(), y.copy()\n",
    "tck = interpolate.bisplrep(x, y, z, s=1.0)\n",
    "znew = interpolate.bisplev(xnew[:,0], ynew[0,:], tck)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(elev=20, azim=60)\n",
    "ax.plot_surface(xnew, ynew, znew,cmap='viridis', edgecolor='none', antialiased=True)\n",
    "ax.set_xlabel('Moyeness')\n",
    "ax.set_ylabel('Time to maturity')\n",
    "ax.set_zlabel('Implied volatility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fo8DWjnxYK4"
   },
   "source": [
    "### Implied volatility difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8x5s7Zk2tTE"
   },
   "outputs": [],
   "source": [
    "# Generating the surface points\n",
    "r= .02\n",
    "tau= 0.5\n",
    "rho = -0.05\n",
    "kappa= 1.5\n",
    "vbar = .1\n",
    "gamma = .3\n",
    "v0 = 0.1\n",
    "moyeness = 0.74\n",
    "n_moneyness = 11\n",
    "n_tau = 6\n",
    "moyeness = np.linspace(moyeness, 1.24,n_moneyness)\n",
    "tau = np.linspace(tau, 1, n_tau)\n",
    "\n",
    "cos_prices = []\n",
    "X = []\n",
    "Y = []\n",
    "for m in moyeness:\n",
    "  for t in tau:\n",
    "    X.append(m)\n",
    "    Y.append(t)\n",
    "    cos_prices.append(call_option_Heston_moy(N = 160, L= 12, r= r, tau= t, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, moyeness = m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmoYE-ad2wS_"
   },
   "outputs": [],
   "source": [
    "rho = np.array(rho*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
    "kappa = np.array(kappa*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
    "vbar = np.array(vbar*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
    "gamma = np.array(gamma*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
    "v0 = np.array(v0*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
    "# Reshaping for torch tensor dataset\n",
    "cos_prices = np.array(cos_prices).reshape((-1, 1))\n",
    "X = np.array(X).reshape((-1, 1))\n",
    "Y = np.array(Y).reshape((-1, 1))\n",
    "r = np.array(r*np.ones(n_tau * n_moneyness)).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2wKtc9r22gK"
   },
   "outputs": [],
   "source": [
    "data_ann = np.concatenate((X, Y, r, rho, kappa, vbar, gamma, v0, cos_prices), axis = 1)\n",
    "data_ann = torch.FloatTensor(data_ann)\n",
    "# Creating a torch dataset\n",
    "data = np.concatenate((X, Y, r, cos_prices), axis = 1)\n",
    "data = torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWoZ5J34xc_v"
   },
   "outputs": [],
   "source": [
    "implied_vol_pipe1 = np.apply_along_axis(calcimpliedvol, 1, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Foh8xvsN3Jy2"
   },
   "outputs": [],
   "source": [
    "# Getting the implied volatility from the dataset\n",
    "with torch.no_grad():\n",
    "  data = data.to(device)\n",
    "  data[:,3] = data[:,3] - torch.maximum(data[:,0]-torch.exp(-data[:,1]*data[:,2]),torch.zeros(data[:,0].shape).to(device))\n",
    "  data[:,3] = torch.log(data[:,3])\n",
    "  output = loaded_IV_ANN(data)\n",
    "  implied_vol_pipe2 = (output).data.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "qlJw_Wg91h2Z",
    "outputId": "4dcb6936-f76a-4351-f28d-9dd2b33cb859"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "Z = (implied_vol_pipe1 - implied_vol_pipe2).flatten()\n",
    "\n",
    "x = np.reshape(X, (-1, min(n_moneyness, n_tau)))\n",
    "y = np.reshape(Y, (-1, min(n_moneyness, n_tau)))\n",
    "z = np.reshape(Z, (-1, min(n_moneyness, n_tau)))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pcolormesh(x, y, z, shading='gouraud')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Moneyness\")\n",
    "plt.ylabel(\"Time to maturity\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ll-jOXXHagp0",
    "CdhtgoThB8B3",
    "yNTO4M51R3cd",
    "z_OVhr6nR-Iz",
    "aWu772JpMRaZ",
    "Glh6JAzfpWHm",
    "lTZo9gF1GJwq",
    "dGGuqkhpzjak",
    "SW3NslRmKsTU",
    "1EImNgxzAbeI",
    "78q0DF6HAeN5",
    "c3QMaOLPPwux",
    "fBQ-dVz4DFUh",
    "py6auAo1DAq3",
    "oF6h1RohC9KX",
    "4fO7JQlDDNcW",
    "ehFOGXCpDPyX",
    "-aCVlS1x_RIt",
    "AOjnH0N6HMXt",
    "C3o5w0J_Hl1O",
    "F2J3aEAhSflZ",
    "DA3X7W_KAhSr",
    "pzV98IpxAm3_",
    "BKD1CeSZkxcO",
    "KLwRkWB09WAh",
    "oYbO_aqdEcyQ",
    "lRyQIRUOEYa0",
    "5fo8DWjnxYK4"
   ],
   "name": "Redwan_Mekrami_Pricing_options_and_computing_implied_volatilities_using_neural_networks_Code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
